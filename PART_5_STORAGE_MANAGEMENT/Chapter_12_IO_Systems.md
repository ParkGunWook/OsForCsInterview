## What we gonna learn

컴퓨터가 해야하는 두가지 주요한 업무는 I/O와 컴퓨팅이다. 많은 사례에서, 주된 업무는 I/O였고, 컴퓨팅 또는 프로세싱은 자주 없었다. 예를 들어서, 우리가 웹 페이지를 둘러보거나 파일을 수정할때, 우리의 주된 관심은 몇몇 정보를 읽거나 쓰는 것이지, 우리의 물음을 컴퓨팅하는 것이아니다.

컴퓨터 I/O에서 운영체제의 역할은 I/O 명령어와 I/O 기기를 관리하고 컨트롤하는 것이다. 비록 다른 장에서도 관련된 주제가 나오지만, 여기서 우리는 I/O의 전체 그림을 완성하기 위해서 조각들을 모아볼 것이다. 먼저, 우리는 I/O 하드웨어의 기초를 설명하는데, 하드웨어 인터페이즈의 본질이 운영체제 내부의 기능에 제한하기때문이다. 다음으로, 우리는 운영체제가 제공하는 I/O 서비스와 I/O 인터페이스 앱에 내장된 서비스를 다루겠다. 그리고, 우리는 어떻게 하드웨어 인터페이스와 앱 인터페이스 사이의 차이를 운영체제가 연결하는지 보겠다. 우리는 또한 드라이브 코드의 파이프라인을 동적으로 합치는 것을 가능하게하는 앱인 유닉스 시스템의 Vstreams 메커니즘을 보겠다. 마지막으로, 우리는 I/O의 성능 측면과 I/O 성능을 증가시키는 운영체제 디자인의 원칙을 보겠다.

## Chapter Objectives

- 운영체제 I/O 서브시스템의 구조를 살펴본다.
- I/O 하드웨어의 원리와 복잡도를 살펴보겠다.
- I/O 하드웨어와 소프트웨어의 성능 측면을 설명하겠다.

## 12.1 개요

컴퓨터에 연결된 디바이스의 조작은 운영체제 디자이너의 주된 고민이다. I/O 디바이스가 그들의 기능과 속도에서 다양하게 있기에(마우스, 하드디스크, 플래시드라이브, 테이프 로봇을 고려하자.), 다양한 방법들이 그들을 통제하기 위해서 필요하다. 커널의 I/O 서브시스템에 있는 메서드들은, I/O 기기를 관리의 복잡도와는 나머지 커널로부터 분리되어있다.

I/O 기기 기술은 두가지 트렌드가 있다. 한쪽에서는, 우리는 소프트웨어와 하드웨어 인터페이스의 표준화가 증가하고 있는 것을 볼수 있다. 이 트렌드는 존재하는 컴퓨터와 운영체제에 향상된 디바이스 세대를 포함하게 해준다. 반면에, 우리는 I/O 기기의 넓은 다양화를 볼 수 있다. 이 도전은 하드웨어와 소프트웨어 기술의 조합때문에 생긴다. 포트, 버스, 디바이스 컨트롤러 같은 기초적인 I/O 하드웨어 구성요소는 다양한 I/O기기를 수용한다. 다른 디바이스의 이상함과 상세정보를 캡슐화하려면, 운영체제의 커널은 디바이스-드라이버 모듈을 사용할 수 있게 구조화되어야한다. **device drivers**는 I/O 서브시스템에 일정한 디바이스 접근 인터페이스를 보여주고, 시스템 콜은 앱과 운영체제 사이에 표준 인터페이스를 제공한다.

## 12.2 I/O 하드웨어

컴퓨터들은 다양한 종류의 디바이스를 운영한다. 일반적인 범주의 저장소기기(디스크, 테이프), 통신기기(네트워크 연결, 블루투스), 휴먼-인터페이스기기(마우스, 키보드, 스크린, 입출력 장치)의 대부분이 알맞다. 다른 기기들은 더욱 전문화되고, 이런 것들은 제트기의 조종도 포함된다. 비행기에서, 사람은 항공기기에 조이스틱과 페달을 통해서 인풋을 주고, 컴퓨터는 방향타와 플랩을 움직이는 모터와 엔진을 점화하는 아웃풋 커맨드를 보낸다. I/O 기기의 무한한 다양성 때문에, 우리는 어떻게 기기가 부착되고 어떻게 소프트웨어가 하드웨어를 조작하는 지에 대해서는 조금의 이해만 존재한다.

디바이스는 케이블 또는 공기를 통해서 시그널을 보내서 컴퓨터 시스템과 통신한다. 디바이스는 연결점 또는 **port**를 통해서 기계와 통신한다.(**PHY**라는 용어는, OSI 계층의 물리 계층의 약어이고, 포트를 설명할때도 쓰이지만 데이터 센터 명명법에 더욱 일반적이다.) 만약 디바이스들이 일반적인 와이어의 집합을 공유하면, 연결은 버스라고 불린다. 전자학에서, 메시지는 정해진 타이밍에 맞게 와이어에 전달된 전압 패턴이다. 디바이스 A가 디바이스 B에 연결하는 케이블이 있으면, 디바이스 B는 디바이스 C에 연결되는 케이블이 있고, 디바이스 C는 컴퓨터의 포트에 플러그인 되고, 이런 정렬을 **daisy chain**이라고 부른다. 데이지 체인은 버스처럼 작동한다.

버스들이 컴퓨터 아키텍처에서 널리 쓰이고 그들의 시그널링 메서드, 속도, 산출량, 연결 메서드는 다양하다. **PCIe bus**(일반적인 PC 시스템 버스)는 프로세스-메모리 서브시스템을 빠른 기기에 연결하고 **expansion bus**는 키보드와 시리얼과 USB 포트 같은 상대적으로 느린 버스에 연결한다. **serial attached SCSI(SAS)** 버스는 SAS 컨트롤러에 연결되어있다. PCIe는 하나또는 여러 레인을 걸쳐서 데이터를 보내는 융ㄴ한 버스이다. 레인은 두가지 시그널 쌍으로 구성되었는데, 하나는 데이터 수신이고 하나는 통신이다. 각 레인은 그러므로 4개의 와이어로 구성되어서, 각 레인은 가득찬 두배의 바이트 스트림으로 사용되고, 8비트 바이트 포멧으로 동시에 양뱡향으로 데이터 패킷을 옮긴다. 물리적으로, PCIe 링크는 1,2,4,8,12,16,32 레인을 포함하고, "x" prefix로 표시되어있다. PCIe 카드 또는 커넥터는 8개를 쓰면 x8이라고 한다. 추가적으로, PCIe는 여러 세대를 거치고있고, 다음 미래또한 오고 있다. 그러므로, 카드는 "PCIe gen3 x8"은 3세대의 PCIe이고 8개의 레인을 쓴다는 것이다. 이런 기기는 최대 초당 8기가바이트의 산출량을 가진다. PCIe 상세는 https:/pcisig.com 에서 볼 수 있다.

**controller**는 포트, 버스, 디바이스를 작동하는 전자기기의 모음이다. 그것은 시리얼 포트의 와이어의 시그널을 조절하는 컴퓨터의 단일 칩이다. 반면에, **fibre channel(FC)** 버스 컨트롤러는 간단하지 않다. 왜냐하면 FC 프로토콜은 복잡하고 PCs보다 데이터 센터에서 사용되고 FC 버스 컨트롤러는 보통 분리된 회로 보드 또는 컴퓨터의 버스를 연결하는 **host bus adapter(HBA)**에서 구현된다. 그것은 보통 프로세서, 마이크로코드, FC 프로토콜 메시지 처리를 가능하게하는 몇몇 사적 메모리로 구성되어있다. 몇몇 디바이스는 그들의 자체 내장 컨트롤러를 가진다. 만약 너가 디스크 드라이브를 보면, 너는 한쪽에 붙어있는 회로 보드를 볼 수 있다. 이 보드는 디스크 컨트롤러이다. 그것은 몇몇 종류의 연결(SAS, SATA)을 위한 디스크 프로토콜을 구현한다. 그것 또한 마이크로코드와 배드섹터 매핑, 프리패칭, 버퍼링, 캐싱같은 많은 일을 할수 있는 프로세스가 존재한다.

### 12.2.1 Memory-Mapped I/O

어떻게 프로세서가 I/O 전송을 완료하기 위해서 컨트롤러에 커맨드와 데이터를 줄수 있을까? 간단한 답변은 컨트롤러가 데이터를 위한 레지스터를 가지고 시그널을 컨트롤할 수 있는 것이다. 프로세서는 레지스터의 비트 패턴을 읽고 씀으로서 컨트롤러와 통신한다. 이런 통신이 일어나는 한가지 방식은 I/O 포트 주소로 바이트 또는 워드의 전송을 지정하는 특별한 I/O 명령어의 사용이다. I/O 명령어는 적절한 기기를 선택하고 디바이스 레지스터에 비트를 넣거나 빼기위해서 버스 라인을 촉발시킨다. 대안으로, 디바이스가 **memory mapped I/O**를 지원할수도 있다. 이런 경우에, 디바이스 컨트롤 레지스터들은 프로세서의 주소 공간에 매핑된다. CPU는 그들의 물리 메모리에서 매핑된 위치의 디바이스 컨트롤 레지스터를 읽고 쓰기위해서 표준 데이터 전송 명령어를 이용해서 I/O 요청을 실행한다.

과거에는, PC들은 다른 것들을 제어하기 위해서 몇몇 디바이스와 메모리 맵드 I/O를 제어하는 I/O 명령어를 사용했다. PC의 일반적인 I/O 포트 주소를 사용했다. 그래픽 컨트롤러는 기본 컨트롤 명령어의 I/O 포트를 가졌고, 컨트롤러는 스크린 컨텐츠를 잡기위한 더 큰 메모리 맵드 리전을 가졌다. 스레드는 메모리 맵드 리전에 데이터를 씀으로서 스크린에 아웃풋을 보냈다. 컨트롤러는 이 메모리의 컨텐츠에 기반해서 스크린 이미지를 생성했다. 이 기술은 쓰기 쉽다. 더욱이, 그래픽 메모리에 수백만의 바이트를 쓰는 것은 수백만의 I/O 명령어를 제출하는 것보다 빠르다. 그러므로, 시간을 넘어서, 시스테들은 메모리 맵드 I/O로 이동했다. 오늘날, 대부분의 I/O는 메모리 맵드 I/O를 이용해서 작동한다.

I/O 디바이스는 4개의 레지스터를 포함하는데, 상태, 컨트롤, 데이터-인, 데이터-아웃 레지스터이다.
- **data-in register**는 인풋을 얻기위해서 읽는다.
- **data-out register**는 호스트에 의해서 쓰이고 아웃풋을 보낸다.
- **status register**는 호스트에 의해서 읽을 수 있는 비트를 포함한다. 이런 비트는 현재 커맨드가 완료되었는지, 데이터 인 레지스터에 있는 바이트가 읽을수 있게 준비되었는지, 디바이스 에러가 생겼는지를 알려준다.
- **control register**는 호스트에 의해서 커맨드를 시작하거나 디바이스의 모드를 바꾸기 위해서 쓰인다. 예를 들어서, 시리얼 포트의 컨트롤레지스터 특정 비트는 full-duplex와 half-duplex 통신사이에서 선택하고 다른 비트는 패리티 체킹, 3번째 비트는 워드 길이 7또는8로 세팅되고, 다른 비트들은 시리얼 포트에 의해서 지원되는 속도를 고른다.

데이터 레지스터는 일반적으로 1~4바이트이다. 몇몇 컨트롤러는 데이터 레지스터의 사이즈를 뛰어넘는 컨트롤러의 용량을 확장하기 위해서 인풋 또는 아웃풋 바이트를 저장하는 데이터 FIFO 칩을 가진다. FIFO 칩은 디바이스 호스트가 이런 데이터를 받을 수 있을떄까지의 작은 데이터의 버스트를 가진다.

### 12.2.2 Polling

호스트와 컨트롤러 사이의 상호작용을 위한 완벽한 프로토콜은 복잡하지만, 기본적인 핸드세이킹 개념은 간단하다. 우리는 핸드셰이킹을 예시로 사용하겠다. 2비트가 컨트롤러 호스트 사이를 생산자-소비자 관계로 조정하기 위해서 사용된다고 생각하겠다. 컨트롤러는 `status` 레지스터의 `busy`비트를 통해서 상태를 가르킨다.(비트를 1로 하는 것이 `set`이고 0으로하는 것이 `clear`라고 하겠다.) 컨트롤러는 현재 사용중이면 비지 비트를 세팅하고 다음 커맨드를 받을 준비가 되면 클리어한다. 호스트는 그것의 바램을 커맨드 레지스터의 `command-ready`비트를 통해서 시그널한다. 예를 들어서, 호스트는 포트를 통해서 아웃풋을 쓰고, 핸드세이킹에 따라서 컨트롤러를 조정한다.

1. 호스트는 반족적으로 비트가 클리어 될때까지 `busy` 비트를 읽는다.
2. 호스트는 커맨드 레지스터의 `write`비트를 세팅하고 `data-out` 레지스터에 바이트를 쓴다.
3. 호스트는 `command-ready`비트를 세팅한다.
4. 컨트롤러가 `command-ready` 비트가 세팅된걸 알면, 그것은 `busy`비트를 세팅한다.
5. 컨트롤러는 커맨드 레지스터를 읽고 `write` 커맨드를 본다. 그것은 바이트를 가져오기 위해서 `data-out` 레지스터를 읽고 디바이스에 I/O를 실행한다.
6. 컨트롤러는 `command-ready`비트를 클리어하고 I/O 디바이스가 성공했다고 알리기 위해서 `error`비트를 클리어하고, `busy` 비트를 클리어해서 완료를 알린다.

이 루프가 매 바이트마다 이루어진다.

첫번째 과정에서, 호스트는 **busy-waiting**또는 **polling**이다. 이것은 루프에 있고, `busy` 비트가 클리어될때까지 `status` 레지스터를 읽고 또 읽는다. 만약 컨트롤로와 디바이스가 빠르면, 이 메서드는 합리적인 선택이다. 그러나 만약에 대기가 너무 길어지고, 호스트가 다른 태스크로 변경해야한다. 그때, 호스트는 컨트롤러가 언제 유휴상태인지 알 수 있을까? 몇몇 디바이스에서, 호스트는 반드시 디바이스를 빠르게 서비스하거나, 데이터는 사라진다. 예를 들어서, 데이터가 키보드로부터 시리얼 포트를 따라서 흐르면, 컨트롤러의 작은 버퍼는 넘치고 만약 호스트가 읽은 데이터를 리턴하기전에 너무 오래기다리면 데이터를 잃게될 것이다. 

많은 컴퓨터 아키텍처에서, 3개의 CPU 명령어 사이클이 디바이스를 poll하기에 적합하다. 디바이스 레지스터를 읽고, 상태 비트를 추출하고 만약 0이 아니면 branch한다. 명백히, 기본 폴링 명령은 효율 적이다. 그러나 폴링은 디바이스가 거의 준비되지 않는 상황에서 반복적으로 시도하면 비효율적이고, 디바이스가 서비스를 할 준비가 되었을때 하드웨어 컨트롤러가 CPU에 알리도록 정렬하면 효율적이다. I/O 완료를 기다리면서 CPU를 반복적으로 poll하는 것은 비효율적이다. CPU에게 하드웨어 메커니즘이 준비되었다고 알리는 것이 바로 **interrupt**이다.

### 12.2.3 인터럽트

기본적인 인터럽트 메커니즘은 다음과 같다. CPU 하드웨어가 CPU가 매 명령어 실행마다 반응하는 **interrupt-request line**라고 불리는 와이어를 가진다. CPU가 컨트롤러가 시그널을 인터럽트 리퀘스트 라인에서 가지고 있다고 감지하면, CPU는 상태를 저장하고 고정된 메모리 주소의 **interrupt handler routine**으로 점프한다. 인터럽트 핸들러는 인터럽트의 원인을 결정하고, 필수적인 프로세싱을 실행하고, 상태를 복구하고, 인터럽트 이전에 CPU가 실행하던 상태로 리턴한다. 우리는 디바이스 컨트롤러가 인터럽트 리퀘스트 라인에 시그널을 주장함으로서 인터럽트를 *raise*했다고 말하고, CPU가 인터럽트를 *catch*하고 인터럽트 핸들러를 *dispatch*했고, 핸들러는 디바이스를 서비스함으로 인터럽트를 *clear*했다고 한다.

우리는 이 장에서 인터럽트를 강조하는데 왜냐하면 싱글 유저 현재 시스템은 초당 수백개의 인터럽트를 관리하기 때문이다. 예를 들어서, macOS의 `latency` 명령어을 실행하면, 10초간 23000개의 인터럽트가 나오기도 한다.

기본적인 인터럽트 메커니즘은 CPU가 디바이스 컨트롤러가 서비스할 준비될때 비동기 이벤트에 반응할 수 있게하는 것이다. 현대 운영체제에서, 우리는 더욱 정교한 인터럽트 핸들링 기능이 필요하다.
1. 우리는 중요한 프로세싱 중에 인터럽트 핸들링을 미룰 능력이 필요하다.
2. 우리는 먼저 인터럽트를 raise할때 모든 디바이스 폴링없이 적절한 인터럽트 핸들러를 실행할 효율적인 방법이 필요하다.
3. 우리는 멀티레벨 인터럽트가 필요하고, 그래서 운영체제가 높고 낮은 우선순위의 인터럽트를 구별하고 여러개의 동시 인터럽트가 있을때 적절한 긴급성의 급에 맞게 반응해야한다.
4. 우리는 운영체제의 주의를 직접 받을 명령어를 필요로하고, 이런 활동은 페이지 폴트 또는 0으로 나누기이다. 보았듯이, 이런 태스크는 "traps"로 해결된다.

현대 컴퓨터 하드웨어에서, 이런 기능들은 CPU와 **interrupt-controller hardware**에 의해서 제공된다.

대부분의 CPU들은 2개의 인터럽트 리퀘스트 라인을 가진다. 하나는 **nonmaskable interrupt**이고, 회복불가능한 메모리 에러같은 이벤트를 보존한다. 두번쨰 인터럽트 라인은 **maskable**이다. 그것은 인터럽트 받으면 안되는 중요한 명령어 순서의 실행전에 꺼진다. 

인터러브 메커니즘은 **address**를 수락했고, 특정한 인터럽트 핸들링 루틴이 작은 집합에서 선택된다. 대부분의 아키텍처에서, 이 주소는 **interrupt vector**라고 불리는 테이블의 오프셋이다. 벡터 인터럽트 메커니즘의 목적은 한번의 서비스만으로 가능한 모든 인터럽트의 원인을 결정하는 것이다. 실제로는, 컴퓨터들은 인터럽트 벡터의 주소 원소보다 더 많은 기기를 가지고 있다. 이 문제를 해결하는 방법은 **interrupt chaining**을 사용하는 것이고, 인터럽트 벡터의 각원소는 인터럽트 핸들러의 리스트의 헤드를 가르킨다. 인터럽트가 raise되면, 상응하는 리스트의 핸들러가 하나하나 불리고, 요청을 서비스할떄까지 찾는다. 이 구조를 큰 인터럽트의 오버헤드와 단일 인터럽트 실행의 비효율성에서 타협한다.

인텔 펜티운 프로세스는 8비트의 인터럽트 벡터를 사용한다. 0~31은 nonmaskable이고 페이지 폴트(즉각적인 행동이 필요하다.), 요청을 디버깅(일반적인 명령을 중단하고 디버거 앱으로 점프한다.)같은 다양한 에러 컨디션(시스템 충돌을 일으키는)을 처리하는데 사용된다. 32~255는 maskable이고 device generated 인터럽트같은 것을 목적으로 사용된다.

인터럽트 메커니즘은 또한 **interrupt priority levels**의 시스템을 구현한다. 이런 레벨들은 CPU가 모든 인터럽트를 마스킹하지 않고 낮은 우선순위의 인터럽트를 무시하고 높은 순위의 인터럽트가 낮은 순위의 인터럽트를 선점하게 끔 만든다. 

현대 운영체제는 몇가지 방법으로 인터럽트 메커니즘과 상호작용한다. 부팅 시점에, 운영체제는 어떤 디바이스가 있는지 하드웨어 버스를 탐색하고 인터럽트 벡터안에 상응하는 인터럽트 핸들러를 설치한다. I/O 중에, 다양한 기기 컨트롤러는 그들이 서비스가 준비되면 인터럽트를 raise한다. 이런 인터럽트들은 아웃풋이 완료되었거나, 인풋 에디터가 존재한다거나, 실패가 탐지되었다고 알려준다. 인터럽트 메커니즘은 다양한 범위의 **exceptions**도 핸들하기 위해서 사용하는데, 0으로 나누기, 보호되거나 존재하지 않는 메모리 주소로의 접근, 유저모드에서의 권한 명령어 실행이 있다. 인터럽트를 만드는 이벤트는 공통의 영역이 있다. 그들은 운영체제가 급하고 독립적인 루틴을 실행하게끔 한다.

많은 사례에서의 인터럽트 핸들링이 시간과 리소스에 제한적이고 그러므로 구현하기 복잡하기 때문에, 시스템은 인터럽트 관리를 **first-level interrupt handler(FLIH)**와 **second-level interrupt handler(SLIH)**로 나눈다. FLIH는 컨텍스트 스위치, 상태 저장, 핸들링 명령어의 큐잉를 실행하는데 비해서, 분리된 스케쥴 SLIH는 요청된 명령의 핸들링을 실행한다. 

운영체제는 인터럽트를 위한 좋은 방법을 가진다. 예를 들어서, 많은 운영체제들은 가상 메모리 페이징을 위한 인터럽트 메커니즘을 사용한다. 페이지 폴트는 인터럽트가 발생시키는 예외이다. 인터럽트는 현재의 프로세스를 중단하고 커널의 페이지 폴트 핸들러로 점프한다. 이 핸들러는 프로세스의 상태를 저장하고, 프로세스를 대기큐에 배치하고 페이지 캐시 관리를 실행하고, 페이지를 fetch하기 위해서 I/O 명령어를 스케쥴하고 명령어를 재시작하기위해서 다른 프로세스를 스케쥴하고 인터럽트로부터 리턴한다.

다른 예시는 시스템 콜의 구현에서 찾아진다. 보통, 프로그램은 시스템 콜을 발행하기 위해서 라이브러리를 사용한다. 라이브러리 루틴은 앱에 의해서 주어진 어규먼트를 체크하고, 커널에 어규먼트를 보내기위해서 데이터 구조를 빌드하고 **software interrupt**또는 **trap**이라고 불리는 특별한 명령어를 실행한다. 이 명령어는 원하는 커널 서비스를 구별하는 연산자를 가진다. 프로세스가 트랩 명령어를 실행하면, 인터럽트 하드웨어는 유저 코드의 상태를 저장하고, 커널 모드로 전환하고, 커널 루틴또는 요청된 서비스를 구현한 스레드를 실행한다. 트랩은 디바이스 인터럽트에 할당된 우선순위보다 상대적으로 낮은 우선순위를 가진다. 앱의 측면에서 시스템콜을 실행하는 것은 그것의 FIFO 큐가 오버플로하고 데이터를 잃기전에 디바이스 컨트롤러를 서비스하는 것보다 덜 급하다.

인터럽트는 또한 커널에서 컨트롤의 플로우를 관리하는데 사용된다. 예를 들어서, 프로세싱이 디스크 읽기 완료를 필요로한다고 가정하겠다. 한가지 단계는 커널 공간에서 유저 버퍼로 데이터를 복사한다. 이 복사는 시간은 잡아먹지만 급하지 않고, 그것은 다른 높은 순위 인터럽트 핸들링을 블락해서는 안된다. 다른 단계는 다른 기다리는 I/O를 시작하는 것이다. 이 단계는 높은 우선도를 가질 수 있다. 만약 디스크가 효율적으로 사용되면, 우리는 다음 I/O를 이전 것이 완료되기전에 다음 I/O를 시작할 필요가 있다. 결과적으로, 인터럽트의 쌍은 디스크 리드를 완료하는 커널 코드를 구현해야한다. 높은 우선도 핸들러는 I/O 상태를 저장하고, 디바이스 인터럽트를 클리어하고, 대기하는 I/O를 시작하고 일을 완료하기 위해서 낮은 우선순위 인터럽트를 인터럽트한다. 후에, CPU가 높은 우선도 일을 차지하지 않으면, 낮은 우선도 인터럽트가 발생한다. 상응하는 핸들러는 데이터를 커널 버퍼의 데이터를 앱 공간에 복사함으로서 유저 레벨 I/O를 완료하고 레디 큐에 앱을 위치하기 위해서 스케쥴러를 부른다.

스레드 커널 구조 또한 다중 인터럽트 우선도를 구현하고 앞의 인터럽트 핸들링을 강조한다. 우리는 이런 포인트를 솔라리스 커널로 설명하겠다. 솔라리스에서, 인터럽트 핸들러는 커널 스레드로서 실행된다. 높은 순위의 스케쥴링 범위는 이런 스레드들에 보존된다. 이런 우선도는 인터럽트 핸들러가 앱 코드와 커널 하우스 키핑을 넘어서게 하고 인터럽트 핸들러 사이에 상대적인 우선도를 구현한다. 우선도는 솔라리스 스레드 스케쥴러가 높은 우선 순위의 인터럽트 핸들러가 선점하고, 스레드 구현은 멀티 프로세서 하드웨어가 다양한 인터럽트 핸들러를 가능하게 한다. 리눅스와 윈도우는 20장과 21장에서 설명하겠다.

요약하면, 인터럽트는 운영체제를 통해서 비동기 이벤트를 핸들하고 커널에서의 감시자 모드 루틴을 트랩하기 위해서 사용된다. 가장 급한 일을 먼저 처리하게 하기위해서, 현대 컴퓨터들은 인터럽트 우선도의 시스템을 사용한다. 디바이스 컨트롤러, 하드웨어 폴트, 시스템 콜들은 커널 루틴을 실행하기 위해서 인터럽트를 raise한다. 인터럽트들이 시간에 민감한 프로세싱에 사용되기에, 효율적인 인터럽트 핸들링이 좋은 시스템 성능을 위해서 필요하다. 인터럽트 기반 I/O는 이제 폴링보다 더 일반적이고, 폴링이 사용되는 것은 높은 산출량 I/O에서만 쓰인다. 때떄로 두개는 공존한다. 몇몇 디바이스 드라이버는 I/O 속도가 느릴때 인터럽트를 사용하고 그것의 속도가 폴링이 빠르고 효율적일때 폴링으로 바꾼다.

### 12.2.4 Direct Memory Access

디스크 드라이브 같이 큰 전송을 하는 디바이스에서, 상태 비트를 감시하고 컨트롤러 레지스터에 한바이트씩 먹이는 비싼 범용 목적 프로세서를 사용하는 것은 낭비이다. 이런 프로세스는 **programmed I/O(PIO)**라고 불린다. 컴퓨터는 메인 CPU에 이 일을 **direct memory access**라는 특별 목적 프로세서가 이 일을 덜어가는 PIO로 무겁게하는 것을 피한다. DMA 전송을 시작하기 위해서, 호스트는 DMA 커맨드 블락을 메모리에 쓴다. 블락은 전송의 출처 포인터, 전송의 목적지 포인터, 전송될 바이트의 수를 포함한다. 커맨드 블락은 연속적이지 않은 목적 주소와 출처의 리스트를 포함함으로서 더욱 복잡해질 수 있다. **scatter-gather** 메서드는 DMA 커맨드를 이용해서 다중 전송을 허용할 수 있다. CPU는 DMA 컨트롤러에 커맨드 블록의 주소를 쓴다. DMA 컨트롤러는 메모리 버스를 직접 운영하기 위해서 진행하고, 메인 CPU의 도움 없이 전송 위해서 주소를 버스에 두는 것을 실행한다. 간단한 DMA 컨트롤러는 모든 현대 컴퓨터의 표준 구성요소이고, 스마트폰부터 메인프레임까지이다.

커널 주소공간에 타겟 주소를 두는 것이 가장 직관적이다. 만약 유저 스페이스이면, 유저는 전송중에 공간의 컨텐츠를 수정하고 몇몇 데이터를 잃게된다. 스레드 접근을 위해서 유저 공간에 DMA 전송 데이터를 얻으면, 두번쨰 복사 명령어, 커널 메모리에서 유저 메모리로의 시간이 필요하다. 시간을 지나, 운영체제는 I/O 전송을 디바이스와 유저 주소 공간이 직접적으로 실행하기 위한 메모리 매핑을 사용하기 시작했다.

DMA 컨트롤러와 디바이스 컨트롤러 간의 핸드쉐이킹은 **DMA-request**와 **DMA acknowledge**라고 불리는 한쌍의 와이어를 통해서 실행된다. 디바이스 컨트롤러는 데이터의 워드가 전송이 준비되면 DMA 리퀘스트 와이어에 시그널을 보낸다. 시그널은 DMA 컨트롤러가 메모리 버스를 설치하게 야기하고, 메모리 주소 와이어에 원하는 주소를 두고, DMA acknowledge 와이어에 시그널을 둔다. 디바이스 컨트롤러가 DMA acknowlege signal을 받으면, 그것은 데이터의 워드에 메모리를 전송하고 DMA 리퀘스트 시그널을 삭제한다.

전체 전송이 완료되면, DMA 컨트롤러는 CPU를 인터럽트한다. DMA 컨트롤러가 메모리 버스를 장악하면, CPU는 일시적으로 메인 메모리 접근으로부터 방지되고, 그것은 여전히 그것의 캐시 아이템에 접근이 가능하다. 비록 이 **cycle stealing**이 CPU 연산을 느리게하고, 데이터 전송 작업을 DMA에 넘기는 것은 전체 시스템의 성능을 향상시킨다. 몇몇 컴퓨터 아키텍처는 DMA를 위해서 물리 메모리 주소를 사용하지만, 다른 것들은 **direct virtual memory access(DVMA)**를 사용한다. 가상 주소가 물리 주소로 번역되게끔 사용한다. DVMA는 CPU의 개입 또는 메인메모리의 사용 없이 메모리 맵드 디바이스에서 전송을 실행한다.

보호받는 모드 커널에서, 운영체제는 보통 프로세스가 디바이스 커맨드를 직접 발생시키는 것을 방지한다. 이 규율은 접근 제어 침범으로부터 데이터를 보호하고 시스템 충돌을 야기하는 디바이스 컨트롤러의 잘못된 사용으로부터 시스템을 보호한다. 대신에, 운영 체제는 충분히 특권을 가진 프로세스가 하드웨어의 낮은 레벨 명령어 접근을 사용하게하는 함수를 전한다. 메모리 보호 없는 커널에서, 프로세스들은 디바이스 컨트롤러에 직접 접근이 가능하다. 이 직접 접근은 높은 성능을 성취하는데 쓰이고, 그것은 커널 통신, 컨텍스트 스위치, 커널 소프트웨어의 레이어를 회피하게 한다. 불행히도, 그것은 시스템 보안과 안정성에서 방해된다. 일반적인 범용 운영체제는 메모리와 디바이스를 보호하고 그래서 시스템은 악성 앱으로부터 에러를 방지한다.

### 12.2.5 I/O 하드웨어 요약

비록 I/O의 하드웨어는 전자공학적인 하드웨어 디자인의 디테일 레벨에서는 복잡하지만, 우리가 방금 표현한 컨셉들은 운영체제의 많은 I/O기능을 이해하는데 충분하다. 메인 컨셉을 다시보자.

- A bus
- A controller
- An I/O port and its registers
- The handshaking realtionship between the host and a device controller
- The offloading of this work to a DMA controller for large trasfers

우리는 디바이스 컨트롤러와 호스트 사이의 핸드세이킹 예제를 이 절에서 보았다. 실제로는, 존재하는 기기의 다양헝이 운영체제 구현에 문제를 발생시킨다. 각 종류의 디바이스는 그것의 능력, 컨트롤 비트 정의, 호스트와 상호작용할 프로토콜을 가지고 그들은 모두 다르다. 어떻게 운영체제를 새롭게 쓰이지 않고도 새로운 디바이스를 붙이도록 디자인할 수 있을까? 그리고 디바이스가 다양하면, 어떻게 운영체제는 간단하고, 일정한 I/O 인터페이스를 앱에게 제공할까?

## 12.3 Application I/O interface

이 절에서, 우리는 운영체제가 I/O 디바이스를 표준적이고 일정하게 다루게 해주는 인터페이스와 기술을 구조화하겠다. 우리는, 예를 들어서, 어떻게 앱이 디스크의 파일을 어떤 디스크인지 모르고 열고 새로운 디스크 또는 다른 디바이스가 운영체제의 혼란없이 컴퓨터에 추가되는지 보겠다.

다른 복잡한 소프트웨어 엔지니어링 문제처럼, 이 문제는 추상화, 캡슐화, 소프트웨어 레이어링을 포함한다. 특별히, 우리는 몇몇 일반적인 종류를 구별함으로서 I/O 디바이스의 자세한 차이를 추상화하겠다. 각 일반적인 종류들은 **interface**라는 표준 함수의 집합을 통해서 접근된다. 차이점은 특정기기에 맞춤형이지만 표준적인 인터페이스를 전달하는 디바이스 드라이버라고 불리는 커널 모듈에 캡슐화된 점이다. 

디바이스 드라이버 층의 목적은 커널의 I/O 서브시스템으로부터 드라이버 컨트롤러의 차이점을 숨기는 것이고, I/O 시스템콜은 디바이스의 행동을 앱으로부터 하드웨어의 차이를 숨기는 몇몇 제너릭 클래스에 캡슐화하는 것이다. I/O 서브시스템을 하드웨어로부터 독립적으로 만드는 것은 운영체제 개발자의 일을 단순화한다. 그것은 또한 하드웨어 생산자에게도 이득을 준다. 그들은 새로운 디바이스를 존재하는 호스트 컨트롤러 인터페이스에 적용가능하게 디자인하거나, 그들은 새로운 하드웨어를 인터페이스하는 디바이스 드라이버를 유명한 운영체제에 쓴다. 그러므로, 우리는 컴퓨터에 새로운 주변장치를 운영체제 판매자가 지원 코드를 개발하는 것을 기다릴 필요가 없어진다.

하드웨어 생산자에게 불행히도, 운영체제의 각 타입은 그들의 자체적인 디바이스 드라이버 인터페이스를 가진다. 주어진 디바이스는 다양한 디바이스 드라이버를 가지고 있다. 디바이스는 여러가지 차원을 가진다.
|aspect|variation|example|
|---|---|---|
|data-transfer mode| character, block | terminal, disk|
|access method| sequential, random| modem, CD-ROM|
|transfer schedule | synchronous, asynchronous| tape, keyboard|
|sharing | dedicated, sharable|tape, keyboard|
|device speed| latency, seek time, transfer rate, delay between operations | |
|I/O direction | read/write only, read-write | CD-ROM, graphics controller, dist|

- **Character-stream or block** character-stream 디바이스는 전송을 바이트씩하고, block 디바이스는 바이트의 블럭을 단위로 전송한다.
- **Sequential or random access** sequential 디바이스는 디바이스의 결정된 고정 순서대로 데이터를 전송하고, random access 디바이스는 존재하는 데이터 저장소의 위치를 찾아서 디바이스를 수행한다.
- **Synchronous or asynchronous** synchronous 디바이스는 데이터 전송을 예측된 반응시간안에 시스템의 다른 측면과 조정하며 수행한다. Asynchronous 디바이스는 시스템의 다른 측면과 조정하지 않기 때문에 비정규적이거나 예측불가한 반응시간을 가진다.
- **Sharable or dedicated** sharable 디바이스는 여러 프로세스 또는 스레드에 의해서 동시에 사용되지만, dedicated 디바이스는 그렇지 않다.
- **Speed of operation** 디바이스 속도는 초당 몇 바이트에서 기가바이트까지 다양하다.
- **Read-write, read only, write once** 몇몇 디바이스는 인풋과 아웃풋을 동시에 시행하지만, 다른 것들은 오직 한방향 데이터 통신을 지원한다. 몇몇은 쓰기 후에 데이터를 수정하게 허용하지만, 다른 것들은 한번쓰이면 읽기만 가능하다.

앱 접근의 목적상, 이런 차이점들은 운영체제의 뒤에 숨어있고, 디바이스들은 몇몇 관습적인 타입으로 그룹지어있다. 디바이스 접근의 결과적인 스타일을 유용하다고 알려지고 널리 적용이 가능하다. 비록 정확한 시스템콜을 운영체제마다 다르지만, 디바이스 카테고리들은 꽤 일반적이다. 주된 접근 관습은 block I/O, character-stream I/O, memory-mapped file access와 network sockets을 포함한다. 운영체제는 또한 TOD 클락과 타이머 같은 몇몇 추가적인 디바이스를 엑세스하는 특별한 시스템콜을 제공한다. 몇몇 운영체제는 graphical display, video, audio devices같은 시스템콜의 집합을 제공한다.

대부분의 운영체제들은 임의의 커맨드를 앱에서 디바이스로 투명하게 전달하는 **escape**(or **back door**)를 가진다. UNIX에서 시스템콜 `ioctl()`이다. `ioctl()` 시스템 콜은 앱이 새로운 시스템콜의 개발 필요 없이 디바이스 드라이버에 의해 구현된 모든 기능에 접근하게 허용하는 것이다. `ioctl()` 시스템 콜은 3가지 아규먼트를 가진다. 첫번째는 앱을 드라이버로 연결하는 드라이버에 의해서 관리되는 하드웨어 디바이스를 언급하는 디바이스 식별자이다. 두번쨰는 드라이버에 구현된 커맨드를 선택하는 정수이다. 세번쨰는 앱과 드라이버가 필요한 컨트롤 정보 또는 데이터를 가능하게하는 메모리의 임의의 데이터 구조 포인터이다.

LINUX와 UNIX의 디바이스 식별자는 "major와 minor" 디바이스 번호의 튜플이다. major number는 디바이스 종류이고, minor는 디바이스의 인스턴스이다. 예를 들어서, 시스템의 SSD 디바이스를 생각해보자. 만약 다음과 같은 커맨드를 입력했다. `% ls -l /dev/sda*` 그러면 다음과 같은 출력이 나온다.

```cpp
brw-rw---- root disk 8, 0 Mar 16 09 :18 /dev/sda
brw-rw---- root disk 8, 1 Mar 16 09 :18 /dev/sda1
brw-rw---- root disk 8, 2 Mar 16 09 :18 /dev/sda2
brw-rw---- root disk 8, 3 Mar 16 09 :18 /dev/sda3
```
여기서 8은 major 디바이스 번호이다. 운영체제는 이 정보를 사용해서 디바이스 드라이브의 적절한 I/O 요청을 찾아낸다. minor 번호는 0,1,2,3이고 디바이스의 인스턴스를 뜻하고, 요청의 정확한 기기를 I/O로의 요청을 선택하도록 돕는다.

### 12.3.1 Block and Character Devices

**block device interface**는 디스크 드라이브와 다른 블록 기반 디바이스에 접근하는 모든 필요한 것을 캡쳐한다. 디바이스는 `read()`와 `write()`같은 커맨드를 알아야한다. 만약 그것이 랜덤 엑세스 디바이스면, 그것은 또한 어떤 블럭이 다음에 전송될지 `seek()`을 통해서 예측한다. 앱들은 보통 디바이스를 통해서 파일 시스템 인터페이스에 접근한다. 우리는 `read()`, `write()`와 `seek()`이 블록 저장소 디바이스의 본질적인 행동을 캡쳐하는 것을 알수 있고, 그래서 앱들은 이런 디바이스 사이에서 낮은 레벨 차이를 보호한다.

운영체제 자체적으로, 데이터 베이스 관리 시스템 같은 특별한 앱들은 단순한 선형 블럭의 행렬로 블럭 디바이스에 접근하느 것을 선호한다. 접근의 모드는 가끔 **raw I/O**라고 불린다. 만약 앱이 그것의 버퍼링을 수행중이면, 파일 시스템을 사용하는 것은 추가적이고, 불필요한 버퍼링이 될 것이다. 이와 같이, 만약 앱이 그것의 블럭 또는 리전의 락을 제공하면, 운영체제 락 서비스는 최소일떄는 불필요하고 최악일때는 모순적일 것이다. 이런 충돌을 피하기 위해서, raw-디바이스 접근은 디바이스의 제어권을 앱에 직접 건내주고, 운영체제가 한발 물러서게 한다. 불행히도, 이 디바이스에 대해서 어떠한 운영체제도 실행하지 않게 된다. 일반적인 것이되는 타협은 운영체제 시스템으로 하여금 버퍼링과 락을 비활성화해서 파일에 명령어의 모드를 허용한다. 유닉스 세계에서는, 이것은 **direct I/O**라고 불린다.

메모리 맵드 파일 접근은 블록 디바이스 드라이브의 꼭대기에 존재한다. 읽기와 쓰기 명령어를 제공하기보다는, 메모리 맵드 인터페이스는 메인 메모리의 바이트 행렬을 통해서 디스크 저장소에 접근하게 한다. 파일을 메모리에 매핑하는 시스템콜은 파일의 복사본을 포함하는 가상메모리 주소를 리턴한다. 실제 데이터 전송은 오직 메모리 이미지에 접근을 만족할때 일어난다. 전송이 디맨드 페이지 가상 메모리 접근과 같은 메커니즘에 의해서 핸들되기 때문에, 메모리 맵드 I/O는 효율적이다. 메모리 매핑은 또한 프로그래머에게 간편한데, 메모리 맵드 파일로의 접근은 메모리로부터 읽기와 메모리에 쓰기가 간단하다. 가상 메모리를 제공하는 운영체제는 커널 서비스의 매핑 인터페이스를 일반적으로 사용한다. 예를 들어서, 프로그램을 실행하려면, 운영체제는 메모리에 실행가능한 파일을 매핑하고 실행가능한 파일의 엔트리 주소로 컨트롤을 전송해야한다. 매핑 인터페이스는 보통 커널 엑세스가 디스크에서 공간을 스왑할때 자주 쓰인다.

키보드는 **character stream interface**로 접근하는 디바이스의 예시다. 이 인터페이스의 기본 시스템콜은 앱이 한 단어를 `get()` 또는 `put()`하게 하는 것이다. 이 인터페이스의 상단에, 라이브러리는 버퍼링과 수정 서비스를 가진 line at a time 접근을 가진다.(우리가 백스페이스를 치면, 인풋 스트림의 이전 글자는 제거된다.) 이 접근의 스타일은 동시에 인풋을 만드는 키보드, 마우스, 모뎀 같은 인풋 디바이스에 편리하다. 즉, 앱에 의해서 정확하게 예측이 될 수가 없다. 이 엑세스 스타일은 또한 프린터, 오디오보드 같은 바이트의 리니어 스트림 개념에 맞는 아웃풋 기기에도 좋다.

### 12.3.2 Network Devices.

네트워크 I/O의 성능과 주소 방식이 disk I/O에 따라서 다르기 때문에, 대부분의 운영체제는 네트워크 I/O 인터페이스에서 디스크에 사용된 `read()`-`write()`-`seek()` 인터페이스와 다르게 제공한다. 많은 운영체제에서 보이는 한가지 인터페이스는 네트워크 **socket** 인터페이스이다.

전기를 위한 wall socket을 생각해보자. 전기 기구는 플러그인될 수 있다. 비유하면, 소켓 인터페이스의 시스템 콜은 앱이 소켓을 만들고, 먼 주소에 로컬 소켓을 연결하고, 멀리 있는 앱에 로컬 소켓을 플러그해서 듣게하고, 연결을 통해서 패킷을 주고 받는다. 네트워크 서버의 구현을 지원하기 위해서, 소켓은 소켓의 집합을 관리하는 `select()`라는 함수를 제공한다. `select()`콜은 어떤 소켓이 수신 대기중인 패킷을 가지고 있는지, 어떤 소켓이 보낼 패킷을 허용할 공간을 가지고 있는지에 대한 정보를 리턴한다. `select()`의 사용은 네트워크 I/O에 어쩌면 필요한 폴링과 비지 웨이팅을 제거한다. 이런 함수들은 존재하는 네트워크 하드웨어와 프로토콜 스택에 사용하는 분산 앱의 생성을 가능하게하는 네트워크의 주요한 행동을 캡슐화한다.

IPC와 네트워크 통신에 많은 접근이 구현되었다. 예를 들어서, 윈도우는 네트워크 인터페이스 카드에 한가지 인터페이스를 제공하고 두번쨰 인터페이스는 네트워크 프로토콜을 제공한다. 유닉스에서는, 네트워크 기술의 검증을 했고, 우리는 half duplex pipe, full duplex FIFOs, full duplex STREAMS, 메시지큐, 소켓등이 있다. 

### 12.3.3 Clock and Timers

대부분의 컴퓨터들은 3가지 기본 함수를 제공하는 하드웨어 클락과 타이머를 가진다.
- 현재 시간을 제공한다.
- 경과한 시간을 제공한다.
- 시간 T에 명령어 X를 실행할 타이머를 세팅한다.

이런 함수들은 운영체제와 시간 정보가 필요한 앱들에 의해서 많이 사용된다. 불행히도, 이런 기능을 구현하는 함수들은 운영체제에서 일정하게 구현되지 않았다.

하드웨어는 경과한 시간을 측정하고 **programmable interval timer**라고 불리는 명령어를 실행한다. 그것은 특정 시간을 기다리도록 세팅하고 인터럽트를 생성하고, 그것은 한번또는 여러번 프로세스가 주기적인 인터럽트를 발생하게 세팅한다. 스케쥴러는 이 메커니즘을 프로세스의 타임슬라이스에 선점하는 인터럽트를 생성하기 위해서 이용한다. 디스크 I/O 서브시스템은 디스크에 더러운 캐시버퍼를 주기적인 플러시하기 위해서 사용하고, 네트워크 서브시스템은 네트워크 혼잡또는 실패로 인해서 너무 느린 일을 취소하기 위해서 사용한다. 운영체제는 유저 시간에 유저 프로세스에 대한 인터페이스를 제공한다. 운영체제는 가상 클락을 시뮬레이팅 함으로서 타이머 하드웨어의 채널보다 많은 수의 타이머 리퀘스트를 지원한다. 그렇게 하기 위해서, 커널(또는 타이머 디바이스 드라이버)는 유저 리퀘스트와 루틴에 의해서 원하는 인터럽트의 리스트를 유지하고, earliest time first order로 정렬한다. 그것은 타이머를 가장 빠른 시간에 맞춘다. 타이머가 인터럽트하면, 커널은 요청자에게 시그널을 보내고 다음 이른 시간을 타이머에 재 로딩한다.

컴퓨터는 다양한 목적으로 사용되는 클락 하드웨어를 가진다. 현대 PC들은 10메가 헤르츠의 범위의 속도로 작동하는 **high performance event timer(HPET)**를 포함한다. 그것은 그들이 가진 값과 HPET의 값을 매칭하는 한번또는 여러번 실행되도록 세팅가능한 비교측정기를 가진다. 트리거는 인터럽트를 발생시키고, 운영체제의 클락 관리 루틴은 타이머가 무엇이었는지 결정하고 어떤 액션을 취할지 결정한다. 트리거의 정확도는 타이머의 해상도에 제한되고, 가상 클락의 유지 오버헤드도 포함된다. 더 나아가, 만약 타이머 틱들이 시스템 tod 클락을 유지하는데 사용하면, 시스템 클락은 표류한다. Drift는 **network time protocol**같은 컴퓨터의 클락을 원자 시계 레벨과 거의 일치하게 지연 시간을 계산하는데 사용하는 목적의 프로토콜을 이용해서 수정이된다. 대부분의 컴퓨터에서, 하드웨어 클락은 높은 주기 카운터에의해서 건설된다. 몇몇 컴퓨터들에서, 카운터의 값은 디바이스 레지스터로부터 읽을수 있고, 카운터는 높은 해상도 클락으로 고려된다. 비록 클락은 인터럽트를 만들지 않지만 그것은 시간 간격의 정확한 측정을 제공한다.

### 12.3.4 Nonblocking and Asynchronous I/

시스템 콜 인터페이스의 다른 측면은 블록킹 I/O와 논블록킹 I/O의 선택도 있다. 앱이 **blocking** 시스템콜을 발행하면, 불린 스레드의 실행은 정지한다. 스레드는 운영체제의 실행 큐에서 대기 큐로 이동된다. 시스템 콜이 완료된 후에, 스레드는 다시 실행 큐로 돌아가고, 실행을 재시작할 수 있다. 그것이 실행을 다시하면, 그것은 시스템 콜에 의해서 리턴된 값을 받는다. I/O 디바이스에 의해서 실시된 물리 행동은 보통 비동기적인데 그들은 가변적이거나 예측불가능한 시간을 가진다. 그럼에도 불구하고, 운영체제는 앱 인터페이스에 블록킹 시스템 콜을 앱 인터페이스에게 제공하는데, 왜냐하면 블록킹 앱 코드는 논 블록킹 앱 코드보다 쓰기가 편하기 때문이다.

몇몇 유저레벨 프로세스는 **nonblocking** I/O를 필요로한다. 한가지 예시는 스크린에 데이터를 프로세싱하고 출력하는 동안 키보드와 마우스 인풋을 받는 유저 인터페이스이다. 다른 예시는 디스플레이의 아웃풋을 압축해제하고 디스플레잉하는 동안 동시에 디스크의 파일로부터 프레임을 읽는 비디오 앱이다.

앱 라이터는 I/O와 실행을 오버랩하는 방법은 멀티스레드 앱을 쓰는 것이다. 몇몇 스레드들은 블록킹 시스템 콜을 수행하는 동안, 다른 것들은 실행을 계속한다. 몇몇 운영체제는 논 블록킹 I/O 시스템 콜을 제공한다. 논블록킹 콜은 스레드의 실행을 확장된 시간만큼 멈추지 않는다. 대신에, 그것은 빠르게 리턴되고, 얼마나 많은 바이트가 통신되었는지만 리턴 값으로 가르킨다.

논블록킹 시스템콜의 대안은 비동기 시스템 콜이다. 비동기 콜은 I/O가 완료되기를 기다리지 않고 즉시 리턴된다. 스레드는 그것의 코드를 계속 실행한다. 미래 시간에 완료된 I/O는 스레드의 주소공간에 있는 몇몇 변수의 세팅 또는 시그널의 트리거 또는 스레드의 선형 컨트롤 플로우 밖에서 발생한 콜백 또는 소프트웨어 인터럽트를 통해서 스레드와 통신한다. 논블록킹과 비동기 시스템콜의 차이는 논블록킹 `read()`는 요청된 전체 바이트의 일부 데이터가 어떻게든 존재하면 즉시 리턴하는 것이다.  비동기 `read()` 콜은 그것의 전체가 수행되지만, 어느 정도의 시간이 걸릴 전송을 요청한다.

비동기 활동은 현대 운영체제에서 일어난다. 자주, 그들은 유저 또는 앱에 노출되지 않고 오히려 운영체제의 명령어에 포함되어있다. 2차 저장 기기와 네트워크 I/O는 좋은 예시이다. 자연스럽게, 앱이 네트워크 송신 리퀘스트 또는 저장 디바이스 쓰기 요청을 발행하면, 운영체제는 요청을 적고 I/O를 버퍼하고, 앱에 리턴한다. 가능할때, 전체 시스템 성능을 최적화하기 위해서, 운영체제는 요청을 완료한다. 만약 시스템 실패가 내부에서 일어나면, 앱은 어떠한 "in-flight" 요청을 잃어버린다. 그러므로, 운영체제는 리퀘스트를 버퍼할 최대 리미트를 걸어둔다. 유닉스의 몇몇 버전은 그들의 2차 저장소 버퍼를 30초마다 비운다. 예를 들어, 모든 요청은 그것의 발생 30초안에 플러시된다. 시스템은 앱에게 몇몇 버퍼(2차 저장소 버퍼)의 플러시를 요청할 방법을 제공해서 버퍼 플러시 간격을 기다릴 필요 없이 2차 저장소의 데이터를 강제할 수 있다. 앱 내부의 데이터 일관성은 I/O 요청을 디바이스에 발행하기전에 그것의 버퍼로부터 데이터를 읽는 커널에 의해서 유지된다. 이것으로 현재 쓰이지 않은 데이터가 요청하는 리더에게 리턴되지 않게 보장한다. 같은 파일에 I/O를 수행하는 멀티 스레드는 커널이 그것의 I/O를 어떻게 구현하느냐에 따라서 일관된 데이터를 받지 못할수 있다. 이런 사오항에서, 스레드는 락킹 프로토콜을 사용할 필요가 있다. 몇몇 I/O 요청은 즉시 사용될 필요가 있고, 그래서 I/O 요청은 주어진 요청 I/O을 가르킬 방법을 가지거나 특별한 기기의 I/O는 반드시 동기적으로 작동해야한다.

논블록킹 행동의 좋은 예시는 네트워크 소켓의 `select()` 시스템콜이다. 이 시스템콜은 최대 대기 시간을 특정하는 아규먼트를 가진다. 0으로 설정함으로서, 스레드는 블락킹 없이 네트워크 활동을 폴할 수 있다. 그러나 `select()`의 사용은 추가적인 오버헤드를 만드는데, 왜냐하면 `select()` 콜은 오직 I/O가 가능한지만 체크하기 때문이다. 데이터 전송을 위해서, `select()` 후에는 반드시 몇몇 종류의 `read()`와 `write()` 커맨드를 따라야한다. 맥에서 사용하는 이 접근의 변형은, 블록킹 다중 리드 콜이다. 그것은 한 시스템콜에서 여러개의 디바이스의 원하는 리드를 명시하고 그들중 하나가 완료되는 순간에 리턴한다.

### 12.3.5 Vectored I/O

몇몇 운영체제는 I/O의 주요한 변형을 앱 인터페이스를 통해서 제공한다. **Vectored I/O**는 한개의 시스템 콜이 여러장소에 포함된 다중 I/O 명령어를 수행하게 허용한다. 예를 들어서, UNIX `readv` 시스템 콜은 다중 버퍼의 벡터를 수락하고 소스부터 벡터까지 읽거나 벡터부터 목적지까지 쓴다. 같은 전송이 다양한 개별적인 시스템콜의 실행으로 생기지만, 이 **scatter-gather** 메서드는 여러가지 이유로 유용하다.

다중 분리 버퍼는 한가지 시스템콜로 컨텍스트 스위칭과 시스템 콜 오버헤드 피하면서 통신될 그들의 컨텐츠를 가질수 있다. 벡터 I/O 없이, 데이터는 우선 정렬된 큰 버퍼로 전송될 필요가 있고 그후에 비효율적으로 전송된다. 추가적으로, 몇몇 scatter-gather의 버전은 원자성을 제공하는데, 모든 I/O가 인터럽트 없이 실행되게 보장한다.(만약 다른 스레드가 이런 버퍼 속에서 I/O를 실행중이면 데이터의 부패를 피한다.) 가능하면, 프로그래머들은 scatter-gather I/O 기능을 산출량을 늘리고 시스템 오버헤드를 줄일 때 사용한다.

## 12.4 커널 I/O 서브 시스템

커널은 I/O와 관련된 많은 서비스를 제공한다. 스케쥴링, 버퍼링, 캐싱, 스풀링, 디바이스 보존, 에러핸들링이 커널의 I/O 서브시스템에 의해서 제공되고 하드웨어와 디바이스 드라이버 인프라스트럭처위에 올려진다. I/O 서브시스템은 또한 잘못된 프로세스와 악성 유저로부터 그것을 보호한다.

### 12.4.1 I/O 스케쥴링

I/O 요청의 집합을 스케쥴하는 것은 그들을 실행할 좋은 순서를 결정하는 것이다. 시스템 콜을 가끔 발행하는 앱의 순서는 최고의 선택이다. 스케쥴링은 전체 시스템 성능을 높일수 있고, 프로세스 사이에 디바이스 접근을 공평하게 나눌 수 있고 I/O가 완료되는 평균 대기시간을 줄일 수 있다. 여기에 설명할 간단한 예제가 있다. 디스크 암이 디스크의 시작 근처에 있고 3개의 앱이 디스크에 블록킹 리드 콜을 요청한다고 가정하겠다. 앱 1은 디스크의 끝에 있는 블럭을 요청하고 앱 2는 시작점 근처, 앱 3은 디스크의 중앙이라고 가정하겠다. 운영체제는 디스크 암이 2, 3, 1순으로 해결함으로서 디스크 암의 이동 거리를 줄일 수 있다. 서비스의 순서를 이렇게 재배열하는 것이 I/O 스케쥴링의 정수이다.

운영체제 개발자는 각 디바이스의 리퀘스트의 대기 큐를 유지함으로서 스케쥴링을 구현한다. 앱이 블록킹 I/O 시스템 콜을 요청하면, 요청은 디바이스의 큐에 배치된다. I/O 스케쥴러는 전체 시스템 효율을 증가시키고 앱의 평균 반응시간을 줄이기위해서 큐의 순서를 재배열한다. 운영체제는 공평하게 적용할 수 있고 모든 앱이 유달리 낮은 서비스를 제공하지 않을 수 잇다. 또는 그것은 지연에 예민한 요청에 우선순위를 부여할 수 있다. 예를 들어서, 가상 메모리 서브시스템의 요청은 앱 요청으로부터 우선도를 뺏을 수 있다. 몇몇 디스크 I/O 스케쥴링 알고리즘은 11.2에 설명되어있다.

커널이 비동기 I/O를 지원하면, 그것은 동시에 많은 I/O 요청을 트래킹하고 있어야한다. 이 목적을 위해서, 운영체제는 **device-status** 테이블에 대기 큐를 부착할 수 있어야한다. 커널은 각 I/O 기기의 엔트리를 포함하는 테이블을 관리한다. 각 테이블 엔트리는 디바이스의 타입, 주소, 상태(작동하지 않음, 유휴, 바쁨)를 가르킨다. 만약 디바이스가 요청시에 바쁘면, 리퀘스트의 타입과 다른 파라미터들이 그 디바이스의 테이블 엔트리에 저장된다.

I/O 명령어를 스케쥴링하는 것은 I/O 서브시스템이 컴퓨터의 효율성을 향샹시키는 한가지 방법이다. 다른 방법은 메인 메모리 또는 버퍼링, 캐싱, 스풀링을 통해서 저장소 계보에 저장 공간을 사용하는 것이다.

### 12.4.2 버퍼링

**buffer**는 디바이스와 앱 사이 또는 두 디바이스 사이에 전송된 데이터를 저장하는 메모리 공간이다. 버퍼링은 3가지 이유로 사용된다. 한가지는 데이터 스트림의 소비자와 생산자 사이의 속도 불일치를 대처하기 위해서이다. 예를 들어서, 인터넷을 통해 받은 파일이 SSD에 저장된다. 네트워크의 속도는 드라이브보다 몇 천배는 느릴 것이다. 그래서 버퍼는 네트워크로부터 받은 바이트를 메인메모리에 누적하기 위해서 저장된다. 데이터의 전체 버퍼가 도착하면, 버퍼는 한개의 명령어안에 드라이브에 쓰일 수 있다. 드라이브 쓰기가 즉각적이지 않고 네트워크 인터페이스는 여전히 다가오는 추가 정보를 저장하기때문에, 두개의 버퍼가 사용된다. 네트워크가 첫번쨰 버퍼를 비우면, 드라이브 쓰기가 요청된다. 그리고 나서 네트워크는 첫번째 버퍼가 저장소에 쓰는 동안 두번쨰 버퍼를 채우기 시작한다. 네트워크가 두번쨰 버퍼를 채우는 동안, 첫번째의 드라이브 쓰기는 완료되어야지만, 네트워크는 첫번째 버퍼로 돌아가고 두번쨰 버퍼가 드라이브를 쓴다. **double buffering**은 생산자의 데이터를 소비자로부터 분리하고, 그러므로 그들 사이에 타이밍 필요를 완화한다. 

버퍼링의 두번째 사용은 다른 전송 사이즈를 가진 디바이스에 적용을 제공하는 것이다. 이런 차이는 버퍼가 단편화와 메시지의 조합으로 사용되는 컴퓨터 네트워킹에 일반적이다. 보내는 쪽에서는, 큰 메시지가 작은 네트워크 패킷으로 단편화된다. 패킷은 네트워크를 넘어서 보내지고, 받는 쪽은 그들을 소스의 데이터의 이미지를 형성하기 위해서 버퍼를 재조립한다.

세번째 버퍼의 사용은 앱 I/O에 복사 시맨틱을 지원하는 것이다. "copy semantics"의 의미는 다음 예제로 명료해질 것이다. 앱이 디스크에 쓰기를 원하는 데이터의 버퍼를 가졌다고 가정하겠다. 그것은 `write()` 시스템 콜을 부르고, 버퍼의 포인터를 제공하고 쓸 바이트의 수를 특정한다. 시스템 콜이 리턴한 후에, 만약 앱이 버퍼의 컨텐츠를 변경하면 무슨 일이 일어나는가? **copy semantics**가 적용되면, 디스크에 쓰인 데이터의 버전은 앱 시스템 콜이 실행된 시간의 버전으로 보장되고, 앱이 가진 버퍼의 다음 변경으로 부터 독립적이다. 운영체제가 `write()`시스템 콜에 copy semantics를 보장하는 간단한 방법은 앱에 컨트롤을 리턴하기 전에 앱 데이터를 커널 버퍼에 복사하는 것이다. 디스크 쓰기는 커널버퍼로부터 적용되고, 앱 버퍼의 변경은 아무런 영향을 주지 않는다. 커널 버퍼와 앱 데이터 공간 사이의 데이터 복사는 이 명령의 오버헤드에도 불구하고 운영체제에서 일반적이다. 같은 효과는 가상메모리 매핑의 사용과 COW 페이지 보호에서도 효율적으로 쓰인다.

### 12.4.3 캐싱

**cache**는 데이터의 카피를 보존하는 빠른 메모리의 리전이다. 캐시된 카피로의 접근은 원본으로의 접근보다 효율적이다. 예를 들어서, 현재 실행중인 프로세스의 명령어는 디스크에 저장되고, 물리 메모리에 캐시되고, CPU의 2차와 1차 캐시에 다시 복사된다. 버퍼와 캐시의 차이는 버퍼는 오직 존재하는 아이템의 복사를 가지고, 반면에 캐시는 어디에나 존재하는 아이템의 빠른 저장소에 카피를 가지는 것이다.

캐싱과 버퍼링은 다른 기능을 가지는데, 그러나 메모리의 리전은 두 목적을 위해서 쓸 수 있다. 예를 들어서, 카피 시맨틱을 보존하고 디스크 I/O의 효율적인 스케쥴링을 하려면, 운영체제는 메인메모리에 디스크 데이터를 저장하기 위해서 버퍼를 사용한다. 이런 버퍼들은 앱들이 공유하거나 빠르게 쓰이고 다시 읽히는 파일의 I/O 효율성을 높이기 위해서 캐시로 사용된다. 커널이 파일 I/O 요청을 받으면, 커널은 파일의 영역이 이미 메인메모리에 있는지 확인하기 위해서 버퍼 캐시에 접근한다. 만약 그렇다면, 물리 디스크 I/O는 회피되거나 무시된다. 또한, 디스크 쓰기는 버퍼 캐시에 누적되고, 큰 전송이 효율적인 쓰기 스케쥴을 위해서 모인다. 이 쓰기 지연의 전략은 I/O 효율성을 높이고, 원거리 파일 접근의 컨텍스트(19.8절)에서 다룬다.

### 12.4.4 Spooling and Device Reservation

**spool**은 프린터 같은 데이터 스트림의 방해를 허용하지 않는 디바이스 아웃풋을 잡는 버퍼이다. 비록 프린터는 한번에 한가지 일을 할 수 있지만, 몇가지 앱은 프린트가 그들의 아웃풋을 그들의 아웃풋이 합쳐지지 않는 선에서 동시에 출력하기를 원한다. 운영체제는 이 문제를 프린터의 아웃풋을 가로챔으로서 해결했다. 각 앱의 아웃풋은 분리된 2차 저장소 파일로 스풀된다. 앱이 프린팅을 끝내면, 스풀링 시스템은 상응하는 프린터에 넣을 아웃풋 스풀 파일을 큐에 넣는다. 스풀링 시스템은 큐 스풀 파일을 프린터에 한번에 복사한다. 몇몇 운영체제에서, 스풀링은 시스템 다이몬 프로세스에 의해서 관리된다. 다른 곳에서는, 그것은 커널 스레드에 의해서 핸들링된다. 두 케이스에서, 운영체제는 유저와 시스템 관리자가 큐를 볼수있게 허용하는 컨트롤 인터페이스를 제공하고, 프린트되기 이전에 원하지 않는 작업을 제거할 수 있고, 프린터가 실행되는 중에 중지가 가능하다.

테이프 드라이브와 프린터 같은 몇몇 디바이스는 동시에 작동하는 앱의 I/O 요청을 유용하게 섞지 못한다. 스풀링은 운영체제가 동시성 아웃풋을 조정하는 한가지 방법이다. 동시성 디바이스 접근을 처리하는 다른 방법은 조정을 위해서 explicit한 기능을 제공한다. 몇몇 운영체제(VMS)는 프로세스가 idle인 디바이스를 할당하고 사용이 끝나면 해제하는 것을 허용함으로서 독점적인 디바이스 접근을 제공한다. 다른 운영체제는 이런 디바이스에 하나의 오픈파일만 가능하게 강제한다. 많은 운영체제는 프로세스가 독점적인 접근 사이에서 조정할 수 있는 기능을 제공한다. 예를 들어서, 윈도우는 디바이스 오브젝트가 가용할때까지 기다리는 시스템 콜을 제공한다. 그것은 또한 다른 동시 스레드에게 허락되는 접근의 타입을 정의하는 `OpenFile()` 시스템콜을 파라미터로 가진다.  이런 시스템에서, 데드락을 피하는 것은 앱에게 달려있다.

### 12.4.5 에러 핸들링

보호되는 메모리를 사용하는 운영체제는 다양한 종류의 하드웨어와 앱 에러로부터 방어할수 있고, 그래서 완벽한 시스템 실패는 각 마이너한 기계적 오작동의 결과는 잘 일어나지 않는다. 디바이스와 I/O 통신은 많은 방법으로 실패하는데, 네트워크가 오버로드되는 일시적인 이유 또는 디스크 컨트롤러가 결함을 일으키는 영구적인 이유가 있다. 운영체제는 일시적인 실패는 효과적으로 보완할 수 있다. 예를 들어서, 디스크의 `read()` 실패는 `read()` 재시도를 일으키고, 네트워크 `send()`에러는 만약 프로토콜이 그렇게 명세했다면, `resend()`를 일으킨다. 불행히도, 만약 중요한 컴포넌트가 영구적인 실패를 일으키면, 운영체제는 복구가 힘들다.

일반적인 룰로서, I/O 시스템 콜은 콜의 성공 또는 실패를 알리는 상태에 대한 1bit의 정보를 리턴한다. 유닉스 운영체제에서, `errno`라는 추가적인 정수 변수가 리턴되는데, 일반적인 실패(범위 초과, 배드 포인터, 파일 열기 실패)를 담은 100개 가량의 값을 가진다. 반면에, 몇몇 하드웨어는 아주 상세한 에러 정보를 제공하는데, 비록 많은 운영체제가 이 정보를 앱에게 전달하게 디자인되지는 않았다. 예를 들어서, SCSI 장비의 실패는 SCSI 프로토콜에서 상세의 3가지 레벨로 리포트된다. **sense key**는 하드웨어 에러 또는 불가능한 요청을 뜻하는 일반적인 실패이다. **additional sense code**는 커맨드 파라미터 오류 또는 셀프 테스트 실패 같은 실패의 카테고리를 말한다. **additional sense code qualifier**는 어떤 커맨드 파라미터가 에러인지, 어던 하드웨어 서브시스템이 그것의 셀프 테스트중 실패했는지를 알리는 실패의 상세한 정보를 준다. 더 나아가서, 많은 SCSI 디바이스는 호스트에 의해서 요청될 수 있는 에러 로그 정보를 유지하는 내부의 페이지를 가진다.

### 12.4.6 I/O 보호

에러는 보호의 문제와 긴밀히 연관되어 있다. 유저 프로세스는 실수로 또는 고의적으로 불가능한 I/O 명령어를 발행하는 것을 시도함으로서 시스템의 일반적인 명령어를 방해하도록 시도할 수 있다. 우리는 이런 방해가 시스템에서 작동하지 않도록 다양한 메커니즘을 사용해야한다.

유저로부터 이상한 I/O를 실행하는 것을 막기위해서, 우리는 모든 I/O 명령어를 권한이 필요한 명령어로 정의한다. 그러므로, 유저들은 I/O 명령어를 직접 발행하지 못하고, 그들은 반드시 운영체제를 거쳐야한다. I/O를 실행하기 위해서, 유저 프로그램은 운영체제가 I/O를 실행하게 하려면 시스템 콜을 실행해야한다. 모니터 모드에서 실행중인 운영체제는 요청이 적절한지 확인하고, 그렇다면 I/O 요청을 실행한다. 운영체제는 그 값을 유저에게 리턴한다.

추가적으로, 메모리 맵드와 I/O 포트 메모리 공간은 반드시 메모리 보호 시스템에 의해서 유저의 접근으로부터 보호되어야한다. 커널은 모든 유저 접근을 단순히 거절하지 않는다. 대부분의 그래픽 게임과 비디오 편집, 실행 소프트웨어는 그래픽의 성능을 높이기 위해서 메모리 맵드 그래픽 컨트롤러에 직접 접근한다. 커널은 이런 경우에 한 프로세스가 한번에 할당할 수 있는 그래픽 메모리의 구역을 허용하기 위한 락킹 메커니즘을 제공한다.

### 12.4.7 Kernel Data Structures

커널은 I/O 컴포넌트의 사용에 대한 정보를 계속 알릴 필요가 있다. 그것은 커널 데이터 구조의 다양성을 통해서 그 일을 하고있는데, 이런 오픈 파일 테이블 구조는 14.1절에서 언급하겠다. 커널은 네트워크 연결을 추적하거나, 캐릭터 디바이스 통신과 다른 I/O 활동을 위해서 비슷한 구조를 사용한다.

유닉스는 유저 파일, raw 디바이스와 프로세스의 주소공간같은 다양한 엔티티에 접근하기 위해서 파일 시스템 접근을 제공한다. 비록 각각의 엔티티가 `read()`명렬어를 지원하지만, semantics는 다른다. 예를 들어서, 유저 파일을 읽기 위해서, 커널은 어떤 디스크 I/O를 실행할지 결정하기 전에 버퍼 캐시를 둘러볼 필요가 있다. raw 디스크를 읽기위해서, 커널은 요청된 사이즈가 디스크의 섹터 사이즈의 배수이고 섹터 바운더리 내에서 할당되었는지를 확인할 필요가 있다. 프로세스 이미지를 읽기위해서, 그것은 메모리로부터 데이터를 복사할 필요가 있다. 유닉스는 이런 차이를 객체 지향 기술을 이용해서 일정한 구조안에 둠으로서 이런 차이를 캡슐화했다. 오픈 파일레코드는 파일의 타입에 맞게, 적절한 루틴으로의 포인터를 가지는 디스패치 테이블을 포함한다.

몇몇 운영체제는 객체 지향 메서드를 더욱 확장적으로 사용한다. 윈도우는 I/O를 위해서 메시지 패싱 구현을 사용한다. I/O 요청은 커널에서 I/O매니저에서 디바이스 드라이브로 보내지는 메시지로 변환된다. 각각의 과정에서 메시지의 컨텐츠는 바뀔수 있다. 아웃풋으로는, 메시지는 쓰일 데이터를 포함할 것이다. 인풋으로는, 메시지는 받을 데티어의 버퍼를 가진다. 메시지 패싱 접근은 공유 데이터 구조를 사용하는 절차지향 기술과는 반대로 오버헤드를 추가시키지만, I/O 시스템의 디자인과 구조를 단순화하고 유연성을 추가한다.

### 12.4.8 파워 관리

데이터 센터에 있는 컴퓨터는 파워 사용의 이슈로 부터 지워져서 멀리 있는 것처럼 보이지만, 전력 값이 비싸지고 세계가 지구 온난화의 문제에 대해서 고민할 수록, 데이터 센터들은 고려의 대상이 되고 증가된 효율성의 타겟이 된다. 전기 사용은 열을 만들고, 컴퓨터 컴포넌트는 높은 온도떄문에 실패할 수 있기에, 쿨링이 필요하다. 현대의 데이터 센터를 쿨링하는 것은 실제 장비들이 파워를 하는 것의 두배이다. 데이터 센터 파워 최적화에는 다양한 접근이 시도되었고, 데이터 센터의 온도를 태양 패널또는 강의 물같은 것으로 식히려고 했다.

운영체제는 전력 소모에서 주요한 일을 맡을 수 있다. 클라우드 컴퓨팅 환경에서, 부하를 프로세싱하는 것은 모든 유저 프로세스를 시스템으로부터 대피시키는 모니터링과 관리 툴을 통해서 조정이 가능하다. 이런 시스템을 휴식시키고 그들의 사용이 필요할때까지 끄는 것이다. 운영체제는 그것의 부하를 분석하고 만약 충분히 낮고 하드웨어가 사용가능하면, CPU와 외부 I/O 디바이스를 종료한다.

CPU 코어들은 시스템 로드가 그들을 필요로하지않을때 종료되고 로드가 증가하고 스레드의 큐를 처리할 더욱 많은 코어가 필요하면 다시 시작한다. 그들의 상태는, 종료시에 저장되어야하고 재시작시에 복구되어야한다. 이 기능은 서버에서 필요한데 왜냐하면 서버로 가득찬 데이터 센터는 엄청난 양의 전기를 사용하고, 필요없는 코어를 비활성화하는 것은 전기 사용량을 줄이는데 도움이 된다.

모바일 컴퓨팅에서, 파워 관리는 운영체제의 주요한 우선순위를 가진다. 파워를 최소한 사용하고 배터리 수명을 최대로하는 것은 디바이스의 유용성을 증가시키고 다른 대안 디바이스와 경쟁력을 갖추게한다. 오늘날의 모바일 디바이스는 어제의 하이엔드 데스크탑의 기능을 제공한다. 만족스러운 배터리 수명을 제공하기 위해서 현대 모바일 운영체제는 파워 관리를 주요 기능으로서 설정한다. 안드로이드의 모바일 시스템이 유명해지게한 3가지 유명한 기능을 살펴보겠다. power collapse, component-level power management와 wakelocks이다.

Power collapse는 디바이스를 깊은 수면상태로 두는 기능이다. 디바이스는 완전히 꺼진 상태보다 아주 약간의 파워만을 사용하고, 그것은 버튼 클릭 같은 외부의 자극에 빠르게 반응할 수 있다. Power collapse는 스크린, 스피커, I/O 서브시스템 같은 많은 개별적인 컴포넌트를 끔으로서 성취했다. 운영체제는 CPU를 그것의 가장 낮은 슬립 상태로 둔다. 현대 ARM CPU는 일반적인 로드에서 수백의 밀리와트를 소비하는데 가장 낮은 수면 상태에서는 손으로 셀수있는 밀리와트만을 사용한다. 이런 상태에서, CPU는 유휴상태이지만, 그것은 인터럽트를 받을 수 있고, 일어나고, 이전의 활동을 빠르게 재시작한다. 그러므로, 유휴중인 안드로이드 폰은 아주 작은 파워를 사용중이고, 전화가 오면 바로 그것의 삶을 시작한다.

어떻게 안드로이드는 폰의 개별적인 컴포넌트를 끌 수 있을까? 어떻게 그것은 플래스 저장소가 전원을 꺼도 안전한 것을 알고 어떻게 전체 I/O 서브시스템을 파워다운 하기전에 알고 있을까? 정답은 컴포넌트 레벨 파워 관리인데, 컴포넌트 사이의 관계를 이해하고 각 컴포넌트가 사용중인지 아는 구조이기 때문이다. 컴포넌트 사이의 관계를 이해하기 위해서, 안드로이드는 핸드폰의 물리 디바이스 위상 배치를 대표하는 디바이스 트리를 짓는다. 예를 들어서, 플래시와 USB 저장소가 CPU에 연결된 시스템의 버스노드인 I/O 서브시스템의 서브노드이다. 사용하는지를 알기위해서, 각 컴포넌트는 그것의 디바이스 드라이버와 연결되고, 드라이버는 컴포넌트가 사용중인지 추적한다. 예를 들어서, 만약 플래시를 기다리는 I/O 또는 앱이 오디오 서브시스템의 레퍼런스를 열었다. 이 정보들과 함께, 안드로이드는 폰의 개별적인 컴포넌트의 파워 관리를 할 수 있다. 만약 컴포넌트가 사용중이 아니라면, 그것은 꺼진다. 만약 시스템 버스 위의 모든 컴포넌트가 사용중이 아니면, 시스템 버스는 꺼진다. 그리고 만약 전체 디바이스 트리의 모든 컴포넌트가 사용중이 아니면, 시스템은 power collapse에 들어간다.

이런 기술들과 함께, 안드로이드는 그것의 파워 소비를 공격적으로 관리한다. 마지막 솔루션의 조각을 놓치고 있다. 앱이 시스템때문에 power collpase에 들어가는 것으로부터 방지하는 것이다. 우리가 게임을 플레이하고, 비디오를 보고, 웹페이지가 열리는 것을 기다린다고 가정하자. Wakelocks가 이 기능을 가능하게한다. 앱은 필요한만큼 wakelocks를 획득하고 해제한다. 앱이 wakelock을 가지는 동안, 커널은 시스템으로부터 power collapse에 들어가는 것을 방지한다. 예를 들어서, 안드로이드 마켓이 앱을 업데이트할때, 그것은 시스템이 업데이트가 완료되기전까지 멈추지 않게 보장한다. 완료되면, 안드로이드 마켓은 wakelock을 해제하고, 시스템이 power collapse에 들어가게 허용한다.

파워 관리는 디바이스 관리에 기반되어있고, 우리가 말한 것보다 더욱 복잡하다. 부트 시간에, 펌웨어 시스템은 펌웨어 시스템은 시스템 하드웨어를 분석하고 RAM의 디바이스 트리를 생성한다. 커널은 디바이스 드라이버를 로드하고 디바이스를 관리하는데 쓴다. 많은 추가적인 활동이 디바이스가 관리되도록 유지된다. 실행중인 시스템으로부터 디바이스를 추가하고 제거하는 것과 디바이스 상태를 이해하고 바꾸는 것과 전력 소모를 모두 포함한다. 현대의 범용 컴퓨터는 **advanced configuration and power interface(ACPI)**라는 펌웨어 코드를 사용해서, 하드웨어의 측면을 관리한다. ACPI는 많은 표준으로 산업 표준이다. 그것은 커널에 의해서 불러지는 디바이스 상태 발견/관리, 디바이스 에러 관리, 파워 관리 같은 루틴을 실행한다. 예를 들어서, 커널이 디바이스를 작업거부해야하면, 그것은 디바이스와 얘기하는 ACPI 루틴을 부르는 디바이스 드라이버를 호출한다.

### 12.4.9 커널 I/O 서브시스템 요약

요약하면, I/O 서브시스템은 앱과 커널의 다른 부분에 가용한 서비스의 집합을 확장한다. I/O 서브시스템은 이런 단계들을 감독한다.

- 파일과 디바이스의 이름 공간의 관리
- 파일과 디바이스로 접근 관리
- 명령어 관리
- 파일 시스템 공간 할당
- 디바이스 할당
- 버퍼링, 캐싱, 스풀링
- I/O 스케쥴링
- 디바이스 상태 모니터링, 에러 핸들링, 실패 복구
- 디바이스 드라이버 설정과 초기화
- I/O 디바이스의 파워관리

I/O 서브시스템보다 상위인 디바이스 접근은 디바이스 드라이버에 의해서 제공되는 동일한 인터페이스를 통한다.

## 12.5 Transforming I/O Requests to Hardware Operations

우리는 디바이스 드라이버와 디바이스 컨트롤러간의 handshaking에 대해서 설명했지만, 우리는 어떻게 운영체제가 네트워크 와이어또는 특정 디스크 섹터를 앱에 연결하는지는 설명하지 않았다. 디스크로부터 파일 읽는 예시를 들겠다. 앱은 파일 이름으로 데이터를 언급한다. 디스크에서, 파일 시스템은 파일의 공간 할당 위치를 알려주는 파일 시스템 디렉토리를 통해서 파일 이름으로부터 찾는다. FAT(오늘 날에도 쓰이는 상대적으로 단순한 운영체제이자 파일 시스템)의 MS-DOS에서, 이름은 파일 엑세스 테이블의 엔트리를 가르키는 숫자를 가르키고, 테이블 엔트리는 파일에 어떤 디스크 블럭이 할당되었는지 말한다. 유닉스에서는, 이름이 인노드 넘버를 가르키고, 상응하는 인노드는 공간 할당 정보를 포함한다. 그러나 파일 이름에서 디스크 컨트롤러로의 연결이 어떻게 만들어질까?(하드웨어 포트 또는 메모리 맵드 컨트롤러 레지스터)

한가지 메서드는 FAT의 MS-DOS를 이용한다. 콜론 앞의 MS-DOS 파일이름의 첫부분은 특정 하드웨어 디바이스를 가르킨다. 예를 들어서, C:에서 운영체제의 주요 하드 디스크를 의미한다. C:은 디바이스 테이블을 통해서 특정 포트 주소에 매핑되어있다. 콜론 분리때문에, 디바이스 이름 공간은 파일 시스템 이름 공간과 분리되어있다. 이 분리가 운영체제가 각 디바이스아 연결된 추가적인 기능을 수행하는데 편의를 준다. 이 분리는 운영체제가 각 디바이스와 관련된 추가적인 기능을 하는데 편하게한다. 예를 들어서, 프린터에 쓰일 어떤 파일도 쉽게 스풀링을 실행하게 해준다.

만약, 대신에, 디바이스 이름 공간이 일반적인 파일 시스템 이름 공간에 포함되면, 일반적인 파일 시스템 이름 서비스는 자동으로 제공된다. 만약 파일 시스템이 소유권과 접근 컨트롤을 모든 파일 이름에 제공하면, 디바이스는 권한과 접근 컨트롤을 얻는다. 파일이 디바이스에 저장되기 때문에, 인터페이스가 I/O 시스템에 접근을 제공한다. 이름들은 디바이스에 접근하거나 디바이스에 저장된 파일에 접근할때 사용된다.

유닉스는 일반적인 파일 시스템 이름 공간에서 디바이스 이름을 대표한다. 콜론 분리를 가진 MS-DOS FAT 파일이름과 다르게, 유닉스 경로 이름은 디바이스 부분의 병확한 구분이 없다. 실제로, 경로 이름의 어떠한 부분도 디바이스의 이름이 아니다. 유닉스는 특정 디바이스 이름이 경로의 접두사와 연관된 **mount table**을 가진다. 경로 이름을 해결하기 위해서 유닉스는 가장 긴 매칭되는 접두사를 마운트 테이블에서 찾는다. 마운트 테이블에서 상응하는 엔트리는 디바이스 이름을 제공한다. 디바이스 이름은 파일 시스템 이름 공간에서 이름의 형태를 가진다. 유닉스가 파일 시스템 디렉토리 구조에서 이 이름을 살펴볼때, 그것은 아이노드 이름이 아니라 <major, minor> 디바이스 번호를 찾는다. 메이저 디바이스 번호는 I/O를 핸들하기위해서 반드시 불러야하는 디바이스 드라이버를 식별한다. 마이너 디바이스 번호는 디바이스 테이블를 가르키기 위해서 디바이스 드라이버에게 전해진다. 상응하는 디바이스 테이블 엔트리는 포트 주소 또는 디바이스 컨트롤러의 메모리 맵드 주소를 주어야한다.

현대의 운영체제는 요청과 물리 디바이스 컨트롤러 사이의 경로의 룩업 테이블의 다양한 단계로부터 큰 유연성을 얻었다. 앱과 드라이버 사이의 요청을 전달하는 메커니즘은 일반적이다. 그러므로, 우리는 새로운 장치와 드라이버를 커널에 다시 컴파일 하지않고 컴퓨터에 사용할 수 있다. 실제로, 몇몇 운영체제는 수요에 맞게 디바이스 드라이버를 로딩하는 능력이 있다. 부트할때는, 시스템은 어떤 디바이스가 있는지 하드웨어 버스를 탐색해서 결정한다. 그것은 그리고 필요한 드라이버를 즉시 또는 I/O 리퀘스트가 처음 발생하면 로딩한다. 부트후에 추가된 디바이스들은 그들이 발생시킨 에러(적절한 인터럽트 핸들러 없이 인터럽트가 발생한 것)에 의해서 감지되고, 커널로 하여금 디바이스 상세를 살펴보고 적절한 디바이스 드라이버를 동적으로 로딩하게 한다. 물론, 동적인 로딩(and unloading)은 정적인 로딩보다 더욱 복잡하고, 더욱 복접한 커널 알고리즘을 필요로하고, 디바이스 구조 락킹, 에러 핸들링 등이 필요하다.

우리는 다음으로 블록킹 리드 요청의 생명주기를 보겠다. I/O 명령어는 여러개의 CPU 사이클을 소비하는 많은 과정을 필요로한다.

1. 프로세스가 블록킹 `read()` 시스템콜을 이전에 열렸었던 파일의 파일 식별자에게 발행한다. 
2. 커널의 시스템 콜 코드가 정확성을 위해서 파라미터를 체크한다. 인풋의 경우에는, 만약 데이터가 버퍼 캐시에 존재하면, 데이터는 프로세스에 리턴되고, I/O 요청은 완료된다.
3. 다른 경우에는, 물리 I/O가 실행되어야한다. 프로세스는 실행 큐로부터 제거되고 디바이스의 대기 큐에 두고, I/O 요청이 스케쥴된다. 마침내, I/O 서브시스템이 디바이스 드라이버에 보내졌다. 운영체제에 따라서, 요청은 서브루틴 콜 또는 커널 내부 메시지를 통해서 전송된다.
4. 디바이스 드라이버는 데이터를 받기위해서 커널 버퍼를 할당하고 I/O를 스케쥴한다. 마침내, 드라이버는 디바이스 컨트롤 레지스터에 씀으로서 디바이스 컨트롤러에 커맨드를 보낸다. 
5. 디바이스 컨트롤러는 디바이스 하드웨어를 데이터 전송을 위해서 사용한다.
6. 드라이버는 상태와 데이터를 poll하거나 커널 메모리에 DMA 전송을 준비한다. 우리는 전송이 전송이 완료되면 인터럽트를 발생시키는 DMA 컨트롤러에 의해서 관리된다고 가정하겠다.
7. 정확한 인터럽트 핸들러는 인터럽트 벡터 테이블을 통해서 받아지고, 필요한 데이터를 저장하고, 디바이스 드라이버에 신호를 보내고, 인터럽트로부터 리턴한다.
8. 디바이스 드라이버는 시그널을 받고, 어떤 I/O 요청이 완료되었는지 결정하고, 요청의 상태가 어떤지 판단하고, 리퀘스트가 완료되었다고 커널 I/O 서브시스템에 시그널한다.
9. 커널은 데이터를 전송하거나 요청한 프로세스의 주소 공간 코드로 리턴하고 프로세스를 대기 큐에서 레디큐로 이동시킨다.
10. 프로세스를 레디 큐에 이동시키는 것은 프로세스를 unblocks한다. 스케쥴러가 CPU에 프로세스를 할당하면, 프로세스는 시스템콜의 완료에 실행을 시작한다.

## 12.6 STREAMS

유닉스 시스템 V는 앱이 드라이버 코드의 파이프라인을 동적으로 합치는 **STREAMS**라는 흥미로운 메커니즘을 가졌다. 스트림은 디바이스 드라이버와 유저 레벨 프로세스 사이의 full-duplex(양방향) 연결이다. 그것은 유저프로세스와 접속하는 **stream head**, 디바이스를 컨트롤하는 **driver end**와 스트림 헤드와 드라이버 엔드 사이의 없거나  많은 **stream modules**로 구성되어 있다. 각각의 컴포넌트는 read queue와 write queue를 가진다. 메시지 패싱은 큐사이에서 데이터를 통신하는데 사용된다. 

모듈은 STREAMS 프로세싱의 기능을 제공한다. 그들은 `ioctl()` 시스템 콜의 사용으로 스트림위에 *pushed*된다. 예를 들어서, 프로세스는 스트림을 통해서 USB 디바이스를 열고 인풋 수정을 핸들하는 모듈을 푸쉬한다. 메시지들이 근접한 모듈 사이에서 교환되기 때문에, 한 모듈의 큐는 근접한 큐에 오버플로할 수 있다. 이런 상황을 방지하기 위해서, 큐는 **flow control**을 지원한다. 플로 컨트롤이 없다면, 큐는 모든 메시지를 수용하고 근접한 모듈의 큐에 그들을 버퍼링하지 않고 즉시 보낸다. 플로 컨트롤을 지원하는 큐는 메시지를 버퍼하고 충분한 버퍼 공간 없이는 메시지를 수용하지 않는다. 이 과정은 근접한 모듈의 큐 사이에서 메시지 컨트롤의 교환을 포함한다.

유저 프로세스는 `write()`또는 `putmsg()`를 이용해서 디바이스에 데이터를 쓴다. `write()` 시스템 콜은 스트림에 raw 데이터를 쓰는 반면에, `putmsg()`는 유저 프로세스가 메시지를 특정하는 것을 허용한다. 유저 프로세스에 의해서 사용되는 시스템 콜에 상관없이, 스트림 헤드는 데이터를 메시지로 복사하고 다음 모듈에 들어갈 큐에 전달한다. 이 메시지의 복사는 메시지가 드라이버에 복사되고 디바이스에 복사될때까지 진행된다. 비슷하게, 유저 프로세스는 `read()` 또는 `getmsg()` 시스템 콜을 이용해서 스트림 헤드로부터 데이터를 읽는다. 만약 `read()`가 사용되면, 스트림 헤드는 근접한 큐로부터 메시지를 읽고 프로세스에 ordinary data(구조화되지 않은 바이트 스트림)을 리턴한다. 만약 `getmsg()`가 사용되면, 메시지는 프로세스에 리턴된다.

STREAMS I/O는 유저 프로세스가 스트림헤드와 통신할때를 제외하고 비동기(nonblocking)이다. 스트림에 쓸때, 유저 프로세스는 블럭되는데, 다음 큐는 플로우 컨트롤을 메시지를 복사할 공간이 있을때까지 사용할 것이다. 이와 같이, 유저 프로세스는 데이터가 존재할때 스트림으로 부터 읽는 동안 블락한다.

말했듯이, 드라이버 엔드는 스트림 헤드와 모듈 처럼 read와 write 큐를 가진다. 그러나, 드라이버 엔드는 반드시 프레임이 네트워크로부터 읽을 준비가 된 것과 같은 인터럽트에 반응해야한다. 다음 큐로 메시지를 복사 못할때 블럭되는 스트림 헤드와는 달리, 드라이버 엔드는 반드시 오는 데이터들을 핸들할 수 있어야한다. 드라이버는 반드시 플로 컨트롤을 지원해야한다. 그러나 만약 디바이스의 버퍼가 가득차면, 디바이스는 오는 메시지를 버림으로서 보존한다. 인풋 버퍼가 가득찬 네트워크 카드를 생각해보자. 네트워크 카드는 반드시 오는 메시지를 저장할 공간이 충분해질떄까지 오는 메시지를 떨궈내야한다.

STREAMS를 사용하는 이득은 그것이 디바이스 드라이버와 네트워크 프로토콜을 모듈러하고 증가하는 방향으로 제공하는 프레임워크를 제공하기 때문이다. 모듈은 다른 디바이스와 다른 스트림에 의해서 사용될 수 있다. 예를 들어서, 네트워킹 모듈은 이더넷 네트워크 카드와 802.11 무선 네트워크 카드에 의해서 사용될 수 있다. 더 나아가, 캐릭터 디바이스 I/O를 구조화되지 않은 바이트 스트림으로 대하는 것보다, STREAMS는 메시지 바운더리를 지원하고 모듈간의 통신할 때의 정보를 컨트롤한다. 대부분의 UNIX 변형들은 STREAMS를 제공하고, 그것은 프로토콜과 디바이스 트라이버를 쓰는 메서드로서 선호된다. 예를 들어서 시스템V 유닉스와 솔라리스는 소켓 메커니즘을 STREAMS를 이용해서 사용한다.

## 12.7 성능

I/O는 시스템 성능에서 중요한 요소이다. 그것은 디바이스 드라이버 코드를 실행시키고 프로세스가 블록하고 논블럭하기에 공평하고 효율적으로 스케쥴하는데 CPU에 큰 부담을 준다. 이에 발생하는 컨텍스트 스위치는 CPU와 그것의 하드웨어 캐시에 부담을 준다. I/O는 또한 커널의 인터럽트 해늘링 메커니즘의 비효율성에 노출된다. 추가적으로, I/O는 컨트롤러와 물리 메모리 사이에서 데이터를 카피하는 동안 그리고 다시 커널 버퍼와 앱 데이터 공간사이에서 복사하는 동안 메모리 버스를 다운로드한다.  이런 수요를 적절하게 복사하는 것은 컴퓨터 구조의 주요한 고민중 하나이다.

비록 현대 컴퓨터들은 초당 수천개의 인터럽트를 핸들하지만, 인터럽트 핸들링은 상대적으로 비싼 태스크이다. 각 인터럽트는 시스템으로 하여금 상태를 변화시키고, 인터럽트 핸들러리를 실행시키고, 다시 상태를 복구시킨다. 프로그램된 I/O는 만약 비지 웨이팅을 지낸 사이클의 수가 과하지 않으면 인터럽트 기반 I/O보다 더욱 효율적이다. I/O 완료는 프로세스를 언블록하고, 컨텍스트 스위치의 가득찬 오버헤드로 이끈다. 

네트워크 트래픽은 높은 컨텍스트 스위치 비율을 차지한다. 예를 들어서, 한 머신으로부터 원격 로그인을 하겠다. 로컬 머신에서 입력된 각 문자는 반드시 원격 머신으로 전송되어야한다. 로컬 머신에서, 문자는 입력된다. 키보드 인터럽트가 발생한다. 그리고 문자는 인터럽트 핸들러를 통해 디바이스 드라이버, 커널 그리고 유저 프로세스에 전달된다. 유저 프로세스는 네트워크 I/O는 문자를 원격 머신에 전송하기 위해서 네트워크 I/O 시스템콜을 발행한다. 문자는 로컬 커널로 들어가고 네트워크 패킷을 구축해주는 네트워크 레이어를 통해 네트워크 디바이스 드라이버로 들어간다. 인터럽트는 커널을 통해 백업을 건내주고  네트워크 I/O 시스템콜이 완료되도록 한다. 

이제, 원격 시스템의 네트워크 하드웨어는 패킷을 받고, 인터럽트는 생성된다. 문자는 네트워크 프로토콜로부터 해제되고 적절한 네트워크 다이몬에 주어진다. 네트워크 다이몬은 어떤 원격 로그인 세션이 포함되었는지 구별하고 패킷을 세션을 위한 적절한 서브 다이몬에 전달한다. 이 흐름을 통해서, 컨텍스트 스위치와 상태 스위치가 있다는 것을 알 수 있다. 일반적으로, 수신자는 송신자에게 캐릭터를 다시 확인한다.(두배로 일하게 된다.) 

몇몇 시스템은 메인 CPU의 인터럽트 부담을 줄이기 위해서 분리된 터미널 I/O **front-end processors**를 사용한다. 예를 들어서, **terminal concentrator**는 큰 컴퓨터의 한 포트에 수백개의 원격 터미널을 통해서 트래픽을 다중화가능하다. **I/O channel**은 메인프레임과 하이엔드 시스템에서 쓰이는 특별한 목적 CPU이다. 채널의 일은 메인 CPU로부터 I/O 일을 더는 것이다. 발상은 채널이 데이터 흐름을 부드럽게 유지하고, 메인 CPU는 데이터를 프로세스하기 위해서 여유롭게 둔다. 작은 컴퓨터에서 발견되는 디바이스 컨트롤러와 DMA 컨트롤러처럼, 채널은 더 일반적이고 정교한 프로그램을 프로세스할 수 있고, 그래서 채널은 특정 업무에 조정될 수 있다.

우리는 I/O의 효율성을 높이기 위해서 몇가지 원칙을 정했다.
1. 컨텍스트 스위치의 수를 줄인다.
2. 디바이스와 앱 사이에 전달되는 사이에 반드시 복사되어야하는 데이터의 수를 줄인다.
3. 큰 전송, 스마트 컨트롤러, 폴링(만약 busy waiting이 최소화할 수 있다면)을 사용하는 인터럽트의 빈도를 줄인다.
4. CPU로 부터 간단한 데이터 복사 업무를 오프로드하는 DMA-knowledgeable 컨트롤러 또는 채널을 이용해서 동시성을 증가시킨다.
5. 프로세싱 기기를 하드웨어로 이동시키고, CPU와 버스 명령어가 동시에 작동하게 한다.
6. CPU, 메모리 버스 시스템, 버스, I/O 성능의 밸런스를 맞추는데, 왜냐하면 한 분야의 과적은 다른 것의 유휴를 일으키기 떄문이다.

I/O 디바이스들은 복잡성이 다양하다. 예를 들어서, 마우스는 간단하다. 마우스 이동과 버튼 클릭은 하드웨어, 디바이스 드라이버, 앱순으로 전달되고 숫자 값으로 변형된다. 반면에, 윈도우 디스크 드라이버에 의해서 제공되는 기능은 매우 복잡하다. 그것은 개인적인 디스크 뿐만이 아니라 RAID 행렬을 구현한다. 이걸 수행하기 위해서, 그것은 앱의 읽기와 쓰기를 디스크 I/O 명령어의 조정된 집합로 전환한다. 더욱이 그것은 정교한 에러 핸들링과 데이터 복구 알고리즘을 구현하고 디스크 성능을 최적화하기 위해서 많은 과정을 거친다.

I/O 기능이 어디에 구성될까? 디바이스 하드웨어, 디바이스 드라이버, 또는 앱 소프트웨어?

- 먼저, 우리는 실험적인 I/O 알고리즘을 앱 레벨에 구현하는데, 왜냐하면 앱 코드는 유연하고 앱 버그는 시스템 크래시를 잘 일으키지 않는다. 더 나아가, 앱 레벨에서 코드를 개발함으로서, 우리는 코드를 바꿀때마다 디바이스 드라이버를 리부트나 리로드 할 필요가 없다. 앱 레벨 구현은 비효율적일 수 있다, 왜냐하면 컨텍스트 스위치의 오버헤드 때문이고, 앱이 내부 커널 데이터 구조와 커널 기능의 장점을 취하지 못하기 때문이다.(in-kernel messaging, threading, locking같은) FUSE 시스템 인터페이스는 파일 시스템이 유저모드에서 작동하고 쓰일수 있게 허용한다.
- 앱 레벨 알고리즘이 그것의 가치를 증명하면, 우리는 그것을 커널에 다시 구현할 수 있다. 이것은 성능을 향상시키지만, 그러나 개발 노력은 더욱 도전적인데, 왜냐하면 운영체제 커널은 크고 복잡한 소프트웨어 시스템이기 때문이다. 더나아가, 인커널 구현은 반드시 데이터 부패와 시스템 크래시를 피하도록 엄격히 디버그되어야한다.
- 가장 높은 성능은 디바이스 또는 컨트롤 하드웨어에 특수화된 구현이다. 하드웨어 구현의 단점은 더 나은 발전을 만드는 것과 버그를 고치는 것의 비용과 어려움, 긴 개발 시간과 낮은 유연성을 포함한다. 예를 들어서, 하드웨어 RAID 컨트롤러는 순서 또는 개인 블럭 리드와 라이트의 위치에 영향을 줄 커널에 아무런 수단을 제공하지 않을 것이다. 심지어는 커널이 I/O 성능을 증가시킬 워크로드에 관한 특별한 정보가 있어도 무시한다. 

시간이 지날수록, 컴퓨팅의 관점에서, I/O 디바이스들은 빨라졌다. 비 휘발성 메모리 디바이스들은 점점 인기있어지고 다양한 분야의 디바이스가 생겼다. NVM 디바이스의 속도는 세대에 따라서 DRAM에 가까운 속도를 가진다. 이런 발전은 I/O 서브시스템이 운영체제 알고리즘에서 현재 가능한 read/write 속도의 이득을 취할 수 있게 부담을 준다. 