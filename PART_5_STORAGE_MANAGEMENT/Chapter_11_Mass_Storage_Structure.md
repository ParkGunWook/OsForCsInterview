## what we gonna study

이 장에서, 우리는 컴퓨터의 비휘발성 대량 저장소가 어떻게 구성되는지 보겠다. 주요 대량 저장소 시스템은 2차 저장소이고, HDD와 NVM을 통해서 제공된다. 몇몇 시스템은 더욱 크고 3차저장소인 자기 테이프, 광학 디스크, 클라우드 저장소를 가진다.

현대에서 가장 중요하고 일반적인 저장소는 HDD와 NVM이고, 두 저장소에 대한 설명이 대부분일 것이다. 우리는 I/O의 성능을 최대화하는 스케쥴링 알고리즘을 살펴보겠다. 다음으로, 우리는 디바이스 포매팅과 부트 블럭 관리, 손상 블럭, 스왑스페이스에 대해서 논의하겠다. 마지막으로, RAID 시스템의 구조를 확인하겠다.

대량 저장소에는 다양한 종류가 있고, 우리는 보통 *nonvolatile storage(NVS)*라고 부른다. 그리고 모든 타입의 기기를 일반적으로 "drives"라고 표현하겠다. HDD와 NVM같은 특정 기기는 정확하게 정의될 것이다.

## objectives

- 다양한 2차 저장소의 물리적인 구조와 사용에 맞게 디바이스의 구조 효과를 설명하겠다.
- 대량 저장 기기의 성능 성질을 설명하겠다.
- I/O 스케쥴링 알고리즘을 평가하겠다.
- RAID를 포함한 대량 저장소를 위해 운영체제가 제공하는 서비스를 보겠다.

## 11.1 Mass-Storage Structure

현대 컴퓨터의 2차 저장소는 대부분 **hard disk drives(HDDs)**와 **nonvolatile memory(NVM)**으로 제공된다. 이 절에서, 우리는 이런 기기의 메커니즘을 설명하고 어떻게 운영체제가 주소 매핑을 통해서 그들의 물리적 공간을 논리주소로 번역하는지 보겠다.

### 11.1.1 Hard Disk Drives

개념적으로, HDD들은 꽤나 단순하다. 각 디스크 **platter**는 평평한 원형이다. 일반적인 플래터 지름은 1.8~3.5인치이다. 플래터의 두 표면은 자성 물질로 덮여있다. 우리는 플래터위에서 자기적으로 저장하고, 우리는 플래터위의 자기적인 패턴을 감지해서 정보를 읽는다.

읽기-쓰기 헤드는 모든 플래터의 위에 있다. 그 헤드들은 헤드를 하나의 유닛처럼 움직여주는 **disk arm**에 붙어있다. 플래터의 표면은 **sector**로 다시 나누어지는 원형 **tracks**로 논리적으로 나누어진다. 주어진 암 위치의 트랙의 집합은 **cylinder**를 만든다. 디스크 드라이브에는 수천개의 동심원 실린더가 있고, 각 트랙은 수천개의 섹터를 가진다. 각 섹터는 고정된 크기를 가지고 전송의 최소 단위이다. 섹터 크기는 2010년에는 512바이트였다. 그 시점에서, 많은 제조사들이 4KB 섹터로 이전하기 시작했다. 일반적으로 디스크의 커패시티는 기가바이트와 테라바이트이다. 

디스크 드라이브 모터는 높은 속도로 회전한다. 대부분의 드라이브는 초당 60~250번을 회전하고 **rotations per minutes(RPM)**이라고 부른다. 일반적인 드라이브는 5400, 7200, 10000, 15000의 RPM을 가진다. 몇몇 드라이브는 사용되지 않거나 I/O 요청을 기다릴때까지 전원을 꺼둔다. 회전 속도는 전송 속도와 관계가 있다. **Transfer rate**는 드라이브와 컴퓨터 사이에 데이터가 흐르는 속도이다. 다른 성능 측면은 **positioning time**또는 **random-access time**은 두가지 부분으로 구성된다. 첫번쨰는 원하는 실린더에 디스크 암을 이동시키는 **seek time**이고 원하는 섹터를 회전시키는 **rotational latency**이다. 일반적인 디스크는 초당 100메가바이트를 전송하고, 그들은 탐색 시간과 회전 지연에 걸리는 시간은 몇 미리초에 불과하다. 그들은 드라이브 컨트롤러에 DRAM 버퍼르 가짐으로서 성능을 향상시킬 수 있다.

디스크 헤드 파일들은 헬륨 같은 가스로 이루어진 극단적으로 얇은 쿠션위에 있고, 헤드가 디스크 표면을 접촉할 위험이 있다. 디스크 플래터가 얇은 보호막으로 코팅되어도, 헤드는 자기 표면을 손상시킨다. 이런 사고를 **head crash**라고 부른다. 이 사고는 고쳐질 수 없다. 전체 디스크는 교체되어야하고, 데이터의 디스크는 다른 저장소에 백업 되거나 RAID로 보호되있지 않으면 영영 사라진다.

HDDs들은 봉인된 유닛이고, 몇몇 HDD를 쥐고 있는 chassis는 시스템의 종료나 저장소 chassis없이 그들의 제거를 허용한다. 이것은 시스템이 더 많은 저장소를 필요로하거나 그것이 배드드라이브를 일하는 것과 교체하기 위해서 필요하다. 다른 타입의 저장 미디어도 **removalble**하다.

### 11.1.2 Nonvolatile Memory Devices

NVM 디바이스는 점점 중요해지고 있다. 간단히 말하면 NVM 장치는 기계라기보다는 전기적인 장치이다. 가장 일반적으로, 이런 디바이스는 컨트롤러와 플래시 NAND 반도체 칩으로 구성되어있다. 다른 NVM 기술은 존재하는데, 배터리가 있는 DRAM은 그것의 컨텐츠를 잃지않고, 그러나 그들은 덜 일반적이고 책에서 다루지 않는다.

#### 11.1.2.1  Overview of NVM

NVM 베이스의 플래시 메모리는 컨테이너 같은 디스크 드라이브에서 사용되고, **solid state disk**라고 부른다. 다른 예시로는 **USB drive**또는 DRAM 스틱이 있다. 그것은 또한 스마트폰 기기에서 메인 저장소로서 마더보드의 위에 마운트되어있다. 모든 형태에서, 그것은 같은 방식으로 작동한다. NVM 기기의 주된 논의는 이 기술에 대한 것이다.

NVM 기기는 움직이는 부분이 없기 때문에 HDD보다 믿음직하고 탐색시간이나 회전 지연이 없기 때문에 더욱 빠르다. 추가적으로 그들은 적은 파워를 소모한다. 부정적인 측면은, 그들은 고전적인 하드디스크보다 비싸고 큰 하드디스크보다 적은 용량을 가진다. 시간에 걸쳐서, NVM 디바이스의 용량은 하드디스크의 용량 증가속도를 넘어섰고, 그들의 가격은 더욱 빠르게 떨어지고 있다. 그래서 그들의 사용은 급격히 증가하고 있다. 실제로, SSDs와 비슷한 기기는 더 작고 빠르고 에너지 효율적인 랩탑을 위해서 사용되고 있다.

NVM 기기가 하드디스크보다 더욱 빠르기 때문에, 표준 버스 인터페이스는 산출량에 주요한 리미트를 초래한다. 몇몇 NVM 기기들은 시스템 버스(PCIe)에 직접 연결되게 디자인된다. 이 기술은 전통적인 컴퓨터 디자인을 바꾸고 있다. 몇몇 시스템은 디스크 드라이브의 직접 교체로 사용하고, 그것을 새로운 캐시 티어에 넣기도하고, 성능을 최적화하기 위해서 자기 디스크, NVM, 메인 메모리 사이의 데이터를 움직인다.

NAND 반도체는 그들의 저장소와 신뢰성 문제를 가직 있다. 예를 들어서, 그들은 "page" 증가(섹터와 비슷하다.)를 통해서 읽고 쓰는데, 그러나 데이터는 NAND 셀이 지워지기 이전에는 덮어쓰기가 안된다. "block" 증가를 초래하는 삭제는 크기가 여러 페이지이고, 읽기나 쓰기보다 훨씬 많은 시간이 필요하다. 이 상황을 돕기위해서 NVM 플래시 디바이스는 각각이 데이터 패스로 연결된 여러가지 금형으로 이루어져있고, 그래서 명령어는 병렬적으로 일어날 수 있다. NAND 반도체는 또한 여러 삭제 사이클에 의해서 감소되고, 100000 프로그램 삭제 사이클 후에, 각 셀은 더이상 데이터를 보관할 수 없다. write wear 때문에, 더이상 움직일 수 없는 파트때문에, NAND NVM 수명주기는 년단위가 아니라 **Drive Writes Per Day(DWPD)**로 측정된다. 그 측정은 드라이브가 고장나기 전까지 드라이브가 매일 쓸수있는지를 측정한다. 예를 들어서 5DWPD를 가진 1TB NAND 드라이브는 보장기간동안 5TB를 매일 쓸수있다.

이런 제한은 몇가지 개선 알고리즘을 이끌었다. 다행히도, 그들은 NVM 기기에 구현이 되었고 운영체제의 역할이 아니다. 운영체제는 단순히 논리 블럭을 읽고 쓰고, 디바이스는 어떻게 되는지 매니지한다. 그러나, NVM 디바이스는 운영체제 알고리즘에 따라서 다양한 성능을 가지고, 그래서 컨트롤러가 무엇을 하는지에 대한 간단한 설명이 필요하다.

#### 11.1.2.2 NAND Flash Controller Algorithms

NAND 반도체가 한번 쓰이면 덮어쓰기가 안되기에, invalid 데이터를 포함한 페이지가 있다. 파일 시스템 블럭을 고려하면, 한번 쓰이고 후에 다시 쓰인다. 만약 그 기간동안 삭제가 일어나지 않으면, 페이지는 예전 데이터를 가지고, 현재는 invalid하다. 그리고 두번째 페이지가 현재의 정확한 버전의 블럭을 가진다. 유효하고 유효하지 않은 페이지를 NAND 블럭이 포함하고 있다. 어떤 논리 블럭이 유효 데이터를 가지고 이쓴지 추적하기 위해서, 컨트롤러는 **flash translation layer**를 유지한다. 이 테이블은 어떤 물리 페이지가 현재 유효 논리 블럭을 가진지 매핑한다. 그것은 또한 물리 블럭 상태를 추적하고 어떤 블럭이 유효하지 않은 페이지를 포함한지 확인하고 삭제시킨다.

이제 쓰기 요청을 기다리는 full SSD를 고려하자. SSD가 가득 찼기 때문에, 모든 페이지들은 쓰여있고, 그러나 이 상황에서 블럭들은 유효 데이터가 없을 수도 있다. 이런 경우에, 쓰기는 삭제가 이루어지기를 기다릴 수 있고, 그후에 쓰기가 일어난다. 그러나 만약 여유 블럭이 없다면 어떻게 될까? 만약 페이지들이 invalid data를 포함한다면 가용한 공간이 있을 것이다. 그런 경우에는, **garbage colletion**이 일어나고 좋은 데이터는 다른 공간에 복사되고 블럭을 깔끔히 한후에 쓰기 요청을 받을 수 있다. 그러나, 가비지 컬렉션은 어디에 좋은 데이터를 저장할까? 이 문제를 해결하고 쓰기 성능을 증가시키기 위해서, NVM 디바이스는 **overprovisioning**을 사용한다. 디바이스는 쓸 수 있는 페이지를 보통 전체의 20%정도로 유지한다. 가비지 컬렉션에 의해서 완벽히 유효하지 않은데이터, 데이터의 오래된 버전들은 over porvisioning 공간에 두고 만약 디바이스가 가득차면 여유 풀로 리턴한다.

over provisioning 공간은 **wear leveling**을 돕는다. 만약 몇몇 블럭이 반복적으로 지워지면, 그 지워지는 블럭은 다른 것들보다 빨리 닳고, 전체 기기는 만약 블럭이 동시적으로 닳는 것보다 짧은 수명주기를 가질 것이다. 컨트롤러는 적게 삭제된 블럭에 데이터를 위치해서 전체기기의  닳은 정도의 밸런스를 맞추는 다양한 알고리즘을 사용한다.

데이터 보호에서, NVM 디바이스는 에러 수정 코드를 제공하는데, 데이터를 읽고 쓰는 과정에서 에러를 찾아내고 고치는 것을 저장하고 계산한다. 만약 페이지가 자주 에러를 가지면, 페이지는 나쁘다고 마킹되고 다음 쓰기에서 사용되지 않는다. 일밙적으로, 단일 NVM 기기는 정보가 썩거나 쓰기/읽기 요청에 답하지 못할때 재앙적인 실패를 가진다. 이런 것을 복구하기 위해서 RAID 보호가 사용된다.

### 11.1.3 Volatile Memory

대량 저장소 구조에 관한 장에서 휘발성 메모리를 논의하는 것은 이상할 수 있는데, 그것은 DRAM이 대량 저장 디바이스로 쓰이기 때문에 정당화가 가능하다. 특히, **RAM drives**(RAM 디스크라고도 불림)는 2차 저장소로 역할하지만, 시스템의 DRAM에서 발굴한 디바이스 드라이버에 의해서 생성되고 그것이 저장소였던 것처럼 작동한다. 이런 "drives"들은 raw block devices로 쓰일 수있고, 더욱 일반적으로, 파일 시스템들이 일반적인 파일 명령어로 생성된다.

컴퓨터들은 이미 버퍼링가과 캐싱을 가지는데, 그래서 임시 데이터 저장소를 위한 DRAM의 다른 목적은 무엇일까? DRAM은 휘발성이기에, RAM 드라이브의 데이터는 시스템 크래시, 종료, 파워 다운에서 살아남지 못한다. 캐시들과 버퍼들은 프로그래머 또는 운영체제에 의해서 할당되지만, RAM 드라이브는 일반적인 파일 명령어를 통해서 메모리에 둘 수 있다. 실제로, 램 드라이브 기능은 대부분의 운영체제에서 사용된다. 리눅스에서, /dev/ram, 맥에서는 diskutil 커맨드, 윈도우는 서드파티 툴, 솔라리스와 리눅스는 /tmp를 부트 시간에 만들고 그것이 RAM 드라이브이다.

램 드라이브들은 높은 속도의 임시 저장 공간에서 유용하다. 비록 NVM이 빨라도, DRAM이 더욱 빠르고, RAM 드라이브의 I/O 명령어는 생성, 읽기, 쓰기, 삭제가 그 무엇보다도 빠르다. 많은 프로그램들은 임시 파일을 저장하기 위해서 RAM 드라이브를 사용한다. 예를 들어서 프로그램들은 RAM 드라이브로의 공유 데이터를 통해서 읽기 쓰기를 쉽게 구현한다. 다른 예시로는, 리눅스는 시스템의 다른 파트를 루트 파일과 그것의 컨텐츠 엑세스를 가지는 것을 허용하는 임시 루트 파일 시스템(`initrd`)을 운영체제의 다른 파트가 저장 디바이스를 로드하기 전에 부트시간에 만든다.

### 11.1.4 Secondary Storage Connection Methods

2차 저장소 기기는 시스템 버스 또는 **I/O bus**로 연결되어있다. 몇가지 버스가 존재하는데, **advanced technology attatchment(ATA)**, **seral ATA(SATA)**, **eSATA**, **serial attatched SCSI(SAS)**, **universal serial bus(USB)**와 **fibre channel(FC)**가 있다. 대부분의 일반적인 연결 방식은 SATA이다. NVM 기기들이 HDD보다 빠르기 때문에, NVM 디바이르를 위한 빠른 인터페이스인 **NVM express(NVMe)**가 있다. NVMe는 시스템 PCI 버스로 연결되어있고, 산출량이 증가하고 다른 연결방식에 비해서 낮은 지연율을 가진다.

버스에서의 데이터 통신은 특별한 전자 프로세서인 **controllers**(or **host-bus adapters(HBA)**)를 통해서 전달한다. **host controller**는 컴퓨터 버스의 끝단에 있다. 대량 저장소 I/O 명령어를 실행하기 위해서, 컴퓨터는 호스트 컨트롤러에 명령을 주는데, 메모리 매핑 I/O 포트를 이용한다. 호스트 컨트롤러는 메시지를 통해서 커맨드를 전송하고 컨트롤러는 커맨드를 가져오기 위해서 드라이브 하드웨어를 실행한다. 디바이스 컨트롤러는 대개 캐시안에 있다. 드라이브에서의 데이터 전송은 캐시와 저장 매체에서 일어나고, 데이터는 호스트에게 빠른 전자 속도로 전달되고, DMA를 통해서 캐시 호스트 DRAM으로 일어난다.

### 11.1.5 Address Mapping

저장기기들은 **logical blocks**의 일차원 행렬로 가르쳐지고, 논리 블럭은 전송의 가장 단위이다. 각 논리 블럭은 물리 섹터 또는 반도체 페이지에 매핑된다. 섹터 0은 HDD의 실린더 가장 외각의 첫번째 트랙의 첫번째 섹터이다. 매핑은 트랙을 따라서, 실린더의 나머지 트랙을 따라서, 나머지 외부에서 내부의 실린더를 따라서 진행된다. NVM의 매핑은 칩의 튜플, 블럭, 페이지로 이루어진다. logical block address(**LBA**)는 섹터, 실린더, 헤드 튜플 또는 칩, 블럭, 페이지 튜플보다 알고리즘을 사용하기 간단하다. 

HDD의 매핑을 사용함으로서, 우리는 논리 블럭 넘버에서 구식 디스크로 전환하는 것을 배울수 있다. 실전에서는, 이 번역을 행하는 것은 3가지 이유때문에 어렵다. 첫번째, 대부분의 드라이브는 손상된 섹터가 존재하지만, 매핑은 이런 것을 드라이브의 어떤 스페어 섹터로 대체해서 숨겨버린다. 논리 블럭 주소는 연속적이지만, 물리 섹터 위치는 변했다. 두번쨰, 트랙의 섹터 수는 몇몇 드라이브에서 상수가 아니다. 세번째, 디스크 생산자는 LBA에서 물리 주소 매핑을 내부적으로 관리하고 그래서 현재의 드라이브에서, LBA와 물리 섹터의 관계는 적어졌다. 물리 주소의 예측 밖의 변화때문에, HDD와 대응하는 알고리즘은 논리 주소를 논리 주소는 상대적으로 물리 주소에 상관있다고 가정하는 경향이 있다. 즉, 논리 주소가 증가하는 것은 물리주소도 증가하는 것이다.

2번째 이유에 대해서 깊게 살펴보자. **constant linear velocity(CLV)**를 사용하는 미디어에서, 트랙 당 비트의 밀도는 일정하다. 트랙이 디스크의 센터에서 멀수록, 그것의 길이는 더크고, 그래서 더 많은 섹터를 가질 수 있다. 우리가 바깥에서 안쪽으로 이동하면, 트랙당 섹터의 수는 감소한다. 가장 밖의 트랙은 가장 안쪽 보다 40퍼센트나 많은 섹터를 가질 수 있다. 드라이브는 그것의 회전속도가 헤드가 안쪽으로 들어갈수록 빠르게 해서 헤드 아래에서 같은 속도를 갖게끔한다.  이 방법은 CD-ROM과 DVD-ROM에서 주로 쓰인다. 대안으로, 디스크 회전속도는 상수로 머물 수 있다. 이런 경우에, 안쪽에서 바깥 트랙의 비트의 밀도는 감소하므로 데이터 속도는 일정하다. 하드 디스크에서의 이런 메서드는 **constant angular velocity(CAV)**라고 한다.

## 11.2 HDD 스케쥴링

운영체제의 책임중에 하나는 하드웨어를 효율적으로 사용하는 것이다. HDD에서, 이 책임은 접근시간의 최소화와 데이터전송 대역을 최대화하는 것이다.

플래터를 사용하는 HDD와 다른 기계적 저장 기기에서, 접근 시간은 2가지 주요한 요소가 존재한다. 탐색 시간은 기기의 암이 원하는 섹터를 가르키는 실린더로 이동시키는 것이고 회전 지연은 플래터가 원하는 섹터로 회전하는 시간을 가르킨다. 기기 **bandwith**는 전송되는 바이트의 총 수이고, 전체시간은 처음 요청한 서비스와 마지막 전송의 완료로 나누어진다. 우리는 접근 시간과 대역폭을 어떤 저장소 I/O 요청이 서비스 되었는지 순서를 관리함으로서 향상시킨다.

프로세스가 드라이브로부터 I/O를 필요로하면, 그것은 운영체제에 시스템 콜을 호출한다. 요청은 다음과 같은 정보로 구성되어있다.

- 이 명령어가 인풋인지 아웃풋인지.
- 오픈 파일 핸들이 실행할 파일을 가르키는지.
- 전송할 메모리 주소가 무엇인지.
- 전송될 데이터의 양

만약 원하는 드라이브와 컨트롤러가 존재하면, 요청은 즉시 이루어진다. 만약 드라이브나 컨트롤러가 바쁘면, 서비스를 위한 요청은 그 드라이브를 위한 큐에 위치할 것이다. 다양한 프로세스를 가진 멀티 프로그래밍에서, 디바이스 큐는 몇가지 대기 요청을 가질 것이다.

디바이스에 대한 리퀘스트의 큐의 존재는 그것의 성능을 헤드 탐색을 피함으로서 최적화해서 디바이스 드라이버가 큐 순서를 통해서 성능을 향상시킬 수 있도록한다.

과거에는, HDD 인터페이스는 호스트가 어떤 트랙과 어떤 헤드를 이용할지 특정할 필요가 있었고, 디스크 스케쥴링 알고리즘에 많은 노력을 소비했다. 한 세기전의 드라이브는 이런 컨트롤을 호스트에게 노출시키지 않았을 뿐 아니라, 드라이브 컨트롤에 따라서 LBA에서 물리 주소로 매핑했다. 현재 디스크 스케쥴링의 목표는 시퀀스에서 나타나는 연달은 읽기, 쓰기같은 것에서 fairness, timeliness, optimizations을 포함한다. 그러므로 몇몇 스케쥴링 노력은 여전히 유용하다. 몇몇 디스크 스케쥴링 알고리즘은 쓰일수 있고, 우리는 다음에 논의하겠다. 헤드 위치와 물리 블럭/신린더의 절대적인 지식은 현대의 드라이브에는 쓸모가 없다. 그러나 적당한 근사화로, 알고리즘들은 LBA의 증가는 물리주소의 증가로 가정하고, LBA들은 물리 블럭 정확도에 따라 동일시된다.

### 11.2.1 FCFS 스케쥴링

디스크 스케쥴링의 가장 간단하 형태는 당연히 FCFS 알고리즘이다. 이 알고리즘은 본질적으로 공정하지만, 그것은 가장 빠른 서비스는 제공하지 못한다. 예를 들어서, I/O에서 실린더 블록의 디스크 큐가 다음과 같다고 생각하겠다. 

`98,183,37,122,14,124,65,67`

만약 디스크 헤드가 실린더 53위치에서 시작한다면, 순서대로 진행되면 총 640의 이동이 있을 것이다. 122~14~124로의 이동은 스케쥴의 시간에 문제가 된다. 만약 37과14가 같이 서비스된다면 전체 움직임은 줄어들고, 성능은 증가할 수 있을 것이다.

### 11.2.2 SCAN 스케쥴링

**SCAN algorithm**에서, 디스크 암은 디스크의 한 끝에서 시작하고 다른 끝으로 이동하는데, 각 실린더에 도착할때 리퀘스트를 서비스한다. 다른 끝에서, 헤드의 이동방향은 반대가 되고 서비스가 계속된다. 헤드는 디스크를 앞뒤로 순회한다. 스캔 알고리즘은 **elevator algorithm**이라고도 불리고, 디스크 암이 빌딩의 엘리베이터처럼 작동하기 때문이다.

위의 예시를 사용하겠다. 우리는 헤드의 현재 위치와 헤드 이동의 방향을 알아야한다. 디스크 암이 0으로 향한다고 가정하고 초기 헤드 위치가 다시 53이라면, 헤드는 37을 서비스하고 14를 서비스할 것이다. 반대로 이동하면서 65, 67, 98, 122, 124, 183을 서비스 할 것이다. 만약 리퀘스트가 헤드가 도달하기 직전에 큐에 들어오면 거의 즉시 서비스가 될것이다. 헤드가 지난 후에 도달한 리퀘스트는 암이 끝을 갔다가 돌아올때까지 기다려야할 것이다.

실린더의 요청이 균일한 분포라고 가정하겠다. 이런 지점에서, 상대적으로 적은 리퀘스트가 헤드의 앞에서 요청되는데, 실린더가 최근에 서비스되었기 때문이다. 리퀘스트의 가장 무거운 밀집도가 디스크의 끝에 있다. 이런 요청들은 오랜시간 기다려야하고, 그래서 여기를 먼저 방문하는 방안을 생각했다. 다음 알고리즘이 그러하다.

### 11.2.3 C-SCAN 스케쥴링

**Circular SCAN(C-SCAN) Scheduling**은 더욱 균일한 대기시간을 제공하기 위해서 제공된 스캔의 변형이다. 스캔과 C-SCAN은 헤드를 한 끝에서 다른 끝으로 이동시키고, 그 길을 따라서 리퀘스트를 서비스한다. 헤드가 끝에 도달했을때, 그것은 돌아오는 여정에서 어떠한 리퀘스트를 서비스하지 않는다.

위의 예제를 다시 이용하겠다. 우리는 우선 헤드를 53에서 다시시작하고, 리퀘스트는 끝점에 도달할떄까지 SCAN과 같지만, 199에 도달한 후에는 14-37의 순서로 돌아가는 것이다. C-SCAN은 실린더의 마지막과 첫번째가 연결된 원형 리스트로 가정하는 것이다.

### 11.2.4 디스크 스케쥴링 알고리즘의 선택

디스크 스케쥴링에는 여기서 다루지 않은 다양한 알고리즘들이 존재한다.(물론 거의 사용되지 않는다.) 그러나 어떻게 운영체제 디자이너들은 무엇을 구현할지 정하고, 사용자들은 가장 좋은 것을 어떻게 고를지 고민한다. 어떠한 요청의 리스트에서, 우리는 최적의 순서를 정의할 수 있지만, 최적의 스케쥴을 찾기 위한 계산은 SCAN을 넘어서는 절약을 정의하지는 못한다. 스케쥴링 알고리즘과 함꼐, 성능은 리퀘스트의 수와 종류에 크게 의지한다. 예를 들어서, 큐가 오직 하나의 특별한 요청을 한다고 가정하자. 그러면, 모든 스케쥴링 알고리즘은 같게 작동하는데, 왜냐하면 그들은 오직 한가지 행동만하고, 그들은 FCFS와 다름이 없다.

스캔과 C스캔은 디스크에 큰 부하를 주는 시스템에서 좋은 성능을 가지는데, 그들은 기아 문제를 거의 일으키지 않기 때문이다. 물론 리눅스가 **deadline** 스케쥴러를 만들어서 기아가 생길수는 있다. 이 스케쥴러는 읽기와 쓰기 큐를 분리해서 유지하고, 프로세스들이 읽기를 더많이 하므로 읽기에 우선순위를 준다. 큐는 LBA 순서로 정렬되어있고, C-SCAN을 구현한다. 모든 I/O 요청들은 LBA 오버로 배치로서 보내진다. 데드라인은 4가지 큐를 유지한다. 2개의 읽기와 2개의 쓰기이다. 하나는 LBA로 정렬되어있고 하나는 FCFS이다. 그것은 각 배치에서 설정한 나이보다 늙은 것이 있는지 확인한다. 만약 그렇다면, 그 요청을 포함하는 LBA 큐가 다음 I/O의 배치로 선택이 된다.

데드라인 I/O 스케쥴러는 리눅스 레드햇7버전에서 디폴트이지만, **RHEL**7은 다른 두가지도 포함한다. NOOP는 빠른 NVM 저장소를 이용한 CPU 바운드 시스템을 선호하고, **Completely Fair Queueing scheduler(CFQ)**는 SATA드라이브에 디폴트이다. CFQ는 3가지 큐를 유지한다.(인서트 소트를 이용해서 LBA의 순서를 유지한다.) : real time, best effort, idle. 각각은 다른 것을 넘어서는 독점적인 우선도를 가지고, 기아 가능성이 있다. 그것은 과거 데이터를 이용하고, 프로세스가 I/O 요청을 곧 할것이라고 예측한다. 만약 그렇게 결정되면, 그것은 새로운 I/O를 기다리며 유휴한다. 이 방법은 탐색시간을 줄이고 I/O 리퀘스트의 레퍼런스의 로컬리티를 에측하게한다.

## 11.3 NVM 스케쥴링

이전 절에서 나온 디스크 스케쥴링 알고리즘은 HDD 같은 기계적인 플래터 베이스의 저장소에 적용된다. 그들은 디스크 헤드 이동의 양을 최소화하는데 집중했다. NVM은 디스크 헤드를 움직이지 않고 보통 FCFS를 사용한다. 예를 들어 리눅스 **NOOP** 스케쥴러는 FCFS 정책을 사용하지만 근접한 요청을 합쳐서 수정한다. NVM 기기의 행동은 읽는데 걸리는 시간은 플래시 메모리의 특성에 따라서 일정해 보이는데, 쓰기 서비스 시간은 일정하지 못하다. 몇몇 SSD 스케쥴러는 이 성질 때문에 나빠지고 근접한 쓰기 요청을 합치고, 모든 읽기 요청을 FCFS로 서비스한다.

우리가 보았듯이, I/O는 연속적으로나 랜덤으로 일어날 수 있다. 연속적인 접근은 HDD같은 기계적인 기기에서 최적화되었다. **input/output operations per second(IOPS)**로 측정되는 랜덤 접근 I/O는 HDD 디스크 헤드의 이동을 유발한다. 자연적으로 랜덤 엑세스 I/O는 NVM에서 빠르다. HDD는 수백개의 IOPs를 만드는데 SSD는 수십만의 IOPs를 만든다.

NVM 기기는 연속적인 산출량에 적은 이득을 제공하고, HDD 헤드 탐색들은 감소되고 데이터의 리딩과 라이팅이 강조된다. 이런 경우에, 읽는 경우에, 두 가지 유형의 장치에 대한 성능은 NVM 장치에 대한 규모의 순서에 해당하는 정도까지 다양할 수 있습니다. NVM에 쓰는 것은 읽기보다 느리고, 장점이 감소된다. 더 나아가, HDDs의 쓰기 성능이 얼마나 디바이스가 가득찬지에 달려있고 얼마나 "닳아"있는지도 중요하다. 수명이 거의 다된 NVM 기기는 새거에 비해서 훨씬 나쁜 성능을 가지고 있다.

수명 주기를 시간에 따른 NVM 기기의 성능을 증가시키는 방법은 파일이 지워졌을때 파일 시스템이 디바이스에 알리고, 디바이스는 이런 파일들이 저장된 블록을 삭제한다. 이 방법은 14.5.6에서 더욱 자세히 다루겠다.

가비지 컬렉션이 성능에 영향을 주는 것에 대해 자세히 보겠다. NVM 기기가 랜덤 읽기와 쓰리 로드에 있다고 가정하자. 모든 블럭들은 쓰여있지만, 그들은 여유 공간이 존재한다. 가비지 컬렉션은 invalid 데이터에 의해 차지된 공간을 반드시 재정의해야한다. 이것은 쓰기가 하나 또는 여러 페이지에 읽기를 일으키고, 좋은 데이터의 쓰기는 공간을 제공한다. I/O 요청의 생성은 앱이 아니라 NVM 기기가 가비지 컬렉션과 공간 관리(**write amplification**)을 실행하는 것에 의해서 생성되고 기기의 쓰기 성능에 큰 영향을 준다. 최악의 경우에는 여러개의 추가 I/O가 각 쓰기 요청에 발생가능하다.

## 11.4 Error Detection and Correction

에러 감지와 수정은 메모리, 네트워킹, 저장소의 컴퓨팅에서 중요하다. **Error detection**은 만약 문제가 일어났다고 결정되면 생긴다. 예를 들어 DRAM의 비트가 자발적으로 0에서 1로 바뀌었고, 통신하는 동안에 네트워크의 패킷 정보가 바뀌거나, 데이터의 블럭이 쓰거나 읽힐때 바뀐 것이다. 이슈를 감지해서, 시스테은 에러가 실행되기전에 멈춰지고, 유저나 관리자에게 에러를 보고하고, 기기가 실패하거나 이미 실패되었다고 경고한다.

메모리 시스템은 다음의 에러를 parity bits를 사용해서 감지한다. 이 시나리오에서, 메모리 시스템의 각 바이트는 그것과 관련된 parity bit를 가지고 바이트가 가진 비트의 수가 짝수인지 홀수인지 세팅한다. 만약 바이트의 한가지 비트가 손상되면, 바이트의 parity가 바뀌고 저장된 parity와 일치하지 않는다. 비슷하게, 만약 저장된 parity 비트가 손상되면, 그것은 계산된 parity와 일치하지 않는다. 그러므로, 모든 단일 비트는 메모리 시스템에 의해서 감지된다. 더블 비트 에러는 감지되지 않을 수 있다. parity는 XOR연산으로 쉽게 계산되나. 모든 메모리의 바이트에서, 우리는 parity를 저장할 추ㅏㄱ 비트가 필요하다.

parity는 **checksums**의 한 종류이고, 계산하기 위해서 모듈러 연산을 사용하고, 고정된 길이의 워드 값을 비교한다. 네트워크에서 주로 이용되는 다른 에러 감지 메서드는 **cyclic redundancy check(CRCs)**인데, 다중 비트 에러를 해시 함수로 감지한다.

**error correction code(ECC)**는 문제를 감지할뿐 아니라, 고친다. 수정은 저장소의 추가공간과 알고리즘을 이용해서 이루어진다. 코드는 추가 저장소가 얼마나 필요하고 얼마나 많은 에러가 고쳐지는지에 달려있다. 예를 들어서, 디스크 드라이브는 섹터당 ECC를 사용하고 플래시 드라이브는 페이지당 ECC를 사용한다. 컨트롤러가 일반적인 I/O동안 데이터의 섹터/페이지를 쓰면, ECC는 데이터가 쓰이면서 사용된 모든 바이트의 값을 계산한다. 섹터/페이지가 읽으면, ECC는 다시 계산되고 저장된 값과 비교된다. 저장된 수와 계산된 수가 다르면, 이 미스매치는 데이터가 손상된 지점과 저장 매체가 나쁜 곳을 가르킨다.(11.5.3). ECC는 에러를 고치는데, 그것이 충분한 정보를 가지기 떄문이고, 만약 소수의 비트가 손상되면, 컨트롤러가 어떤 비트가 손상되면, 컨트롤러가 어떤 비트가 바뀐지 구별하고 그들의 원래 값이 무엇인지 계산한다. 그것은 회복가능한 **soft error**를 보고한다. 만약 많은 변화가 생기면, ECC는 에러를 수정하고, 고칠수 없는 **hard error**가 시그널된다. 컨트롤러는 ECC 과정을 어떤 섹터 또는 페이지가 읽거나 쓰이면 자동으로 실행한다.

에러 감지와 수정은 소비자 제품과 기업 제품사이에서 보통 다르다. ECC는 DRAM 에러 correction과 데이터 패스 보호를 위해서 사용된다.

## 11.5 Storage Device Management

운영체제는 저장 디바이스 관리의 몇가지 측면에 책임이 있다. 여기서 우리는 드라이브 초기화, 드라이브로부터 부팅, 배드 블럭 회복을 살펴보겠다.

### 11.5.1 Drive Formatting, Partitions, Volumes

새로운 저장소 디바이스는 빈 슬레이트이다. 그것은 자성 저장 장치의 플래터나 초기화되지 않은 반도체 저장 셀일뿐이다. 저장 디바이스가 데이터를 저장하기 전에, 그것은 반드시 컨트롤러가 읽고 쓰는 섹터로 분할되어야한다. NVM 페이지들은 반드시 초기화되고 FTL이 생성되어야한다. 이 프로세스는 **low-level formatting** 또는 **physical formatting**이라고 불린다. 로우 레벨 포매팅은 디바이스를 각 저장소 공간에 특별한 데이터구조로 채운다. 섹터 또는 페이지를 위한 데이터구조는 보통 헤더, 데이터 공간 트레일러를 구성한다. 헤더와 트레일러는 섹터/페이지 넘버와 에러 탐지 또는 수정 코드 같은 컨트롤러에 의해서 사용되는 정보를 포함한다.

대부분의 드라이브들은 생성 단계의 일부로 공장에서 로우레벨 포맷을 가진다. 이 포매팅은 생산자가 디바이스를 테스트하고 미디어의 페이지와 프리섹터를 버리기 위해서 논리 블럭 수로부터 매핑을 초기화한다. 그것은 보통 512 바이트와 4KB같은 몇가지 섹터 사이즈를 선택한다. 디스크를 포매팅하는 것은 소수의 섹터만이 각 트랙에 맞는 것인데, 그것은 또한 적은 헤더와 트레일러가 각 트랙과 더많은 공간에 쓰임을 의미하고 더 많은공간이 유저데이터로 사용가능하다.

그것이 파일을 잡기위한 드라이브를 쓰기전에, 운영체제는 여전히 디바이스에 그것의 전용 데이터 구조를 저장할 필요가 있다. 그것은 3가지 단계로 진행된다.

첫 단계는 디바이스를 블럭 또는 페이지의 그룹으로 **partition**하는 것이다. 운영체제는 각 파티션을 그것이 마치 분리된 디바이스인 것처럼 취급한다. 운영체제는 각 파티션을 분리된 디바이스로 취급한다. 예를 들어서, 한 파티션이 운영체제의 실행가능 코드의 카피를 가지면, 다른 것은 스왑 공간이고 다른 것은 유저파일을 포함하는 파일 시스템이다. 몇몇 운영체제와 파일 시스템은 전체 디바이스가 파일 시스템에 의해서 관리될 때 자동으로 파티셔닝을 실행한다. 파티션 정보는 저장 디바이스위에 고정된 위치에 고정된 포맷으로 쓰인다. 리눅스에서, `fdisk` 커맨드는 저장 디바이스에서 파티션을 관리하는데 쓰인다. 디바이스, 운영체제에 의해서 인식되면, 그것의 파티션 읽기를 가지고, 운영체제는 파티션의 디바이스 엔트리를 생성한다. 그곳에서부터, 설정파일, /etc/fstab은 운영체제가 특정 위치에 파일 시스템을 포함한 각 파티션을 마운트하고 읽기 전용같은 마운트 옵션을 사용한다. 파일 시스템을 **mounting**하는 것은 파일 시스템이 시스템과 그것의 유저에 의해서 가용하게 한다. 

두번째 단계는 볼륨 생성과 관리이다. 가끔, 이 단계는 내포되어있는데, 파일 시스템이 파티션안에 직접 위치된다. **volume**은 마운트되고 사용될 준비가 된다. 다른 시간에, 볼륨 생성과 관리는 explicit한데, 다중 파티션 또는 디바이스가 하나 또는 이상의 파일 시스템이 디바이스를 거쳐 퍼진 RAID 집합으로 사용된다. 리눅스 볼륨 매니저 lvm2는 이런 기능을 제공하고, 리눅스와 다른 운영체제를 위한 서드 파티 툴을 사용할 수 있다. ZFS는 볼륨 관리와 기능과 커맨드의 집합으로 통합된 파일 시스템을 제공한다.("volume"은 마운트 가능한 파일 시스템을 의미하고, 파일 시스템을 포함한 CD이미지를 의미한다.)

세번쨰 단계는 **logical formatting** 또는 파일 시스템의 생성이다. 이 단계에서는, 운영체제는 초기 파일 시스템 데이터 구조를 디바이스위에 저장한다. 이런 데이터 구조는 해제되고 할당된 공간의 맵과 초기 빈 디렉토리를 포함한다.

파티션 정보는 만약 파티션이 부트가능한 파일 시스템을 포함하면 가르킨다. 부트를 위해 라벨링된 파티션은 파일 시스템의 루트를 건설한다. 한번 그것이 마운트되면, 다른 디바이스와 그들의 파티션을 링크하는 디바이스는 생성된다. 일반적으로, 컴퓨터의 파일 시스템은 모든 마운트 볼륨을 포함한다. 윈도우에서, 이런 것들은 C, D, E로 정의되기도 한다. 리눅스에서, 부트 시간에 부트 파일 시스템이 마운트되고, 다른 파일 시스템들은 트리 구조안에서 마운트될 수 있다. 윈도우에서, 파일 시스테 인터페이스는 주어진 디바이스가 사용될때 명료해지는데, 리눅스에서 단일 파일 접근은 요청된 파일 시스템에서 요청된 파일이 접근되기 전에 많은 디바이스를 순회한다. 윈도우를 예로 들면, 디스크 관리 툴이 3개의 볼륨을 보인다. 그리고 몇가지는 디스크 1 디바이스의 파티션이고 더 많은 할당되지 않은 공간이 존재할 수 있다.

효율성을 높이기 위해서, 대부분의 파일 시스템 그룹이 큰 청크로 뭉쳐지고 **cluster**라고 불린다. 디바이스 I/O가 완료되면, 그러나 파일 시스템 I/O가 클러스터를 통해서 실행되면, I/O가 연속접근을 가진 것을 확신하고 적은 랜덤 접근 성능을 가진다. 파일 시스템들은 파일 컨텐츠를 그것의 메타데이터 근처에 그룹할려고하는데, 파일이 운영중일때 HDD 헤드 탐색을 줄인다.

몇몇 운영체제는 파티션을 큰 연속 논리 블럭의 파티션으로 사용할 능력을 특별한 프로그램에게 주고, 파일 시스템 데이터 구조가 없어도 된다. 이 행렬은 가끔 **raw disk**라고 불리고, 이 행렬의 I/O는 *raw I/O*라고 한다. 그것은 스왑공간에 주로 사용된다. 그리고 몇몇 데이터 베이스 시스템은 그것이 정확한 위치에 컨트롤하게 해서 선호하기도 한다. Raw I/O는 버퍼 캐시, 파일 락킹, 프리패칭, 공간할당, 파일 이름, 디렉토리 같은 모든 파일 시스템 서비스를 우회한다. 우리는 특정한 앱을 그들의 특수 목적 저장소를 구현하는 것을 허용해서 효과적으로 만든다. 그러나 대부분의 앱들은 그들이 데이터를 직접 관리하기보다는 제공된 파일 시스템을 사용한다. 리눅스는 raw I/O를 제공하지 않고 DIRECT 플래그로 비슷한 일을 처리한다.

### 11.5.2 Boot Block

실행을 시작하기 위해서 컴퓨터는, 그것이 파워가 공급되거나 리부트 될때, 그것은 반드시 실행할 초기 프로그램을 필요로한다. 첫번째 **bootstrap** 로더는 간단한 경향이 있다. 대부분의 컴퓨터에서, 부트스트랩은 마더보드의 NVM 플래시 메모리 펌웨어에 저장되고 알려진 메모리 위치에 매핑된다. 그것은 필요에 따라서 제품 제작자에 의해서 업데이트되고, 시스템을 감염시키는 바이러스에 의해서 쓰일 수 있다. 그것은 CPU 레지스터부터 디바이스 컨트롤러와 메인 메모리의 컨텐츠까지 시스템의 모든 부분을 초기화한다.

이 작은 부트스트랩 로더 프로그램은 또한 2차저장소로부터 풀 부트스트랩 프로그램을 가져오기에 충분히 똑똑하다. 풀 부트스트랩 프로그램은 디바이스의 고정된 위치에 "boot blocks"에 저장된다. 리눅스 부트 스트랩 로더는 grub2이다. 부트 파티션을 가진 디바이스는 **boot disk**또는 **system disk**라고 불린다. 

부트스트랩 NVM안의 코드는 메모리안으로 부트블럭을 읽기 위기위해서 저장소 컨트롤러를 지시하고 그 코드를 실행하기 시작한다. 풀 부트스트랩 프로그램은 부트스트랩 로더보다 더 정교하다. 그것은 디바이스의 고정되지 않은 위치로부터 전체 운영체제를 로드할 수 있고 운영체제 실행을 시작한다.

윈도우에서의 부트 프로세스를 설명하겠다. 먼저, 윈도우는 드라이브가 파티션으로 나뉘게 허용한다는 것을 알아야한다. 그리고 한 파티션은 운영체제와 디바이스 드라이버를 포함한 **boot partition**으로 식별된다. 윈도우 시스템은 하드디스크의 첫번째 논리 블럭 또는 NVM의 첫번째 페이지안의 부트 코드를 둔다. 이 코드는 **master boot record**또는 **MBR**이라고 한다. 시스템의 펌웨어에 거주하는 코드를 실행함로서 부팅은 시작된다. 이 코드는 시스템이 부트 코드를 MBR로부터 읽게하고, 섹터를 그것으로부터 로드하기 위해서 저장소 컨트롤러와 저장소 디바이스에 관해서 충분히 이해한다. 부트 블럭을 포함하는 것에 추가로, MBR은 드라이브를위한 파티션을 리스팅한 테이블과 시스템이 부트된 파티션을 가르키는 플래그를 포함한다. 한번 시스템이 부트 파티션을 식별하면, 그것은 커널로 그것을 지시하는 파티션으로부터 첫번째 섹터/페이지를 읽는다. 그것은 그러면 다양한 서브 시스템과 시스템 서비스를 로딩하는 것을 포함한 부트 프로세스의 잔여물로 계속된다.

### 11.5.3 Bad Blocks

디스크가 움직이는 파트와 작은 참을성(디스크 표면위의 디스크 헤드를 상기하겠다.)을 가지기 떄문에, 그들은 실패하기 쉽다. 가끔 실패가 완료되면, 이 경우에, 디스크는 교체될 필요가 있고 그것의 컨텐츠들은 새로운 디스크에 백업 미디어로 복구된다. 더 자주, 하나 또는 이상의 섹터는 쓸수없게된다. 대부분의 디스크는 **bad block**을 가진채로 공장으로부터 오기도 한다. 디스크와 컨트롤러에 따라서, 이런 블럭들은 다양한 방법으로 사용된다.

IDE 컨트롤러를 가진 몇몇 디스크 같은 오래된 디스크에서, 배드 블럭들은 수동으로 핸들된다. 한가지 전략은 디스크가 포맷되는 과정에 배드블럭을 찾기위해서 디스크를 스캔하는 것이다. 발견된 어떠한 배드 블럭들은 사용될수 없다고 플래그되고 그래서 파일 시스템은 그들을 할당하지 않는다. 만약 블럭이 일반적인 명령동안에 나빠지면, 특별한 프로그램은 반드시 배드블럭을 찾기 위해서 수동으로 탐색되고 그들을 락해야한다. 배드블럭에 상주하는 배드블럭은 보통 잃는다.

더 정교한 디스크들은 배드블럭 복구에 더욱 똑똑하다. 컨트롤러는 디스크에서 배드블럭의 리스트를 유지한다. 리스트는 공장에서 로우 레벨 포매팅을 하는 동안 초기화되고 디스크의 생애동안 업데이트된다. 로우 레벨 포매팅은 또한 운영체제에 보이지 않는 스패어 섹터를 옆에둔다. 컨트롤러는 각 배드 섹터를 논리적으로 하나의 스페어 섹터로 교체하게 말한다. 이 구조는 **sector sparing** 또는 **fowarding**이라고 한다. 일반적인 배드 섹터 트랜잭션은 다음과 같다.

- 운영체제는 논리 블럭 87을 읽으려고 시도한다.
- 컨트롤러는 ECC를 계산하고 그 섹터가 나쁘다는 것을 찾는다. 그것은 운영체제에게 I/O 에러로서 이 찾음을 제보한다.
- 디바이스 컨트롤러는 베드 섹터를 스페어와 교체한다.
- 그 후에, 시스템이 논리 블럭 87을 요청하면, 요청은 컨트롤러에 의해서 교체 섹터의 주소로 번역된다.

컨트롤러에 의한 이런 리디렉션은 운영체제의 디스크 스케쥴링 알고리즘에 의한 어떠한 최적화를 무효화한다. 이 이유로, 대부분의 디스크는 각 실린더에 소수의 스페어 섹터를 제공하고 스페이 실린더도 제공한다. 배드 블럭이 다시 매핑되면, 컨트롤러는 같은 실린더로부터 스페어 섹터를 사용한다.

섹터 스페어링의 대체로, 몇몇 컨트롤러는 **sector slipping**에 의해서 배드블럭을 교체하도록 지시한다. 논리 블럭 17이 낡고 첫번째 가용 스패어가 섹터 202를 따른다. 섹터 슬리핑은 그러면 17에서 202의  모든 섹터를 리맵하고, 그들은 한가지 스팟으로 옮긴다. 즉, 섹터 202는 스페어에 복사되고, 섹터 201은 202로, 200은 201로 변한다. 슬리핑 섹터는 섹터 18의 공간을 풀어주고 그래서 섹터 17은 매핑될 수 있다.

회복가능한 소프트 에러는 블럭 데이터의 카피가 만들어지고 블럭이 스페어되거나 슬립된 디바이스 활동을 트리거한다. 회복 불가능한 **hard error**는 데이터 손실을 초래한다. 파일이 블럭을 사용해도 반드시 수리되어야하고, 그것은 수동 개입을 필요로한다.

NVM 디바이스는 또한 생산 시간에 작동하지 않거나 시간을 거쳐 나빠진 비트, 바이트, 심지어 페이지를 가진다. 이런 오류 영역의 관리는 HDD보다 간단한데 왜냐하면 탐색 시간 성능 로스가 회피되기 떄문이다. 다중 페이지들은 치워지고 교체 위치 또는 공간으로 사용될수 있다. 어떤 방법이든지, 컨트롤러는 배드 페이지의 테이블을 유지하고 그런 페이지들을 쓰도록 하지 않고, 그래서 결코 접근되지 않는다.

## 11.6 스왑 공간 관리

스와핑은 9.5절에서 처음 소개했고, 전체 프로세스를 2차저장소와 메인 메모리 사이에서 움직이는 것이었다. 그 절에서 스와핑은 물리 메모리의 양이 낮을때 일어났고 프로세스들은 메모리에서 지워져서 여유 공간으로 바뀌었다. 실제로는, 소수의 운영체제만이 스와핑을 구현한다. 게다가, 시스템들은 스와핑을 가상 메모리 테크닉과 스왑페이지를 합성했다.(굳이 전체 프로세스가 아니다.) 실제로, 몇몇 시스템들은 "스와핑"과 "페이징"을 교차해서 사용하고, 두 컨셉이 합쳐졌음을 시사한다.

**Swap-sapce management**는 운영체제의 낮은 레벨 태스크이다. 가상 메모리는 2차 메모리를 메인 메모리의 확장으로 사용한다. 드라이브 엑세스가 메모리 엑세스보다 느리기 때문에, 스왑 스페이스를 사용하는 것은 시스템 성능을 극적으로 하향시킨다. 스왑 공간의 디자인과 구현의 주요한 목적은 가상 메모리 시스템에 최고의 산출량을 제공하는 것이다. 이 절에서, 우리는 어떻게 스왑공간이 사용되고, 스왑 공간은 어느 저장 기기에 위치하고, 스왑 공간이 어떻게 관리되는지 보겠다.

### 11.6.1 스왑 공간 사용

스왑 공간은 다른 운영체제에서 메모리 관리 알고리즘에 따라서 다양한 방법으로 사용된다. 예를 들어서, 스와핑을 구현한 시스템은 스왑 공간을 코드와 데이터 부분을 포함한 전체 프로세스 이미지를 잡기위해서 사용한다. 페이징 시스템들은 메인 메모리 밖으로 몰아낸 페이지를 저장하기도 한다. 시스템에 필요한 스왑공간은 소수의 메가 바이트에서 기가바이트로 다양할 수 있고, 물리 메모리의 양에 따라서, 가상 메모리가 백업된 양에 따라서, 가상 메모리가 어떻게 사용되냐에 따라서 다르다.

필요한 스왑공간을 저평가하는 것보다는 과평가하는 것이 안전한데, 만약 시스템의 스왑 공간이 동나면 그것은 프로세스를 실패시키거나 전체에 크래시를 일으킬 수 있다. 과 평가는 물론 다른 파일들이 사용될 2차 저장 공간을 낭비하지만, 그것은 손상을 주지는 않는다. 몇몇 시스템은 특정한 스왑공간을 보존하는 것을 추천한다. 솔라리스를 예를 들면, 페이징할 물리 공간을 초과하는 가상 메모리와 같은 스왑공간을 세팅했다고 가정하겠다. 과거에는, 리눅스는 물리메모리의 두배를 스왑공간으로 지정하게 제안했다. 오늘날, 페이징 알고리즘이 바뀌어서, 대부분의 리눅스 시스템들은 생각보다 적은 스왑공간을 사용한다.

리눅스를 포함한 몇몇 운영체제는 다중 스왑공간을 허용하는데, 파일과 스왑 파티션을 포함한다. 이 스왑 공간들은 분리된 저장 기기에 두어지고 그래서 페이징과 스와핑에 의해서 I/O 시스템에 놓아진 부하는 시스템의 I/O 대역에 퍼질 수 있다.

### 11.6.2 스왑 공간 위치

스왑 공간은 두 공간중 한 곳에 있다. 그것은 일반적인 파일 시스템을 개척하거나, 분리된 파티션에 있을 수 있다. 만약 스왑 공간이 파일 시스템안에서 단순히 큰 파일이면, 일반 파일 시스템 루틴이 그것을 생성하고, 이름 붙이고, 그것의 공간을 할당하기 위해서 사용할 수 있다.

대안으로는, 스왑 공간은 분리된 **raw partition**에서 생성될 수 있다. 어떠한 파일 시스템이나 디렉토리 구조는 이 공간에 쓰이지 않는다. 이 매니저는 공간 효율성보다는 속도를 최적화한 알고리즘을 사용하는데, 스왑 공간이 사용중일때는 파일 시스템보다 훨씬 자주 접근하기 때문이다. 내부 단편화가 증가할 수는 있지만, 이 트레이드 오프는 스왑 공간의 생명주기가 파일 시스템의 파일보다 훨씬 짧기 때문에 수용할만하다. 스왑 공간이 부트 시간에 초기화되면, 모든 단편화는 짧은 생이다. raw-partition 접근은 디스크 파티셔닝 동안에 고정된 양을 생성한다. 스왑 공간을 추가하는 것은 디바이스를 재 파티셔닝(다른 파일 파티셔닝을 움직이거나 그들을 파괴하고 백업으로부터 보존하는 것)하거나 다른 스왑 공간을 추가하는 것이다.

몇몇 운영체제들은 유연하고 raw partitions와 파일 시스템 공간에서 스왑하는 것이 가능하다. 리눅스는 이 예시이다. 정책과 구현이 분리되어서, 기계의 관리자는 어떤 타입의 스와핑을 사용할지 결정할 수 있다. 파일 시스템에서의 할당과 관리의 편리성과 raw parittions에서의 스와핑의 성능이 트레이드 오프이다.

### 11.6.3 스왑 공간 관리 : 예시

우리는 다양한 유닉스 시스템에서의 스와핑과 페이징의 발전에 따라서 어떻게 스왑공간이 사용되었는지 보겠다. 전통적인 유닉스 커널은 연속된 디스크 리전과 메모리 사이에 전체 프로세스의 카피를 스와핑에 구현했다. 유닉스는 페이징 하드웨어가 가용해진 후에 스와핑과 페이징을 합쳤다.

솔라리스 1에서, 디자이너는 효율성을 증가시키고 기술적 배치를 반영해서 표준 UNIX 메서드를 바꾸었다. 프로세스가 실행하면, 코드를 포함한 텍스트 세그먼트 페이지를 파일 시스템에서 들여오고, 메인 메모리가 접근했고 만약 페이지 아웃되면 버려졌다. 스왑 공간에 쓰고 다시 읽는 것보다 그냥 페이지를 파일 시스템에서 다시 읽는 것이 더욱 효율적이다. 스왑 공간은 프로세스의 스택, 힙, 초기화되지 않은 데이터인 **anonymous** 메모리의 페이지를 백업하는 용도로만 쓰였다.

솔라리스의 후 버전에서 더 많은 변화가 생겼다. 가장 큰 변화는 솔라리스가 스왑 공간을 가상 메모리 페이지가 처음 생성될떄보다는 오직 물리 메모리의 밖으로 페이지 아웃할때 할당하는 것이다. 이 구조는 현대 컴퓨터에서 더욱 좋은 성능을 제공했고, 구식 시스템보다 더 많은 물리 페이지를 가지고 페이지를 적게했다.

리눅스는 솔라리스와 비슷하게 anonymous memory에 대해서만 스왑 공간을 사용했다. 리눅스는 하나 또는 여러개의 스왑공간을 설치했다. 스왑 공간은 정규 파일 시스템의 스왑 공간 또는 스왑 전용 파티션에 있다. 각 스왑 공간은 4KB **page slots**의 연속에 포함되고, 스왑 공간을 붙잡기 위해서 사용된다. 각 스왑 공간에 관련된 것은 **swap map**인데, 정수 카운터의 행렬이고, 각각은 스왑 공간에 일치한다. 0보다 큰 값은 스왑 페이지에 의해서 페이지 슬롯이 점유된 것이다. 카운터의 값은 매핑의 수에서 스왑된 페이지를 가르킨다. 예를 들어서, 3의 값은 스왑된 페이지가 3가지 다른 프로세스에 매핑되었다고 가르킨다.(스왑된 페이지가 3개의 프로세스에 의해서 공유된 메모리를 가지는 것이다.)

## 11.7 저장소 attatchment

컴퓨터는 2차 저장소에 3가지 방법으로 접근한다. 호스트에 부착된 저장소, 네트워크에 부착된 저장소, 클라우드 저장소가 있다.

### 11.7.1 Host attached Storage

**Host attatched storage**은 로컬 I/O 포트를 통해서 접근되는 저장소이다. 이런 포트들은 다양한 기술을 사용하는데, 보통은 SATA를 이용한다. 시스템은 하나또는 소수의 SATA 포트를 가진다.

시스템이 더 많은 저장소를 가지려면, 개별 저장 기기, chassis의 기기, 또는 chassis의 다양한 드라이브가 USB FireWire 또는 Thunderbolt ports와 케이블을 통해서 얻을 수 있다. 

하이엔드 워크 스테이션과 서버들은 더 많은 저장소또는 공유된 저장소를 필요로해서, 광학 섬유를 이용하는 매우 빠른 직렬 아키텍처 **fibre channel** 또는 4개의 도체 구리 선에서 작동하는 정교한 I/O 아키텍쳐를 사용한다. 큰 주소 공간과 통신의 변경 환경 때문에, 다중 호스트들과 저장기기들은 I/O 통신에 큰 유연성을 허용할 수 있다.

다양한 저장 기기들은 host-attached storage에 적합하다. 이중에는 HDDs, NVM, CD, DVD, BLu-ray, tape와 storage-area network(**SANs**)가 있다. host-attached storage에 데이터 전송을 시작하는 I/O 커맨드는 지정된 저장 유닛에 있는 논리 데이터 블럭을 읽고 쓴다.

### 11.7.2 Netowork-Attached Storage

**Network-attached storage(NAS)**는 네트워크를 거쳐서 저장소에 접근한다. NAS 디바이스는 특정 목적 저장 시스템이거나 그것의 저장소를 그저 네트워크를 건너서 호스트에게 제공하는 일반 목적 컴퓨터 시스템일 수 있다. 고객들은 NAS에 유닉스와 리눅스의 NFS와 윈도우의 CIFS같은 remote procedure call interface를 통해서 접근한다. Remote procedure calls(RPCs)들은 TCP또는 UDP를 통해서 IP 네트워크를 건너서 옮겨진다. 보통 같은 local area network가 모든 데이터 트래픽을 클라이언트에세 가져다준다. NAS 유닛은 RPC 인터페이스가 구현된 소프트웨어와 함께 보통 저장소 행렬로 구현된다.

CIFS와 NFS는 다양한 락킹 기능을 제공하고, NAS 프로토콜로 접근하는 호스트 간에 파일의 공유를 허용한다. 예를 들어서, 다중 NAS에 로그인한 유저는 그녀의 홈에서 다른 클라이언트의 홈까지 동시에 접근이 가능하다.

NAS는 LAN을 가진 모든 컴퓨터가 같은 이름의 저장소 풀을 공유하고 로컬 host attached storage에 접근하는 편리한 기능을 제공한다. 그러나, 그것은 몇몇 직접 연결 저장소보다는 낮은 성능과 효율성을 보이는 경향이 있다.

**iSCSI**는 가장 최근의 NAS 프로토콜이다. 그것은 IP 네트워크 프로토콜을 이용해서 SCSI 프로토콜을 옮긴다. 그러므로, SCSI 케이블보다는 네트워크가 호스트와 그들의 저장소사이에 서로 연결되어있다. 결과적으로 호스트는 그것이 멀리 연결되어있어도 직접 연결되어있는 것처럼 다룬다. 반면에 NFS와 CIFS는 네트워크를 통해서 파일 시스템을 보이고 파일의 일부분을 보내는데 비해서, iSCSI는 네트워크를 통해서 논리 블럭을 보내고 클라이언트가 직접 블럭을 이용하거나 그들과 파일 시스템을 만든다.

### 11.7.3 Cloud Storage

1.10.5절에서 클라우드 컴퓨팅에 대해서 말했었다. 클라우드 제공자가 제공하는 것은 **cloud storage**이다. NAS와 비슷하게, 클라우드 저장소는 네트워크를 통해서 저장소의 접근을 제공한다. NAS와 달리, 저장소는 인터넷또는 WAN으로 요금으로 제공되는 원거리 데이터 센터에 접근한다.

NAS와 클라우드 저장소의 차이점은 저장소가 어떻게 접근되는지와 유저에게 제공되는지이다. NAS는 만약 CIFS나 NFS 프로토콜이 사용되면 다른 파일 시스템처럼 접근되거나, iSCSI 프로토콜을 사용하면 raw block 디바이스처럼 접근된다. 대부분의 운영체제들은 통합된 프로토콜을 가지고 NAS 저장소를 다른 저장소와 같은 방식으로 보인다. 반대로, 클라우드 저장소는 API 기반이고, 프로그램이 저장소에 접근하기 위해서 API를 사용한다. 아마존 S3는 클라우드 저장소이다. 드롭박스는 그들이 제공하는 클라우드 저장소에 연결하는 앱을 제공하는 회사의 예시이다. 다른 것은 MS의 원드라이브나 애플의 iCloud이다.

API가 존재하는 프로토콜 대신에 쓰이는 이유는 WAN의 지연과 실패 시나리오 떄문이다. NAS 프로토콜들은 LAN을 위해서 디자인되었고, WAN에서는 낮은 지연율을 가지고 저장소 유저와 저장소 기기가 자주 끊기게 된다. 만약 LAN 연결이 실패되면, NFS와 CIFS를 사용하는 시스템은 회복될때까지 기다려야 할 것이다. 클라우드 저장소와 함께, 그런 실패들은 더욱 그런 경향이 생기고, 그래서 앱은 간단하게 연결이 복구되기까지 접근을 간단히 정지하면된다.

### 11.7.4 Storage-Area Networks and Sotrage Arrays

NAS의 단점은 저장소 I/O명령이 데이터 네트워크에서 대역을 소비하고, 네트워크 통신의 지연을 늘린다. 이 문제는 특히 서버(서버와 클라이언트 사이의 통신이 서버와 저장기기 사이의 대역폭을 경쟁한다.)같은 대량 클라이언트에서 격심하다.

**Storage area network(SAN)**은 서버와 저장 유닛을 연결하는 개인 네트워크이다. SAN의 장점은 유연성이다. 다양한 호스트와 다양한 저장소 배열은 같은 SAN에 연결되어있고, 저장소는 동적으로 호스트에게 할당될 수 있다. 저장소 행렬은 RAID의 보호를 받을수도 아닐수도 있다.(**Just a Bunch of Disks**) SAN은 호스트와 저장소 사이에 접근의 허용/방지를 조절한다. 예를 들어서, 만약 호스트가 낮은 디스크 공간에서 실행중이면, SAN은 호스트에게 더욱 큰 저장소를 할당한다. SANs는 서버의 클러스터들이 같은 저장소를 공유하고 저장 행렬이 다양한 직접 호스트 연결을 포함하게 한다. SANs는 많은 포트를 가지고 저장소 행렬보다 더 많이 소모한다. SAN 연결은 짧은 거리에 있고 라우팅이 없고, 그래서 NAS는 SAN보다 더 많은 호스트를 가질수 있다. 

저장소 행렬은 SAN 포트, 네트워크 포트를 포함하는 특정 목적 기기이다. 그것은 데이터를 저장할 드라이브와 저장소를 관리하고 네트워크를 통해서 저장소 접근을 허용하는 드라이브를 포함한다. 컨트롤러는 CPU, 메모리, 행렬의 기능을 구현한 소프트웨어로 구성되어있고, 네트워크 프로토콜, 유저 인터페이스, RAID 보호, 스냅샷, 압축, 암호화를 포함한다. 이러한 기능들은 14장에서 언급하겠다.

몇몇 저장소는 SSDs를 포함한다. SSD를 가지면, 최대의 효율을 결과로 내지만, 용량은 작을 것이다. 만약 HDD와 SSD가 섞여있으면 소트프웨어는 SSD를 캐시로 사용하고 HDD를 대량 저장소로 사용해서 최고의 매체를 선택하게한다.

FC는 SAN의 가장 일반적인 연결이고, iSCSI또한 편리성때문에 늘어가고 있다. 또다른 SAN 연결은 **InfiniBand(IB)**이고 서버와 저장 유닛을 가장 빠르게 연결하는 소프트웨어와 하드웨어를 제공하는 특별 목적 버스 아키텍처이다.

## 11.8 RAID 구조

저장 기기는 점점 작아지고 싸지고 있어서, 이제 컴퓨터 시스템에 많은 디바이스를 부착하는 것이 경제적으로 적합하다. 만약 드라이브들이 병렬로 작동하면,시스템에 많은 수의 드라이브를 가지는 것은 데이터를 읽고 쓰는 속도를 향상시키는데 많은 기회를 가진다. 더 나아가, 이 설정은 데이터 저장소의 신뢰성을 향상시키는데, 왜냐하면 불필요한 정보들이 다양한 드라이브에 저장되기 때문이다. 그러므로 한가지 기기의 실패가 데이터의 손실로 이어지지 않는다. **Redundant arrays of independent disks(RAIDs)**라 불리는 디스크 구성 기술의 다양성은 성능과 신뢰성 이슈를 조절하는데 사용된다.

과거에는, 작고 싼 디스크로 구성된 RAID들은 크고 비싼 디스크들의 비용 효율면에서 쓰였다. 오늘날, RAIDs는 높은 신뢰성과 빠른 데이터 속도 떄문에 경제적인 이유는 무시된다. 여기서 RAID의 I는 한때 "inexpensive"로 쓰였지만, 이제는 "independent"로 쓰인다.

### 11.8.1 Redundancy를 이용한 Reliablility의 증가

HDDs의 RAID의 신뢰성을 고려하겠다. N개의 디스크의 점위에서 벗어난 몇몇 디스크는 특정 단일 디스크가 실패할 확률보다 크다. 단일 디스크의 **mean time between failures(MTBF)**는 100000시간이라고 가정하겠다. 100개의 어레이를 가진 디스크의 MTBF는 1000일 것이다.(41.66일) 여전히 길지는 않다. 만약 우리가 데이터의 하나의 카피를 저장하면, 각 디스크 실패는 엄청난 양의 데이터 손실을 초래하고 데이터 손실의 속도는 받아들이기 힘들것이다.

신뢰선의 문제 해결책은 **redundancy**이다. 우리는 일반적으로 필요하지 않지만 잃은 정보를 재건설해야하는 디스크 실패의 이벤트에서 사용되는 추가 정보를 저장한다. 그러므로, 디스크가 실패해도, 데이터들을 잃지않는다. RAID는 NVM기기에 적용될수 있지만, NVM 기기들은 움직이는 파트가 없고 그러므로 HDDs보다 덜 실패한다.

Redundancy를 소개하는 가장 간단한 접근은 모든 드라이브를 복제하는 것이다. 이 기술은 **mirroring**이라고 불린다. 미러링이 있으면, 논리 디스크는 두개의 물리 드라이브로 구성되고 모든 쓰기는 두 드라이브에 적용된다. 이 결과는 **mirrored volume**이라고 불린다. 만약 한개의 볼륨이 실패하면, 데이터는 다른 것으로부터 읽힌다. 데이터는 오직 첫번째 실패한 드라이브가 교체되기전에 두번쨰 드라이브가 실패할떄 잃게된다.

실패가 데이터의 손실인 미러링된 볼륨의 MBTF는 2가지 요소에 의존한다. 한가지는 개인 드라이브의 MTBF이다. 다른 것은 실패한 드라이브를 교체하고 그 데이터를 복구하는데 걸린 시간인 **mean time to repair**이다. 두 드라이브의 실패는 독립적이라고 가정하겠다. 즉, 한개의 실패는 다른 한개의 실패와 연결되있지 않다. 그떄, 만약 단일 드라이브의 실패가 100,000시간이고 평균 수리시간이 10시간이면, 미러된 드라이브 시스템의 **mean time to data loss**는 100000^2/(2*10) = 57000년이된다.

너는 드라이브가 실패가 실제로 독립적이라고 가정하면 안된다. 전력 실패, 지진, 화재, 홍수같은 자연재해는 동시에 두개의 드라이브 장애를 초래한다. 또한, 드라이브의 배치에서 결점을 생성하는 것은 연관된 실패를 유발한다. 드라이브가 낡으면, 실패의 확률은 증가하고, 첫번쨰 장비가 수리되는 도중에 두번쨰장비가 실패될 확률또한 늘어난다. 이런 고려에도 불구하고, 아무튼, 미러든 드라이브 시스템은 싱글 드라이브 시스템보다 훨씬 높은 신뢰성을 제동한다.

전력 차단은 자연재해보다 자주 일어나는 이유때문에 특별히 고려해야할 요소이다. 드라이브의 미러링이 있어도, 만약 쓰기가 두 드라이브의 같은 블럭에 실행중에 전력이 없어지면, 두 정보모두 inconsistent한 데이터를 가지게된다. 이 문제의 해결책은 한개를 먼저쓰고 다음을 쓰는 것이다. 다른 것은 RAID 행렬에 solid state nonvolatile 캐시를 추가하는 것이다. write back 캐시는 전력 차단에 의한 데이터 손실을 보호하고, 그래서 쓰기는 그 시점에서 완료되었다고 고려되고, 캐시가 에러 보호와 수정의 한 종류를 이룬다.

### 11.8.2 Improvement in Performance vis Parallelism

이제 어떻게 다중 드라이브의 병렬 연결이 성능을 향상시키는지 보겠다. 미러링과 함께, 읽기 요청을 처리하는 속도는 두배가 되었고, 이전에는 각 드라이브에 요청이 보내졌다. 각 읽기의 전송 속도는 싱글 드라이브 시스템과 같지만, 읽는 속도는 두배가 된 것이다.

다중 드라이브와 함꼐, 우리는 전송속도를 높이고 드라이브를 통해서 데이터를 분해했다. 가장 단순한 형태에서, **data striping**은 **bit level striping**이라고 불리는 다중 드라이브의 바이트의 비트를 분해하는 것이다. 예를 들어서 우리가 8개의 드라이브를 하나로 사용하면 일반적인 사이즈보다 8배의 속도를 가질 수 있다. 모든 드라이브는 모든 접근에 사용된다.(read or write). 그래서 초당 연산가능한 접근의 수는 싱글 드라이브와 같지만, 각 접근은 싱글 드라이브에 비해서 동시간에 8배를 읽어낸다.

비트레벨 스트리핑은 8의 배수거나 8로 나누어지는 수의 드라이브를 포함한다. 예를 들어서 만약 우리가 4개의 형랠 드라이브를 사용하면, 비트 i와 4+i가 드라이브 i에서 읽힌다. 더나아가서, 스트리핑은 비트레벨에서 일어날 필요는 없다. **block level striping**에서는, 예를 들어, 파일의 블럭은 다중 드라이브에서 스트립된다. n 드라이브면, 파일의 i블록은 (i mod n)+1의 드라이브에서 읽힌다. 다른 섹터의 바이트 또는 블럭의 섹터레벨의 스트리핑또한 가능하다. 현재 가장 많이 쓰이는 것은 Block level striping이다.

저장소 시스템의 병렬성은, 스트리핑에 의해서 성취된 것은 두가지 주요한 목적을 가진다.
1. 로드 밸런싱을 통해서 다중 접근의 산출량을 증가시켰다.
2. 큰 엑세스에 대한 반응시간을 감소시켰다.

### 11.8.3 RAID Levels

미러링은 높은 신뢰도를 주지만, 값비싸다. 스트리핑은 높은 데이터 전송 속도를 보장하지만, 그것은 신뢰성을 높이지는 못한다. 패리티 비트를 가진 디스크 스트리핑을 사용함으로서 redundancy를 줄이는 다양한 구조가 제안되었다. 이런 구조들은 다른 비용-성능 트레이드오프를 가지고 **RAID levels**라고 불리는 레벨로 분류되었다. 우리는 일반적인 레벨들만 알려주겠다. 

- **RAID level 0** 레벨 0은 드라이브 어레이에 블럭의 레벨에 따른 스트리핑은 있지만, redundancy(미러링 또는 패리티 비트)가 없는 것이다.
- **RAID level 1** 레벨 1은 드라이브 미러링을 의미한다.(두배의 드라이브가 필요하다.)

#### 11.8.3.1 RAID level 4

레벨 4는 memory style error correcting code(ECC) 구조이다. ECC는 레벨 5와 6에서도 쓰인다.

ECC의 아이디어는 드라이브를 걸쳐서 블럭의 스트리핑을 통해서 저장소 행렬에 사용된다. 예를 들어서, 첫번쨰 데이터 블럭의 시퀀스의 쓰기는 드라이브 1에 저장되고, 두번쨰는 드라이브 2에 저장되고 N까지 이어진다. 에러 수정 계산은 드라이브 N+1에 저장이된다. 이 드라이브는 P라고 라벨링 해두고 에러 수정 블럭을 저장한다. 만약 드라이브중 하나가 싪하면, 에러 수정 코드 계산이 에러를 감지하고 잘못된 데이터가 요청하는 프로세스에게 에러를 주면서 에러를 건내는 것을 방지한다.

RAID 4는 한개의 ECC 블럭으로 실제로 에러를 수정한다. 그것은 메모리 시스템과 다르게, 드라이브 컨트롤러가 섹터가 정확한지 감지한다. 그래서 한개의 패리티 블럭이 에러 감지와 수정에 사용될 수 있다. 아이디어는 다음과 같다. 만약 한개의 섹터가 손상되면, 우리는 그것이 어떤 섹터인지 정확히 알고 있다. 우리는 그 섹터의 데이터를 무시하고 잘못된 데이터를 페리티 데이터를 통해서 재계산한다. 블럭의 모든 비트에 대해서, 우리는 다른 드라이브에 있는 섹터의 상응하는 비트의 패리티가 1인지 0인지 결정한다. 만약 남은 비트의 패리티가 저장된 패리티와 같으면, 잃은 비트는 0이고 아니면 1이다.

블럭 읽기는 오직 하나의 드라이브에 접근되고, 다른 드라이브에 의해서 프로세스되는 요청을 허용한다. 모든 디스크가 병렬로 읽히기에, 대량의 읽기를 위한 속도는 높다. 쓰기는 높은 전송속도를 가지는데, 데이터와 패리티가 병렬로 쓰일수 있기 때문이다.

작은 독립적인 쓰기는 병렬로 실행하지 못한다. 운영체제는 블록보다 작은 데이터를 쓰는 것은 새로운 데이터가 읽힐떄, 새로운 데이터로 수정되어서 다시 쓰이는 조건을 필요로한다. 패리티 블록은 같이 업데이트 되어야한다. 이것은 **read-modify-write cycle**이라고 불린다. 그러므로, 단일 쓰기는 4번의 드라이브 접근을 필요로한다. 두개는 오래된 블럭에 읽기, 두개는 새로운 블럭에 쓰기이다.

WAFL(14장에 나옴)은 RAID level4를 이용하는데, 이 레벨은 드라이브가 RAID 셋에 매끄럽게 추가되는 성질때문이다. 만약 추가된 블럭이 오직 0을 가진다면, 패리티 값은 바뀌지 않고, RAID 셋은 조정된다.

RAID 레벨 4는 레벨 1에 비해서 두가지 장점이 있다. 저장소 오버헤드가 여러개의 드라이브에 하나만 필요하고, 반면에 미러링은 모든 드라이브가 필요하다. 두번쨰, 블록의 연속의 읽기와 쓰기가 데이터의 N가지 방법으로 읽기에, 블럭을 읽고 쓰는 속도는 레벨 1에 비해서 N배 빠르다.

RAID 4의 성능 문제는, 모든 패리티 베이스 RAID 레벨의 문제는, XOR 패리티 쓰기와 계산의 비용이다. 이 오버헤드때문에 패리티 RAID가 아닌 행렬에 비해서 느리게 쓰게된다. 현대의 CPU는 드라이브 I/O에 비해서 매우 빠르지만, 그래서 퍼포먼스 적합도는 적다. 또한 많은 RAID 저장소 행렬 또는 호스트 버스 어댑터는 전용 패리티 하드웨어에 하드웨어 컨트롤러를 가진다. 이 컨트롤러는 CPU 행렬의 계산에서 벗어났다. 행렬은 패리티가 계산되는 동안 블럭을 저장하고 컨트롤러에서 드라이브로 쓰는 것을 완충하는 NVRAM 캐시를 가진다. 이런 버퍼링은 전체 스르타이프에서 쓸 데이터 수집 때문에 read-modify-write 사이클을 피하고 스트라이프를 동시에 모든 드라이브에 쓴다. 하드웨어 가속과 버퍼링의 혼합은 패리티 RAID를 느리게 한다.

#### 11.8.3.2 RAID level 5

RAID level 5 또는 block interleaved distributed parity는 level 4와는 다르게 N개의 드라이브에 1개의 패리티가 아니라 N+1개에 데이터와 패리티가 공존하고 있다. 각 N개의 블럭 집합에, 1개의 드라이브는 패리티를 저장하고 나머지는 데이터를 저장한다. 예를 들어 5개의 드라이브가 있으면, n번째 블럭의 패리티는 (n mod 5)+1의 블럭에 패리티가 저장된다. n번째 블럭은 4개의 드라이브가 실제로 저장해야할 데이터를 저장하고 있다. 패리티 블럭은 같은 드라이브에 패리티를 저장할 수 없는데, 왜냐하면 드라이브의 실패가 패리티또한 잃게 하기 때문이고, 따라서 복구할수가 없기 떄문이다. 패리티를 모든 드라이브에 퍼뜨림으로서, RAID 5는 단일 패리티 드라이브의 잠재적인 overuse를 방지한다. RAID 5는 parity RAID에서 가장 일반적인 방식이다.

#### 11.8.3.3 RAID level 6

level 6는 **P + Q redundancy scheme**이라고 불리고, 레벨 5와 비슷하지만 다중 드라이브 실패에 대비할 추가적인 redundant 정보가 존재한다. XOR 패리티는 같고 추가 회복 정보를 더 줄수 없기 떄문에 동시에 두 패리티 블럭에 사용될 수없다. 패리티 대신에, **Galois field math**라는 코드가 Q를 계산하기 위해서 존재한다. 4개의 블럭을 위해서 2개의 블럭이 사용되고 이 시스템은 두개의 드라이브 실패를 견뎌낼 수 있다.

#### 11.8.3.4 Multidimensional RAID level 6

몇몇 정교한 저장소 행렬은 RAID level 6를 증폭시킨다. 수백개의 행렬을 포함하는 드라이브를 고려해보겠다. RAID level6에서 이 방법을 사용하는 것은 많은 데이터 드라이브와 오직 2개의 논리 패리티 드라이브를 필요로한다. 다차원 RAID 레벨 6은 논리적으로 드라이브를 행과 열로 정렬하고 RAID 레벨 6을 수평적으로 행을 따르고 수직적으로 열을 따라서 구현된다. 이 시스템은 어떠한 실패로부터든 회복가능하다.

#### 11.8.3.5 RAID levels 0+1 and 1+0

RAID 레벨 0+1은 RAID level 0과 1을 섞은 것이다. RAID 0은 성능을 제공하고, RAID 1은 신뢰성을 제공한다. 일반적으로, 이 레벨은 RAID 5보다 좋은 성능을 가진다. 이 환경은 성능과 신뢰성이 모두 중요한 곳에서 일반적이다. 불행히도, RAID 1은 저장소에 필요한 드라이브의 수를 두배로 늘리고, 그래서 그것은 상대적으로 비싸다. RAID 0+1에서, 드라이브의 집합들은 stiped되어 있고 stripe는 다른  동일한 stripe에 미러된다.

#### 11.8.3.6 Another implementation

다른 RAID 변형은 RAID level 1+0인데, 드라이브가 쌍으로 미러되고 미러링된 결과 페어가 stripe된다. 이 구조는 0+1보다 이론적으로 더 큰 이득을 얻을 수 있다. 예를 들어서, 만약 단일 드라이브가 0+1에서 실패하면, 단일 드라이브는 존재하지 않지만, 미러된 드라이브는 존재하고, 드라이브의 나머지또한 마찬가지이다. 다양한 변형이 기본 RAID 구조에서 제안되었다. 결과적으로, 다른 RAID 레벨의 정확한 정의에 대한 혼동이 생긴다.

다양한 영역에서의 RAID 변형이 존재한다. 어떤 RAID가 계층별로 구현되었는지 보겠다.

- volume-management 소프트웨어는 커널 또는 시스템 소프트웨어 계층에서 RAID가 구현된다. 이 경우에는, 하드웨어 저장소는 최소 기능을 제공하고 full RAID 해결책의 파트이다.
- RAID는 host bus-adapter(HBA) 하드웨어에서 구현가능하다. HBA에 연결된 드라이브는 RAID 집합을 받는다. 이 해결책은 비용은 낮지만 유연하지 못하다.
- RAID는 저장소 행렬의 하드웨어에 구현이 가능하다. 저장소 행렬은 다양한 레벨의 RAID 집합을 반들수 있고 심지어는 운영체제에 보여지는 작은 볼륨으로 집합을 나눌수있다. 운영체제는 각 볼륨에 파일 시스템을 구현할 필요만 있다. 행렬은 다양한 연결을 하고 SAN의 일부가 될 수 있고, 다양한 호스트가 행렬의 기능 장점을 가지게 한다.
- RAID는 SAN 인터커넥트 레이어에 드라이브 가상화 기기로 구현이 가능하다. 이 경우에, 디바이스는 호스트와 저장소사이에 존재한다. 그것은 서버로부터 커맨드를 수용하고 저장소로의 접근을 관리한다. 이것은 미러링을 제공하는데, 두가지 분리된 저장소 기기에 쓰는 것으로 구현한다.

다른 스냅샷이나 replication 같은 기능은 각 레벨에서 구현이 가능하다. **snapshot**은 최근 업데이트 이전에 파일시스템을 복사하는 것이다.(14장에서 추가 설명함) **Replication**은 분리된 공간 사이에 쓰기의 자동 복제를 포함한다. 복제는 동기적이거나 비동기적일 수 있다. 동기화 복제에서, 각 블럭은 전체 쓰기가 완료 되기전에, locally와 remotely로 쓰인다. 반대로 비동기 복제에서는, 쓰기는 함꼐 그룹화되고 주기적으로 쓰인다. 비동기 복제는 만약 첫번째 공간이 실패했을때 데이터 손실을 일으키지만, 빠르고 거리의 제한이 없다. 점점더, 복제는 데이터 센터 또는 호스트사이에서 사용되고 있다. RAID 보호의 대안으로서, 복제는 데이터 손실 방지와 읽기 성능 향상을 얻는다. 그것은 물론 대부분의 RAID보다 더 많은 저장소를 사용한다.

이 기능의 구현은 어떤 레이드가 구현된 계층에 따라서 달라진다. 예를 들어서, 만약 RAID가 소프트웨어에서 구현되면, 각 호스트는 그들의 복제본을 챙기고 관리할 필요가 있다. 만약 복제가 저장소 행렬 또는 SAN 인터커넥트에서 구현되면, 어떤 호스트 운영체제 또는 기능에서든지 호스트의 데이터는 복제되어야한다.

RAID 구현의 다른 한가지 측면은 hot spare drive or drives이다. **hot spare**은 데이터를 위해서 사용되지 않고 드라이브 실패시에 교체하는 용도로 쓰인다. 이런 경우에, RAID 레벨은 자동적으로 재구축되고, 실패된 드라이브를 교체할 필요가 없다. 한개 이상의 hot spare를 허용하면 하나 이상의 실패 수리를 사람의 개입없이 해결 가능하다.

### 11.8.4 레이드 레벨 선택

우리가 가진 많은 선택 중에서, 어떻게 시스템 디자이너가 RAID 레벨을 고를수 있을까? 한가지 고려사항은 성능을 재구축하는 것이다. 만약 드라이브가 실패하면, 그 데잍터를 재구축하는데 걸리는 시간이 중요할 것이다. 이것은 만약 데이터의 연속적인 공급이 필요한 고성능 또는 상호작용 데이터 베이스 시스템에서 중요하다. 더 나아가, 재구축 성능은 실패간의 평균시간에도 영향을 미친다.

재구축 성능은 레이드 레벨에 따라서 다양하다. 레벨 1에서는 재구축이 그저 다른 드라이브로부터 복사만 하면되기에 매우 간단하다. 다른 레벨에서는, 우리는 실패한 데이터를 찾기위해서 모든 드라이브를 접근할 필요가 있다. 레벨 5에서의 재구축시간은 큰 드라이브 집합에서는 시간단위가 된다.

레벨 0은 데이터 손시링 중요하지 않은 고성능 앱에서 사용된다. 예를 들어서, 데이터 셋이 로드하고 탐사하는 과학 컴퓨팅에서, 레벨 0이 적합한데 어떤 드라이브 실패는 오직 소스로부터 데이터를 리로드하고 고치기만 하면 되기 떄문이다. 레벨 1은 빠른 복구와 높은 신뢰성이 필요한 앱에서 필요하다. 0+1과 1+0은 성능과 신뢰성이 중요한 곳에서 사용된다.(작은 데이터베이스) 레벨 1의 높은 공간 오버헤드때문에, 레벨 5는 일반적인 데이터 볼륨 저장에 선호된다. 레벨 6과 다차원 레벨 6은 저장 행렬에서 가장 일반적인 형태이다. 그들은 좋은 성능과 보호를 큰 공간 오버헤드 없이 제공한다.

저장소의 레이드 시스템 디자이너와 관리자는 몇가지 다른 결정을 만들어야한다. 예를 들어서, 얼마나 많은 드라이브가 주어진 레이드 집합에 있어야할까? 얼마나 많은 비트가 각 패리티 비트에 의해서 보호되어야 할까? 만약 더 많은 드라이브가 행렬에 오면, 데이터 전송 속도는 빨라질 것이고, 그러나 시스템은 비싸ㅣㄴ다. 만약 많은 비트가 패리티에 의해서 지켜지면, 패리티 비트에 의한 오버헤드는 낮아지지만, 두번째 드라이브가 실패할 확률이 커지고 데이터 손실을 야기한다.

### 11.8.5 확장

RAID의 컨셉은 테이프, 무선 시스템의 데이터 송신같은 다른 저장 기기에도 일반화되었다. 테이에 적용될때, 레이드 구조는 오직 하나의 테이프가 손상되어도 회복이 가능하다. 데이터의 통신에 적용되면, 데이터의 블럭은 짧은 유닛에 분할되고 다른 패리티 유닛을 따라서 통신된다. 만약 하나의 유닛이 어느 이유로 수신되지 않으면, 그것은 다른 유닛으로부터 회복된다. 일반적으로, 여러개의 테이프 드라이브를 포함하는 테이프 드라이브 로봇은 모든 드라이브에 걸쳐서 데이터를 stripe하고 산출량을 늘리고 백업시간을 감소시킨다.

### 11.8.6 Problems with RAID

불행히도, 레이드는 항상 운영체제와 유저를 위해서 데이터를 available하게 장담하지 않는다. 파일을 향한 포인터가 잘못되거나 파일 구조의 포인터가 잘못된다. 불완전한 쓰기("torn writes")가 적절히 복구되지 않으면, 데이터를 망친다. 다른 프로세스는 파일 시스템 구조에 의도치 않게 쓸수 있다. RAID는 물리적 미디어 에러는 보호하지만, 다른 하드웨어와 소프트웨어 에러를 보호하지는 않느다. 하드웨어 RAID 컨트롤러의 실패 또는 소프트웨어 RAID의 버그는 전체 데이터 손실을 야기하지 않는다. 소프트웨어와 하드웨어의 버그 규모가 큰만큼, 시스템의 데이터 잠재적인 위험또한 존재한다.

**Solaris ZFS** 파일 시스템은 checksum 시스템을 통해서 이러한 문제를 해결했다. ZFS는 데이터와 메타데이터를 포함한 모든 블럭의 내부적인 checksums를 유지했다. 이러한 체크섬들은 세크섬된 블럭에 보관되지 않았다. 대신에, 그들은 그 블럭을 가르키는 포인터에 저장되었다. **inode**라는 파일 시스템 메타데이터(그것의 데이터를 가르키는 포인터)를 저장하는 데이터 구조이다. inode안에서 데이터의 각 블럭의 체크섬이다. 만약 데이터에 문제가 있으면, 체크섬은 일치하지 않고, 파일시스템이 알게된다. 만약 데이터가 미러되고, 정확한 체크섬을 가진 블럭과 아닌 블럭이 있다. ZFS는 자동적으로 나쁜 것을 좋은 것으로 바꾼다. 비슷하게, 아이노드를 가르키는 디렉토리 엔트리는 아이노드의 체크섬을 가진다. 아이노드의 문제는 디렉토리가 접근될떄 감지된다. 이 checksumming은 모든 ZFS 구조에서 사용되고, RAID 드라이브 또는 일반적인 파일 시스템보다 좋은 일관성, 에러 감지, 에러 수정을 제공한다. 체크섬 계산과 추가 블럭 읽기-수정-쓰기 사이클은 ZFS의 성능이 매우 빠르기 떄문에 알기힘들다.(LINUX BTRFS 파일시스템)

대부분의 RAID 구현의 문제는 유연성의 부재이다. 20개의 드라이브가 5개의 드라이브로 4개의 집합으로 나누어져있는 저장소 행렬을 고려하겠다. 결과적으로, 4개의 분리된 볼륨을 각각이 파일시스템을 가진다. 그러나 만약 한개의 파일시스템이 5개의 RAID 5 집합보다 크다. 그리고 다른 파일 시스템은 적은 공간을 필요로한다. 이런 문제는 이미 알고 있었고, 드라이브와 볼륨은 적절히 할당되어야한다. 매우 자주, 드라이브 사용과 필요는 시간을 걸쳐서 변화했다.

저장소 행렬이 20개의 드라이브를 하나이 전체 RAID 셋으로 생성했다해도, 다른 문제는 발생한다. 몇몇 다양한 사이즈의 볼륨은 집합에서 생성된다. 그러나 볼륨 매니저는 우리가 볼륨의 크기를 바꾸도록하지 않는다. 이런 경우에, 우리는 위에서 말한 같은 문제가 남는다. 몇몇 볼륨 매니저는 크기를 변경하게 허용하지만, 다른 파일 시스템은 파일 시스템 성장/축소를 허용하지 않는다. 볼륨은 크기를 바꿀수 있어야지만, 파일 시스템은 이런 변화의 장점을 위해서 재생성되어야한다.

ZFS는 파일 시스템 관리와 볼륨관리를 전통적인 기능 분리가 아닌 큰 기능을 제공하는 하나의 유닛으로 합친 것이다. 드라이브, 또는 드라이브의 파티션들은 저장소의 풀에서 RAID 셋을 통해서 합쳐진다. 풀은 하나또는 여러개의 ZFS 파일 시스템을 가진다. 전체 풀의 여유 공간은 풀안에서 모든 파일 시스템에 존재한다. ZFS는 파일 시스템의 저장소를 해제하고 할당하기 위해서 malloc()과 free()의 메모리 모델을 사용한다. 블럭이 파일 시스템 내에서 사용되고 해제된다. 결과적으로, 저장소 사용에는 인위적인 제한이 없고 볼륨사이에 파일 시스템을 재할당하거나 볼륨을 재조정한다. ZFS는 파일 시스템의 크기를 제한하는 한도를 제공하고, 파일 시스템이 특정 크기만큼 커져서 보존을 확실하게 하고, 그러나 이러한 변수는 어떤 시간에 파일 시스템이 변했는지이다. 리눅스 같은 시스템은 볼륨 매니저가 여러개의 디스크의 논리적인 연결을 통해서 큰 파일 시스템을 가진 큰 디스크 볼륨 생성을 허용한다.

### 11.8.7 Object Storag

범용 목적 컴퓨터들은 일반적으로 파일 시스템을 유저 컨텐츠를 저장하기 위해서 사용한다. 데이터 저장소의 다른 접근은 저장소 풀을 쓰고 풀에 객체를 두는 것이다. 이 접근은 풀을 탐색하고 이 객체를 찾을 방법이 없는 것이 파일 시스템과의 차이이다. 그러므로, 유저 기반보다는 객체 저장소는 컴퓨터 기반이고, 프로그램에 의해서 사용되게 디자인되었다. 일반적인 절차는 다음과 같다.
1. 저장소 풀에 객체를 만들고, 객체 ID를 받는다.
2. 오브젝트 ID를 통해서 오브젝트가 필요할때 사용한다.
3. 오브젝트 ID를 통해서 객체를 삭제한다.

**Hadoop file sysetem(HDFS)**와 **Ceph**같은 오브젝트 저장소 관리 소프트웨어는 어디에 객체를 저장하고 객체 보호를 관리할지 결정한다. 일반적으로, 이것은 저장소 행렬보다는 상업용 하드웨어에서 자주 쓰인다. 예를 들어서 HDFS는 N개의 카피의 객체를 N개의 다른 컴퓨터에 저장한다. 이 접근은 저장소 행렬보다 비용이 낮고 객체에 대한 빠른 접근을 제공한다. 하둡 클러스터의 모든 시스템들은 객체에 접근이 가능하지만, 카피를 가진 시스템만이 카피를 통해서 빠르게 접근할 수 있다. 데이터에 대한 계산은 시스템에서 일어나고, 네트워크를 통해서 보내어지고, 시스템이 그들을 요청한다. 다른 시스템들은 객체를 읽고 쓰는데 네트워크 연결을 필요로한다. 그러므로, 객체 저장소는 bulk storage를 사용하고, 빠른 속도의 랜덤 접근이 아니다. 오브젝트 저장소는 **horizontal scalability**의 장점을 가진다. 즉, 저장소 행렬이 고정된 최대 용량을 가지는데 비해서, 객체 저장에 용량을 더하는 것은 우리는 간단하게 내부 디스크또는 외부 디스크를 컴퓨터에 추가하고 그들을 풀에 넣는것 뿐이다. 객체 저장소 풀은 페타 바이트가 가능하다.

객체 저장소의 다른 주요 기능은 각 객체가 그것의 컨텐츠의 설명을 포함한 self-describing이다. 실제로, 객체 저장소는 **content addressable storage**인데, 왜냐하면 객체가 그들의 컨텐츠에서 검색이 가능하기 떄문이다. 컨텐츠에는 집합 형식이 없고, 시스템은 **Unstructured data**를 저장한다.

객체 저장소가 범용 목적 컴퓨터에서 일반적이지 않기에, 많은 양의 데이터는 객체 저장소에 저장되는데, 구글의 인터넷 검색 컨텐츠, 드랍박스 컨텐츠, 스포티파이의 노래, 페이스북 사진이 있다. 클라우드 컴퓨팅 또한 객체 저장을 고객 앱이 클라우드 컴퓨터를 작동하는 파일 시스템을 저장하기 위해서 사용한다. 객체 저장의 역사는 http://www.theregister.co.uk/2016/07/15/the_history_boys_cas_and_object_storage_map 에서 볼수 있다.