## what we gonna study

이 장에서, 우리는 컴퓨터의 비휘발성 대량 저장소가 어떻게 구성되는지 보겠다. 주요 대량 저장소 시스템은 2차 저장소이고, HDD와 NVM을 통해서 제공된다. 몇몇 시스템은 더욱 크고 3차저장소인 자기 테이프, 광학 디스크, 클라우드 저장소를 가진다.

현대에서 가장 중요하고 일반적인 저장소는 HDD와 NVM이고, 두 저장소에 대한 설명이 대부분일 것이다. 우리는 I/O의 성능을 최대화하는 스케쥴링 알고리즘을 살펴보겠다. 다음으로, 우리는 디바이스 포매팅과 부트 블럭 관리, 손상 블럭, 스왑스페이스에 대해서 논의하겠다. 마지막으로, RAID 시스템의 구조를 확인하겠다.

대량 저장소에는 다양한 종류가 있고, 우리는 보통 *nonvolatile storage(NVS)*라고 부른다. 그리고 모든 타입의 기기를 일반적으로 "drives"라고 표현하겠다. HDD와 NVM같은 특정 기기는 정확하게 정의될 것이다.

## objectives

- 다양한 2차 저장소의 물리적인 구조와 사용에 맞게 디바이스의 구조 효과를 설명하겠다.
- 대량 저장 기기의 성능 성질을 설명하겠다.
- I/O 스케쥴링 알고리즘을 평가하겠다.
- RAID를 포함한 대량 저장소를 위해 운영체제가 제공하는 서비스를 보겠다.

## 11.1 Mass-Storage Structure

현대 컴퓨터의 2차 저장소는 대부분 **hard disk drives(HDDs)**와 **nonvolatile memory(NVM)**으로 제공된다. 이 절에서, 우리는 이런 기기의 메커니즘을 설명하고 어떻게 운영체제가 주소 매핑을 통해서 그들의 물리적 공간을 논리주소로 번역하는지 보겠다.

### 11.1.1 Hard Disk Drives

개념적으로, HDD들은 꽤나 단순하다. 각 디스크 **platter**는 평평한 원형이다. 일반적인 플래터 지름은 1.8~3.5인치이다. 플래터의 두 표면은 자성 물질로 덮여있다. 우리는 플래터위에서 자기적으로 저장하고, 우리는 플래터위의 자기적인 패턴을 감지해서 정보를 읽는다.

읽기-쓰기 헤드는 모든 플래터의 위에 있다. 그 헤드들은 헤드를 하나의 유닛처럼 움직여주는 **disk arm**에 붙어있다. 플래터의 표면은 **sector**로 다시 나누어지는 원형 **tracks**로 논리적으로 나누어진다. 주어진 암 위치의 트랙의 집합은 **cylinder**를 만든다. 디스크 드라이브에는 수천개의 동심원 실린더가 있고, 각 트랙은 수천개의 섹터를 가진다. 각 섹터는 고정된 크기를 가지고 전송의 최소 단위이다. 섹터 크기는 2010년에는 512바이트였다. 그 시점에서, 많은 제조사들이 4KB 섹터로 이전하기 시작했다. 일반적으로 디스크의 커패시티는 기가바이트와 테라바이트이다. 

디스크 드라이브 모터는 높은 속도로 회전한다. 대부분의 드라이브는 초당 60~250번을 회전하고 **rotations per minutes(RPM)**이라고 부른다. 일반적인 드라이브는 5400, 7200, 10000, 15000의 RPM을 가진다. 몇몇 드라이브는 사용되지 않거나 I/O 요청을 기다릴때까지 전원을 꺼둔다. 회전 속도는 전송 속도와 관계가 있다. **Transfer rate**는 드라이브와 컴퓨터 사이에 데이터가 흐르는 속도이다. 다른 성능 측면은 **positioning time**또는 **random-access time**은 두가지 부분으로 구성된다. 첫번쨰는 원하는 실린더에 디스크 암을 이동시키는 **seek time**이고 원하는 섹터를 회전시키는 **rotational latency**이다. 일반적인 디스크는 초당 100메가바이트를 전송하고, 그들은 탐색 시간과 회전 지연에 걸리는 시간은 몇 미리초에 불과하다. 그들은 드라이브 컨트롤러에 DRAM 버퍼르 가짐으로서 성능을 향상시킬 수 있다.

디스크 헤드 파일들은 헬륨 같은 가스로 이루어진 극단적으로 얇은 쿠션위에 있고, 헤드가 디스크 표면을 접촉할 위험이 있다. 디스크 플래터가 얇은 보호막으로 코팅되어도, 헤드는 자기 표면을 손상시킨다. 이런 사고를 **head crash**라고 부른다. 이 사고는 고쳐질 수 없다. 전체 디스크는 교체되어야하고, 데이터의 디스크는 다른 저장소에 백업 되거나 RAID로 보호되있지 않으면 영영 사라진다.

HDDs들은 봉인된 유닛이고, 몇몇 HDD를 쥐고 있는 chassis는 시스템의 종료나 저장소 chassis없이 그들의 제거를 허용한다. 이것은 시스템이 더 많은 저장소를 필요로하거나 그것이 배드드라이브를 일하는 것과 교체하기 위해서 필요하다. 다른 타입의 저장 미디어도 **removalble**하다.

### 11.1.2 Nonvolatile Memory Devices

NVM 디바이스는 점점 중요해지고 있다. 간단히 말하면 NVM 장치는 기계라기보다는 전기적인 장치이다. 가장 일반적으로, 이런 디바이스는 컨트롤러와 플래시 NAND 반도체 칩으로 구성되어있다. 다른 NVM 기술은 존재하는데, 배터리가 있는 DRAM은 그것의 컨텐츠를 잃지않고, 그러나 그들은 덜 일반적이고 책에서 다루지 않는다.

#### 11.1.2.1  Overview of NVM

NVM 베이스의 플래시 메모리는 컨테이너 같은 디스크 드라이브에서 사용되고, **solid state disk**라고 부른다. 다른 예시로는 **USB drive**또는 DRAM 스틱이 있다. 그것은 또한 스마트폰 기기에서 메인 저장소로서 마더보드의 위에 마운트되어있다. 모든 형태에서, 그것은 같은 방식으로 작동한다. NVM 기기의 주된 논의는 이 기술에 대한 것이다.

NVM 기기는 움직이는 부분이 없기 때문에 HDD보다 믿음직하고 탐색시간이나 회전 지연이 없기 때문에 더욱 빠르다. 추가적으로 그들은 적은 파워를 소모한다. 부정적인 측면은, 그들은 고전적인 하드디스크보다 비싸고 큰 하드디스크보다 적은 용량을 가진다. 시간에 걸쳐서, NVM 디바이스의 용량은 하드디스크의 용량 증가속도를 넘어섰고, 그들의 가격은 더욱 빠르게 떨어지고 있다. 그래서 그들의 사용은 급격히 증가하고 있다. 실제로, SSDs와 비슷한 기기는 더 작고 빠르고 에너지 효율적인 랩탑을 위해서 사용되고 있다.

NVM 기기가 하드디스크보다 더욱 빠르기 때문에, 표준 버스 인터페이스는 산출량에 주요한 리미트를 초래한다. 몇몇 NVM 기기들은 시스템 버스(PCIe)에 직접 연결되게 디자인된다. 이 기술은 전통적인 컴퓨터 디자인을 바꾸고 있다. 몇몇 시스템은 디스크 드라이브의 직접 교체로 사용하고, 그것을 새로운 캐시 티어에 넣기도하고, 성능을 최적화하기 위해서 자기 디스크, NVM, 메인 메모리 사이의 데이터를 움직인다.

NAND 반도체는 그들의 저장소와 신뢰성 문제를 가직 있다. 예를 들어서, 그들은 "page" 증가(섹터와 비슷하다.)를 통해서 읽고 쓰는데, 그러나 데이터는 NAND 셀이 지워지기 이전에는 덮어쓰기가 안된다. "block" 증가를 초래하는 삭제는 크기가 여러 페이지이고, 읽기나 쓰기보다 훨씬 많은 시간이 필요하다. 이 상황을 돕기위해서 NVM 플래시 디바이스는 각각이 데이터 패스로 연결된 여러가지 금형으로 이루어져있고, 그래서 명령어는 병렬적으로 일어날 수 있다. NAND 반도체는 또한 여러 삭제 사이클에 의해서 감소되고, 100000 프로그램 삭제 사이클 후에, 각 셀은 더이상 데이터를 보관할 수 없다. write wear 때문에, 더이상 움직일 수 없는 파트때문에, NAND NVM 수명주기는 년단위가 아니라 **Drive Writes Per Day(DWPD)**로 측정된다. 그 측정은 드라이브가 고장나기 전까지 드라이브가 매일 쓸수있는지를 측정한다. 예를 들어서 5DWPD를 가진 1TB NAND 드라이브는 보장기간동안 5TB를 매일 쓸수있다.

이런 제한은 몇가지 개선 알고리즘을 이끌었다. 다행히도, 그들은 NVM 기기에 구현이 되었고 운영체제의 역할이 아니다. 운영체제는 단순히 논리 블럭을 읽고 쓰고, 디바이스는 어떻게 되는지 매니지한다. 그러나, NVM 디바이스는 운영체제 알고리즘에 따라서 다양한 성능을 가지고, 그래서 컨트롤러가 무엇을 하는지에 대한 간단한 설명이 필요하다.

#### 11.1.2.2 NAND Flash Controller Algorithms

NAND 반도체가 한번 쓰이면 덮어쓰기가 안되기에, invalid 데이터를 포함한 페이지가 있다. 파일 시스템 블럭을 고려하면, 한번 쓰이고 후에 다시 쓰인다. 만약 그 기간동안 삭제가 일어나지 않으면, 페이지는 예전 데이터를 가지고, 현재는 invalid하다. 그리고 두번째 페이지가 현재의 정확한 버전의 블럭을 가진다. 유효하고 유효하지 않은 페이지를 NAND 블럭이 포함하고 있다. 어떤 논리 블럭이 유효 데이터를 가지고 이쓴지 추적하기 위해서, 컨트롤러는 **flash translation layer**를 유지한다. 이 테이블은 어떤 물리 페이지가 현재 유효 논리 블럭을 가진지 매핑한다. 그것은 또한 물리 블럭 상태를 추적하고 어떤 블럭이 유효하지 않은 페이지를 포함한지 확인하고 삭제시킨다.

이제 쓰기 요청을 기다리는 full SSD를 고려하자. SSD가 가득 찼기 때문에, 모든 페이지들은 쓰여있고, 그러나 이 상황에서 블럭들은 유효 데이터가 없을 수도 있다. 이런 경우에, 쓰기는 삭제가 이루어지기를 기다릴 수 있고, 그후에 쓰기가 일어난다. 그러나 만약 여유 블럭이 없다면 어떻게 될까? 만약 페이지들이 invalid data를 포함한다면 가용한 공간이 있을 것이다. 그런 경우에는, **garbage colletion**이 일어나고 좋은 데이터는 다른 공간에 복사되고 블럭을 깔끔히 한후에 쓰기 요청을 받을 수 있다. 그러나, 가비지 컬렉션은 어디에 좋은 데이터를 저장할까? 이 문제를 해결하고 쓰기 성능을 증가시키기 위해서, NVM 디바이스는 **overprovisioning**을 사용한다. 디바이스는 쓸 수 있는 페이지를 보통 전체의 20%정도로 유지한다. 가비지 컬렉션에 의해서 완벽히 유효하지 않은데이터, 데이터의 오래된 버전들은 over porvisioning 공간에 두고 만약 디바이스가 가득차면 여유 풀로 리턴한다.

over provisioning 공간은 **wear leveling**을 돕는다. 만약 몇몇 블럭이 반복적으로 지워지면, 그 지워지는 블럭은 다른 것들보다 빨리 닳고, 전체 기기는 만약 블럭이 동시적으로 닳는 것보다 짧은 수명주기를 가질 것이다. 컨트롤러는 적게 삭제된 블럭에 데이터를 위치해서 전체기기의  닳은 정도의 밸런스를 맞추는 다양한 알고리즘을 사용한다.

데이터 보호에서, NVM 디바이스는 에러 수정 코드를 제공하는데, 데이터를 읽고 쓰는 과정에서 에러를 찾아내고 고치는 것을 저장하고 계산한다. 만약 페이지가 자주 에러를 가지면, 페이지는 나쁘다고 마킹되고 다음 쓰기에서 사용되지 않는다. 일밙적으로, 단일 NVM 기기는 정보가 썩거나 쓰기/읽기 요청에 답하지 못할때 재앙적인 실패를 가진다. 이런 것을 복구하기 위해서 RAID 보호가 사용된다.

### 11.1.3 Volatile Memory

대량 저장소 구조에 관한 장에서 휘발성 메모리를 논의하는 것은 이상할 수 있는데, 그것은 DRAM이 대량 저장 디바이스로 쓰이기 때문에 정당화가 가능하다. 특히, **RAM drives**(RAM 디스크라고도 불림)는 2차 저장소로 역할하지만, 시스템의 DRAM에서 발굴한 디바이스 드라이버에 의해서 생성되고 그것이 저장소였던 것처럼 작동한다. 이런 "drives"들은 raw block devices로 쓰일 수있고, 더욱 일반적으로, 파일 시스템들이 일반적인 파일 명령어로 생성된다.

컴퓨터들은 이미 버퍼링가과 캐싱을 가지는데, 그래서 임시 데이터 저장소를 위한 DRAM의 다른 목적은 무엇일까? DRAM은 휘발성이기에, RAM 드라이브의 데이터는 시스템 크래시, 종료, 파워 다운에서 살아남지 못한다. 캐시들과 버퍼들은 프로그래머 또는 운영체제에 의해서 할당되지만, RAM 드라이브는 일반적인 파일 명령어를 통해서 메모리에 둘 수 있다. 실제로, 램 드라이브 기능은 대부분의 운영체제에서 사용된다. 리눅스에서, /dev/ram, 맥에서는 diskutil 커맨드, 윈도우는 서드파티 툴, 솔라리스와 리눅스는 /tmp를 부트 시간에 만들고 그것이 RAM 드라이브이다.

램 드라이브들은 높은 속도의 임시 저장 공간에서 유용하다. 비록 NVM이 빨라도, DRAM이 더욱 빠르고, RAM 드라이브의 I/O 명령어는 생성, 읽기, 쓰기, 삭제가 그 무엇보다도 빠르다. 많은 프로그램들은 임시 파일을 저장하기 위해서 RAM 드라이브를 사용한다. 예를 들어서 프로그램들은 RAM 드라이브로의 공유 데이터를 통해서 읽기 쓰기를 쉽게 구현한다. 다른 예시로는, 리눅스는 시스템의 다른 파트를 루트 파일과 그것의 컨텐츠 엑세스를 가지는 것을 허용하는 임시 루트 파일 시스템(`initrd`)을 운영체제의 다른 파트가 저장 디바이스를 로드하기 전에 부트시간에 만든다.

### 11.1.4 Secondary Storage Connection Methods

2차 저장소 기기는 시스템 버스 또는 **I/O bus**로 연결되어있다. 몇가지 버스가 존재하는데, **advanced technology attatchment(ATA)**, **seral ATA(SATA)**, **eSATA**, **serial attatched SCSI(SAS)**, **universal serial bus(USB)**와 **fibre channel(FC)**가 있다. 대부분의 일반적인 연결 방식은 SATA이다. NVM 기기들이 HDD보다 빠르기 때문에, NVM 디바이르를 위한 빠른 인터페이스인 **NVM express(NVMe)**가 있다. NVMe는 시스템 PCI 버스로 연결되어있고, 산출량이 증가하고 다른 연결방식에 비해서 낮은 지연율을 가진다.

버스에서의 데이터 통신은 특별한 전자 프로세서인 **controllers**(or **host-bus adapters(HBA)**)를 통해서 전달한다. **host controller**는 컴퓨터 버스의 끝단에 있다. 대량 저장소 I/O 명령어를 실행하기 위해서, 컴퓨터는 호스트 컨트롤러에 명령을 주는데, 메모리 매핑 I/O 포트를 이용한다. 호스트 컨트롤러는 메시지를 통해서 커맨드를 전송하고 컨트롤러는 커맨드를 가져오기 위해서 드라이브 하드웨어를 실행한다. 디바이스 컨트롤러는 대개 캐시안에 있다. 드라이브에서의 데이터 전송은 캐시와 저장 매체에서 일어나고, 데이터는 호스트에게 빠른 전자 속도로 전달되고, DMA를 통해서 캐시 호스트 DRAM으로 일어난다.

### 11.1.5 Address Mapping

저장기기들은 **logical blocks**의 일차원 행렬로 가르쳐지고, 논리 블럭은 전송의 가장 단위이다. 각 논리 블럭은 물리 섹터 또는 반도체 페이지에 매핑된다. 섹터 0은 HDD의 실린더 가장 외각의 첫번째 트랙의 첫번째 섹터이다. 매핑은 트랙을 따라서, 실린더의 나머지 트랙을 따라서, 나머지 외부에서 내부의 실린더를 따라서 진행된다. NVM의 매핑은 칩의 튜플, 블럭, 페이지로 이루어진다. logical block address(**LBA**)는 섹터, 실린더, 헤드 튜플 또는 칩, 블럭, 페이지 튜플보다 알고리즘을 사용하기 간단하다. 

HDD의 매핑을 사용함으로서, 우리는 논리 블럭 넘버에서 구식 디스크로 전환하는 것을 배울수 있다. 실전에서는, 이 번역을 행하는 것은 3가지 이유때문에 어렵다. 첫번째, 대부분의 드라이브는 손상된 섹터가 존재하지만, 매핑은 이런 것을 드라이브의 어떤 스페어 섹터로 대체해서 숨겨버린다. 논리 블럭 주소는 연속적이지만, 물리 섹터 위치는 변했다. 두번쨰, 트랙의 섹터 수는 몇몇 드라이브에서 상수가 아니다. 세번째, 디스크 생산자는 LBA에서 물리 주소 매핑을 내부적으로 관리하고 그래서 현재의 드라이브에서, LBA와 물리 섹터의 관계는 적어졌다. 물리 주소의 예측 밖의 변화때문에, HDD와 대응하는 알고리즘은 논리 주소를 논리 주소는 상대적으로 물리 주소에 상관있다고 가정하는 경향이 있다. 즉, 논리 주소가 증가하는 것은 물리주소도 증가하는 것이다.

2번째 이유에 대해서 깊게 살펴보자. **constant linear velocity(CLV)**를 사용하는 미디어에서, 트랙 당 비트의 밀도는 일정하다. 트랙이 디스크의 센터에서 멀수록, 그것의 길이는 더크고, 그래서 더 많은 섹터를 가질 수 있다. 우리가 바깥에서 안쪽으로 이동하면, 트랙당 섹터의 수는 감소한다. 가장 밖의 트랙은 가장 안쪽 보다 40퍼센트나 많은 섹터를 가질 수 있다. 드라이브는 그것의 회전속도가 헤드가 안쪽으로 들어갈수록 빠르게 해서 헤드 아래에서 같은 속도를 갖게끔한다.  이 방법은 CD-ROM과 DVD-ROM에서 주로 쓰인다. 대안으로, 디스크 회전속도는 상수로 머물 수 있다. 이런 경우에, 안쪽에서 바깥 트랙의 비트의 밀도는 감소하므로 데이터 속도는 일정하다. 하드 디스크에서의 이런 메서드는 **constant angular velocity(CAV)**라고 한다.

## 11.2 HDD 스케쥴링

운영체제의 책임중에 하나는 하드웨어를 효율적으로 사용하는 것이다. HDD에서, 이 책임은 접근시간의 최소화와 데이터전송 대역을 최대화하는 것이다.

플래터를 사용하는 HDD와 다른 기계적 저장 기기에서, 접근 시간은 2가지 주요한 요소가 존재한다. 탐색 시간은 기기의 암이 원하는 섹터를 가르키는 실린더로 이동시키는 것이고 회전 지연은 플래터가 원하는 섹터로 회전하는 시간을 가르킨다. 기기 **bandwith**는 전송되는 바이트의 총 수이고, 전체시간은 처음 요청한 서비스와 마지막 전송의 완료로 나누어진다. 우리는 접근 시간과 대역폭을 어떤 저장소 I/O 요청이 서비스 되었는지 순서를 관리함으로서 향상시킨다.

프로세스가 드라이브로부터 I/O를 필요로하면, 그것은 운영체제에 시스템 콜을 호출한다. 요청은 다음과 같은 정보로 구성되어있다.

- 이 명령어가 인풋인지 아웃풋인지.
- 오픈 파일 핸들이 실행할 파일을 가르키는지.
- 전송할 메모리 주소가 무엇인지.
- 전송될 데이터의 양

만약 원하는 드라이브와 컨트롤러가 존재하면, 요청은 즉시 이루어진다. 만약 드라이브나 컨트롤러가 바쁘면, 서비스를 위한 요청은 그 드라이브를 위한 큐에 위치할 것이다. 다양한 프로세스를 가진 멀티 프로그래밍에서, 디바이스 큐는 몇가지 대기 요청을 가질 것이다.

디바이스에 대한 리퀘스트의 큐의 존재는 그것의 성능을 헤드 탐색을 피함으로서 최적화해서 디바이스 드라이버가 큐 순서를 통해서 성능을 향상시킬 수 있도록한다.

과거에는, HDD 인터페이스는 호스트가 어떤 트랙과 어떤 헤드를 이용할지 특정할 필요가 있었고, 디스크 스케쥴링 알고리즘에 많은 노력을 소비했다. 한 세기전의 드라이브는 이런 컨트롤을 호스트에게 노출시키지 않았을 뿐 아니라, 드라이브 컨트롤에 따라서 LBA에서 물리 주소로 매핑했다. 현재 디스크 스케쥴링의 목표는 시퀀스에서 나타나는 연달은 읽기, 쓰기같은 것에서 fairness, timeliness, optimizations을 포함한다. 그러므로 몇몇 스케쥴링 노력은 여전히 유용하다. 몇몇 디스크 스케쥴링 알고리즘은 쓰일수 있고, 우리는 다음에 논의하겠다. 헤드 위치와 물리 블럭/신린더의 절대적인 지식은 현대의 드라이브에는 쓸모가 없다. 그러나 적당한 근사화로, 알고리즘들은 LBA의 증가는 물리주소의 증가로 가정하고, LBA들은 물리 블럭 정확도에 따라 동일시된다.

### 11.2.1 FCFS 스케쥴링

디스크 스케쥴링의 가장 간단하 형태는 당연히 FCFS 알고리즘이다. 이 알고리즘은 본질적으로 공정하지만, 그것은 가장 빠른 서비스는 제공하지 못한다. 예를 들어서, I/O에서 실린더 블록의 디스크 큐가 다음과 같다고 생각하겠다. 

`98,183,37,122,14,124,65,67`

만약 디스크 헤드가 실린더 53위치에서 시작한다면, 순서대로 진행되면 총 640의 이동이 있을 것이다. 122~14~124로의 이동은 스케쥴의 시간에 문제가 된다. 만약 37과14가 같이 서비스된다면 전체 움직임은 줄어들고, 성능은 증가할 수 있을 것이다.

### 11.2.2 SCAN 스케쥴링

**SCAN algorithm**에서, 디스크 암은 디스크의 한 끝에서 시작하고 다른 끝으로 이동하는데, 각 실린더에 도착할때 리퀘스트를 서비스한다. 다른 끝에서, 헤드의 이동방향은 반대가 되고 서비스가 계속된다. 헤드는 디스크를 앞뒤로 순회한다. 스캔 알고리즘은 **elevator algorithm**이라고도 불리고, 디스크 암이 빌딩의 엘리베이터처럼 작동하기 때문이다.

위의 예시를 사용하겠다. 우리는 헤드의 현재 위치와 헤드 이동의 방향을 알아야한다. 디스크 암이 0으로 향한다고 가정하고 초기 헤드 위치가 다시 53이라면, 헤드는 37을 서비스하고 14를 서비스할 것이다. 반대로 이동하면서 65, 67, 98, 122, 124, 183을 서비스 할 것이다. 만약 리퀘스트가 헤드가 도달하기 직전에 큐에 들어오면 거의 즉시 서비스가 될것이다. 헤드가 지난 후에 도달한 리퀘스트는 암이 끝을 갔다가 돌아올때까지 기다려야할 것이다.

실린더의 요청이 균일한 분포라고 가정하겠다. 이런 지점에서, 상대적으로 적은 리퀘스트가 헤드의 앞에서 요청되는데, 실린더가 최근에 서비스되었기 때문이다. 리퀘스트의 가장 무거운 밀집도가 디스크의 끝에 있다. 이런 요청들은 오랜시간 기다려야하고, 그래서 여기를 먼저 방문하는 방안을 생각했다. 다음 알고리즘이 그러하다.

### 11.2.3 C-SCAN 스케쥴링

**Circular SCAN(C-SCAN) Scheduling**은 더욱 균일한 대기시간을 제공하기 위해서 제공된 스캔의 변형이다. 스캔과 C-SCAN은 헤드를 한 끝에서 다른 끝으로 이동시키고, 그 길을 따라서 리퀘스트를 서비스한다. 헤드가 끝에 도달했을때, 그것은 돌아오는 여정에서 어떠한 리퀘스트를 서비스하지 않는다.

위의 예제를 다시 이용하겠다. 우리는 우선 헤드를 53에서 다시시작하고, 리퀘스트는 끝점에 도달할떄까지 SCAN과 같지만, 199에 도달한 후에는 14-37의 순서로 돌아가는 것이다. C-SCAN은 실린더의 마지막과 첫번째가 연결된 원형 리스트로 가정하는 것이다.

### 11.2.4 디스크 스케쥴링 알고리즘의 선택

디스크 스케쥴링에는 여기서 다루지 않은 다양한 알고리즘들이 존재한다.(물론 거의 사용되지 않는다.) 그러나 어떻게 운영체제 디자이너들은 무엇을 구현할지 정하고, 사용자들은 가장 좋은 것을 어떻게 고를지 고민한다. 어떠한 요청의 리스트에서, 우리는 최적의 순서를 정의할 수 있지만, 최적의 스케쥴을 찾기 위한 계산은 SCAN을 넘어서는 절약을 정의하지는 못한다. 스케쥴링 알고리즘과 함꼐, 성능은 리퀘스트의 수와 종류에 크게 의지한다. 예를 들어서, 큐가 오직 하나의 특별한 요청을 한다고 가정하자. 그러면, 모든 스케쥴링 알고리즘은 같게 작동하는데, 왜냐하면 그들은 오직 한가지 행동만하고, 그들은 FCFS와 다름이 없다.

스캔과 C스캔은 디스크에 큰 부하를 주는 시스템에서 좋은 성능을 가지는데, 그들은 기아 문제를 거의 일으키지 않기 때문이다. 물론 리눅스가 **deadline** 스케쥴러를 만들어서 기아가 생길수는 있다. 이 스케쥴러는 읽기와 쓰기 큐를 분리해서 유지하고, 프로세스들이 읽기를 더많이 하므로 읽기에 우선순위를 준다. 큐는 LBA 순서로 정렬되어있고, C-SCAN을 구현한다. 모든 I/O 요청들은 LBA 오버로 배치로서 보내진다. 데드라인은 4가지 큐를 유지한다. 2개의 읽기와 2개의 쓰기이다. 하나는 LBA로 정렬되어있고 하나는 FCFS이다. 그것은 각 배치에서 설정한 나이보다 늙은 것이 있는지 확인한다. 만약 그렇다면, 그 요청을 포함하는 LBA 큐가 다음 I/O의 배치로 선택이 된다.

데드라인 I/O 스케쥴러는 리눅스 레드햇7버전에서 디폴트이지만, **RHEL**7은 다른 두가지도 포함한다. NOOP는 빠른 NVM 저장소를 이용한 CPU 바운드 시스템을 선호하고, **Completely Fair Queueing scheduler(CFQ)**는 SATA드라이브에 디폴트이다. CFQ는 3가지 큐를 유지한다.(인서트 소트를 이용해서 LBA의 순서를 유지한다.) : real time, best effort, idle. 각각은 다른 것을 넘어서는 독점적인 우선도를 가지고, 기아 가능성이 있다. 그것은 과거 데이터를 이용하고, 프로세스가 I/O 요청을 곧 할것이라고 예측한다. 만약 그렇게 결정되면, 그것은 새로운 I/O를 기다리며 유휴한다. 이 방법은 탐색시간을 줄이고 I/O 리퀘스트의 레퍼런스의 로컬리티를 에측하게한다.

## 11.3 NVM 스케쥴링

이전 절에서 나온 디스크 스케쥴링 알고리즘은 HDD 같은 기계적인 플래터 베이스의 저장소에 적용된다. 그들은 디스크 헤드 이동의 양을 최소화하는데 집중했다. NVM은 디스크 헤드를 움직이지 않고 보통 FCFS를 사용한다. 예를 들어 리눅스 **NOOP** 스케쥴러는 FCFS 정책을 사용하지만 근접한 요청을 합쳐서 수정한다. NVM 기기의 행동은 읽는데 걸리는 시간은 플래시 메모리의 특성에 따라서 일정해 보이는데, 쓰기 서비스 시간은 일정하지 못하다. 몇몇 SSD 스케쥴러는 이 성질 때문에 나빠지고 근접한 쓰기 요청을 합치고, 모든 읽기 요청을 FCFS로 서비스한다.

우리가 보았듯이, I/O는 연속적으로나 랜덤으로 일어날 수 있다. 연속적인 접근은 HDD같은 기계적인 기기에서 최적화되었다. **input/output operations per second(IOPS)**로 측정되는 랜덤 접근 I/O는 HDD 디스크 헤드의 이동을 유발한다. 자연적으로 랜덤 엑세스 I/O는 NVM에서 빠르다. HDD는 수백개의 IOPs를 만드는데 SSD는 수십만의 IOPs를 만든다.

NVM 기기는 연속적인 산출량에 적은 이득을 제공하고, HDD 헤드 탐색들은 감소되고 데이터의 리딩과 라이팅이 강조된다. 이런 경우에, 읽는 경우에, 두 가지 유형의 장치에 대한 성능은 NVM 장치에 대한 규모의 순서에 해당하는 정도까지 다양할 수 있습니다. NVM에 쓰는 것은 읽기보다 느리고, 장점이 감소된다. 더 나아가, HDDs의 쓰기 성능이 얼마나 디바이스가 가득찬지에 달려있고 얼마나 "닳아"있는지도 중요하다. 수명이 거의 다된 NVM 기기는 새거에 비해서 훨씬 나쁜 성능을 가지고 있다.

수명 주기를 시간에 따른 NVM 기기의 성능을 증가시키는 방법은 파일이 지워졌을때 파일 시스템이 디바이스에 알리고, 디바이스는 이런 파일들이 저장된 블록을 삭제한다. 이 방법은 14.5.6에서 더욱 자세히 다루겠다.

가비지 컬렉션이 성능에 영향을 주는 것에 대해 자세히 보겠다. NVM 기기가 랜덤 읽기와 쓰리 로드에 있다고 가정하자. 모든 블럭들은 쓰여있지만, 그들은 여유 공간이 존재한다. 가비지 컬렉션은 invalid 데이터에 의해 차지된 공간을 반드시 재정의해야한다. 이것은 쓰기가 하나 또는 여러 페이지에 읽기를 일으키고, 좋은 데이터의 쓰기는 공간을 제공한다. I/O 요청의 생성은 앱이 아니라 NVM 기기가 가비지 컬렉션과 공간 관리(**write amplification**)을 실행하는 것에 의해서 생성되고 기기의 쓰기 성능에 큰 영향을 준다. 최악의 경우에는 여러개의 추가 I/O가 각 쓰기 요청에 발생가능하다.

## 11.4 Error Detection and Correction

에러 감지와 수정은 메모리, 네트워킹, 저장소의 컴퓨팅에서 중요하다. **Error detection**은 만약 문제가 일어났다고 결정되면 생긴다. 예를 들어 DRAM의 비트가 자발적으로 0에서 1로 바뀌었고, 통신하는 동안에 네트워크의 패킷 정보가 바뀌거나, 데이터의 블럭이 쓰거나 읽힐때 바뀐 것이다. 이슈를 감지해서, 시스테은 에러가 실행되기전에 멈춰지고, 유저나 관리자에게 에러를 보고하고, 기기가 실패하거나 이미 실패되었다고 경고한다.

메모리 시스템은 다음의 에러를 parity bits를 사용해서 감지한다. 이 시나리오에서, 메모리 시스템의 각 바이트는 그것과 관련된 parity bit를 가지고 바이트가 가진 비트의 수가 짝수인지 홀수인지 세팅한다. 만약 바이트의 한가지 비트가 손상되면, 바이트의 parity가 바뀌고 저장된 parity와 일치하지 않는다. 비슷하게, 만약 저장된 parity 비트가 손상되면, 그것은 계산된 parity와 일치하지 않는다. 그러므로, 모든 단일 비트는 메모리 시스템에 의해서 감지된다. 더블 비트 에러는 감지되지 않을 수 있다. parity는 XOR연산으로 쉽게 계산되나. 모든 메모리의 바이트에서, 우리는 parity를 저장할 추ㅏㄱ 비트가 필요하다.

parity는 **checksums**의 한 종류이고, 계산하기 위해서 모듈러 연산을 사용하고, 고정된 길이의 워드 값을 비교한다. 네트워크에서 주로 이용되는 다른 에러 감지 메서드는 **cyclic redundancy check(CRCs)**인데, 다중 비트 에러를 해시 함수로 감지한다.

**error correction code(ECC)**는 문제를 감지할뿐 아니라, 고친다. 수정은 저장소의 추가공간과 알고리즘을 이용해서 이루어진다. 코드는 추가 저장소가 얼마나 필요하고 얼마나 많은 에러가 고쳐지는지에 달려있다. 예를 들어서, 디스크 드라이브는 섹터당 ECC를 사용하고 플래시 드라이브는 페이지당 ECC를 사용한다. 컨트롤러가 일반적인 I/O동안 데이터의 섹터/페이지를 쓰면, ECC는 데이터가 쓰이면서 사용된 모든 바이트의 값을 계산한다. 섹터/페이지가 읽으면, ECC는 다시 계산되고 저장된 값과 비교된다. 저장된 수와 계산된 수가 다르면, 이 미스매치는 데이터가 손상된 지점과 저장 매체가 나쁜 곳을 가르킨다.(11.5.3). ECC는 에러를 고치는데, 그것이 충분한 정보를 가지기 떄문이고, 만약 소수의 비트가 손상되면, 컨트롤러가 어떤 비트가 손상되면, 컨트롤러가 어떤 비트가 바뀐지 구별하고 그들의 원래 값이 무엇인지 계산한다. 그것은 회복가능한 **soft error**를 보고한다. 만약 많은 변화가 생기면, ECC는 에러를 수정하고, 고칠수 없는 **hard error**가 시그널된다. 컨트롤러는 ECC 과정을 어떤 섹터 또는 페이지가 읽거나 쓰이면 자동으로 실행한다.

에러 감지와 수정은 소비자 제품과 기업 제품사이에서 보통 다르다. ECC는 DRAM 에러 correction과 데이터 패스 보호를 위해서 사용된다.

## 11.6 스왑 공간 관리

스와핑은 9.5절에서 처음 소개했고, 전체 프로세스를 2차저장소와 메인 메모리 사이에서 움직이는 것이었다. 그 절에서 스와핑은 물리 메모리의 양이 낮을때 일어났고 프로세스들은 메모리에서 지워져서 여유 공간으로 바뀌었다. 실제로는, 소수의 운영체제만이 스와핑을 구현한다. 게다가, 시스템들은 스와핑을 가상 메모리 테크닉과 스왑페이지를 합성했다.(굳이 전체 프로세스가 아니다.) 실제로, 몇몇 시스템들은 "스와핑"과 "페이징"을 교차해서 사용하고, 두 컨셉이 합쳐졌음을 시사한다.

**Swap-sapce management**는 운영체제의 낮은 레벨 태스크이다. 가상 메모리는 2차 메모리를 메인 메모리의 확장으로 사용한다. 드라이브 엑세스가 메모리 엑세스보다 느리기 때문에, 스왑 스페이스를 사용하는 것은 시스템 성능을 극적으로 하향시킨다. 스왑 공간의 디자인과 구현의 주요한 목적은 가상 메모리 시스템에 최고의 산출량을 제공하는 것이다. 이 절에서, 우리는 어떻게 스왑공간이 사용되고, 스왑 공간은 어느 저장 기기에 위치하고, 스왑 공간이 어떻게 관리되는지 보겠다.

### 11.6.1 스왑 공간 사용

스왑 공간은 다른 운영체제에서 메모리 관리 알고리즘에 따라서 다양한 방법으로 사용된다. 예를 들어서, 스와핑을 구현한 시스템은 스왑 공간을 코드와 데이터 부분을 포함한 전체 프로세스 이미지를 잡기위해서 사용한다. 페이징 시스템들은 메인 메모리 밖으로 몰아낸 페이지를 저장하기도 한다. 시스템에 필요한 스왑공간은 소수의 메가 바이트에서 기가바이트로 다양할 수 있고, 물리 메모리의 양에 따라서, 가상 메모리가 백업된 양에 따라서, 가상 메모리가 어떻게 사용되냐에 따라서 다르다.

필요한 스왑공간을 저평가하는 것보다는 과평가하는 것이 안전한데, 만약 시스템의 스왑 공간이 동나면 그것은 프로세스를 실패시키거나 전체에 크래시를 일으킬 수 있다. 과 평가는 물론 다른 파일들이 사용될 2차 저장 공간을 낭비하지만, 그것은 손상을 주지는 않는다. 몇몇 시스템은 특정한 스왑공간을 보존하는 것을 추천한다. 솔라리스를 예를 들면, 페이징할 물리 공간을 초과하는 가상 메모리와 같은 스왑공간을 세팅했다고 가정하겠다. 과거에는, 리눅스는 물리메모리의 두배를 스왑공간으로 지정하게 제안했다. 오늘날, 페이징 알고리즘이 바뀌어서, 대부분의 리눅스 시스템들은 생각보다 적은 스왑공간을 사용한다.

리눅스를 포함한 몇몇 운영체제는 다중 스왑공간을 허용하는데, 파일과 스왑 파티션을 포함한다. 이 스왑 공간들은 분리된 저장 기기에 두어지고 그래서 페이징과 스와핑에 의해서 I/O 시스템에 놓아진 부하는 시스템의 I/O 대역에 퍼질 수 있다.

### 11.6.2 스왑 공간 위치

스왑 공간은 두 공간중 한 곳에 있다. 그것은 일반적인 파일 시스템을 개척하거나, 분리된 파티션에 있을 수 있다. 만약 스왑 공간이 파일 시스템안에서 단순히 큰 파일이면, 일반 파일 시스템 루틴이 그것을 생성하고, 이름 붙이고, 그것의 공간을 할당하기 위해서 사용할 수 있다.

대안으로는, 스왑 공간은 분리된 **raw partition**에서 생성될 수 있다. 어떠한 파일 시스템이나 디렉토리 구조는 이 공간에 쓰이지 않는다. 이 매니저는 공간 효율성보다는 속도를 최적화한 알고리즘을 사용하는데, 스왑 공간이 사용중일때는 파일 시스템보다 훨씬 자주 접근하기 때문이다. 내부 단편화가 증가할 수는 있지만, 이 트레이드 오프는 스왑 공간의 생명주기가 파일 시스템의 파일보다 훨씬 짧기 때문에 수용할만하다. 스왑 공간이 부트 시간에 초기화되면, 모든 단편화는 짧은 생이다. raw-partition 접근은 디스크 파티셔닝 동안에 고정된 양을 생성한다. 스왑 공간을 추가하는 것은 디바이스를 재 파티셔닝(다른 파일 파티셔닝을 움직이거나 그들을 파괴하고 백업으로부터 보존하는 것)하거나 다른 스왑 공간을 추가하는 것이다.

몇몇 운영체제들은 유연하고 raw partitions와 파일 시스템 공간에서 스왑하는 것이 가능하다. 리눅스는 이 예시이다. 정책과 구현이 분리되어서, 기계의 관리자는 어떤 타입의 스와핑을 사용할지 결정할 수 있다. 파일 시스템에서의 할당과 관리의 편리성과 raw parittions에서의 스와핑의 성능이 트레이드 오프이다.

### 11.6.3 스왑 공간 관리 : 예시

우리는 다양한 유닉스 시스템에서의 스와핑과 페이징의 발전에 따라서 어떻게 스왑공간이 사용되었는지 보겠다. 전통적인 유닉스 커널은 연속된 디스크 리전과 메모리 사이에 전체 프로세스의 카피를 스와핑에 구현했다. 유닉스는 페이징 하드웨어가 가용해진 후에 스와핑과 페이징을 합쳤다.

솔라리스 1에서, 디자이너는 효율성을 증가시키고 기술적 배치를 반영해서 표준 UNIX 메서드를 바꾸었다. 프로세스가 실행하면, 코드를 포함한 텍스트 세그먼트 페이지를 파일 시스템에서 들여오고, 메인 메모리가 접근했고 만약 페이지 아웃되면 버려졌다. 스왑 공간에 쓰고 다시 읽는 것보다 그냥 페이지를 파일 시스템에서 다시 읽는 것이 더욱 효율적이다. 스왑 공간은 프로세스의 스택, 힙, 초기화되지 않은 데이터인 **anonymous** 메모리의 페이지를 백업하는 용도로만 쓰였다.

솔라리스의 후 버전에서 더 많은 변화가 생겼다. 가장 큰 변화는 솔라리스가 스왑 공간을 가상 메모리 페이지가 처음 생성될떄보다는 오직 물리 메모리의 밖으로 페이지 아웃할때 할당하는 것이다. 이 구조는 현대 컴퓨터에서 더욱 좋은 성능을 제공했고, 구식 시스템보다 더 많은 물리 페이지를 가지고 페이지를 적게했다.

리눅스는 솔라리스와 비슷하게 anonymous memory에 대해서만 스왑 공간을 사용했다. 리눅스는 하나 또는 여러개의 스왑공간을 설치했다. 스왑 공간은 정규 파일 시스템의 스왑 공간 또는 스왑 전용 파티션에 있다. 각 스왑 공간은 4KB **page slots**의 연속에 포함되고, 스왑 공간을 붙잡기 위해서 사용된다. 각 스왑 공간에 관련된 것은 **swap map**인데, 정수 카운터의 행렬이고, 각각은 스왑 공간에 일치한다. 0보다 큰 값은 스왑 페이지에 의해서 페이지 슬롯이 점유된 것이다. 카운터의 값은 매핑의 수에서 스왑된 페이지를 가르킨다. 예를 들어서, 3의 값은 스왑된 페이지가 3가지 다른 프로세스에 매핑되었다고 가르킨다.(스왑된 페이지가 3개의 프로세스에 의해서 공유된 메모리를 가지는 것이다.)

## 11.7 저장소 attatchment

컴퓨터는 2차 저장소에 3가지 방법으로 접근한다. 호스트에 부착된 저장소, 네트워크에 부착된 저장소, 클라우드 저장소가 있다.

### 11.7.1 Host attached Storage

**Host attatched storage**은 로컬 I/O 포트를 통해서 접근되는 저장소이다. 이런 포트들은 다양한 기술을 사용하는데, 보통은 SATA를 이용한다. 시스템은 하나또는 소수의 SATA 포트를 가진다.

시스템이 더 많은 저장소를 가지려면, 개별 저장 기기, chassis의 기기, 또는 chassis의 다양한 드라이브가 USB FireWire 또는 Thunderbolt ports와 케이블을 통해서 얻을 수 있다. 

하이엔드 워크 스테이션과 서버들은 더 많은 저장소또는 공유된 저장소를 필요로해서, 광학 섬유를 이용하는 매우 빠른 직렬 아키텍처 **fibre channel** 또는 4개의 도체 구리 선에서 작동하는 정교한 I/O 아키텍쳐를 사용한다. 큰 주소 공간과 통신의 변경 환경 때문에, 다중 호스트들과 저장기기들은 I/O 통신에 큰 유연성을 허용할 수 있다.

다양한 저장 기기들은 host-attached storage에 적합하다. 이중에는 HDDs, NVM, CD, DVD, BLu-ray, tape와 storage-area network(**SANs**)가 있다. host-attached storage에 데이터 전송을 시작하는 I/O 커맨드는 지정된 저장 유닛에 있는 논리 데이터 블럭을 읽고 쓴다.

### 11.7.2 Netowork-Attached Storage

**Network-attached storage(NAS)**는 네트워크를 거쳐서 저장소에 접근한다. NAS 디바이스는 특정 목적 저장 시스템이거나 그것의 저장소를 그저 네트워크를 건너서 호스트에게 제공하는 일반 목적 컴퓨터 시스템일 수 있다. 고객들은 NAS에 유닉스와 리눅스의 NFS와 윈도우의 CIFS같은 remote procedure call interface를 통해서 접근한다. Remote procedure calls(RPCs)들은 TCP또는 UDP를 통해서 IP 네트워크를 건너서 옮겨진다. 보통 같은 local area network가 모든 데이터 트래픽을 클라이언트에세 가져다준다. NAS 유닛은 RPC 인터페이스가 구현된 소프트웨어와 함께 보통 저장소 행렬로 구현된다.

CIFS와 NFS는 다양한 락킹 기능을 제공하고, NAS 프로토콜로 접근하는 호스트 간에 파일의 공유를 허용한다. 예를 들어서, 다중 NAS에 로그인한 유저는 그녀의 홈에서 다른 클라이언트의 홈까지 동시에 접근이 가능하다.

NAS는 LAN을 가진 모든 컴퓨터가 같은 이름의 저장소 풀을 공유하고 로컬 host attached storage에 접근하는 편리한 기능을 제공한다. 그러나, 그것은 몇몇 직접 연결 저장소보다는 낮은 성능과 효율성을 보이는 경향이 있다.

**iSCSI**는 가장 최근의 NAS 프로토콜이다. 그것은 IP 네트워크 프로토콜을 이용해서 SCSI 프로토콜을 옮긴다. 그러므로, SCSI 케이블보다는 네트워크가 호스트와 그들의 저장소사이에 서로 연결되어있다. 결과적으로 호스트는 그것이 멀리 연결되어있어도 직접 연결되어있는 것처럼 다룬다. 반면에 NFS와 CIFS는 네트워크를 통해서 파일 시스템을 보이고 파일의 일부분을 보내는데 비해서, iSCSI는 네트워크를 통해서 논리 블럭을 보내고 클라이언트가 직접 블럭을 이용하거나 그들과 파일 시스템을 만든다.