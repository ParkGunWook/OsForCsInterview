## What we gonna study

9장에서, 우리는 컴퓨터 시스템을 이용한 다양한 메모리 관리 전략을 토론했다. 모든 전략들은 같은 목적을 가지고 있다. 많은 프로세스를 멀티프로그래밍을 허용하기 위해서 메모리에 유지하는 것이다. 그러나, 그들은 실행하기 위해서 전체 프로세스를 필요로하는 경향이 있다.

가상 메모리는 메모리 전체가 프로세스에 없어도 실행하게 해주는 기술이다. 이 구조의 한가지 주요한 장점은 프로그램이 물리 프로그램보다 커도 가능할 수 있다는 것이다. 더 나아가, 가상 메모리는 메인메모리를 아주 크고, 일정한 공간의 행렬이고, 논리 메모리를 프로그래머의 입장에서 분리한다. 이 기술은 프로그래머를 메모리 공간 한계로부터 자유롭게 해주었다. 가상 메모리는 또한 프로세스끼리 파일과 라이브러리를 공유하게 해주고, 공유메모리를 구현한다. 추가적으로, 그것은 프로세스 생성에 효율적인 메커니즘을 제공한다. 가상 메모리는 쉽게 구현할 수 없다. 그러나, 그것이 부주의하게 만들어지면, 잠재적으로 성능을 하락시킨다. 이 장에서는, 우리는 가상메모리의 개요를 기술하고, 어떻게 구현되었는지, 그것의 복잡도와 장점을 살펴보겠다.

## Objectives

- [ ] 가상 메모리를 정의하고 그것의 장점을 설명한다.
- [ ] 어떻게 페이지가 디맨드 페이징을 이용해서 로드되는지 설명한다.
- [ ] FIFO, optimal, LRU 페이지 교체 알고리즘을 적용한다.
- [ ] 프로세스의 집합을 설명하고, 어떻게 프로그램 지역성을 관련하는지 설명한다.
- [ ] 리눅스, 윈도우, 솔라리스에서의 가상메모리 관리를 설명한다.
- [ ] C언어를 이용해서 가상 메모리 매니저 시뮬레이션을 디자인한다.


## 10.1 백그라운드

9장에서 설명한 메모리 관리 알고리즘은 한가지 기초 필수 사항 때문에 필요하다. 바로, 실행되는 명령어는 반드시 물리메모리에 있어야한다는 점이다. 이 필요를 충족하는 첫번째 접근은 전체 논리 주소 공간을 물리메모리에 넣는 것이다. 동적 링킹은 이 제한을 완화할 수 있는데, 그것은 특별한 주의와 추가 작업이 프로그래머에게 필요하다.

명령어가 반드시 물리 메모리에 있어야한다는 필요점은 필수적이고 근거있어보인다. 불행한 점인 이유는 프로그램의 사이즈가 물리메모리의 사이즈에 의해서 제한된다. 실제 프로그램를 평가하면, 많은 경우에서, 전체 프로그램이 필요하지는 않다. 예를 들어서, 다음을 고려해보자.

- 프로그램들은 보통 비정상적인 에러 상황을 코딩해야한다. 이런 에러는 가끔 일어나고, 코드는 거의 실행되지 않는다.
- 행렬, 리스트, 테이블들은 보통 필요한 것보다 더 많이 할당된다. 행렬은 10개가 필요한데도 100개가 선언되기도 한다.
- 프로그램의 일부 옵션과 기능은 거의 사용되지 않는다. 예를 들어서, US 정부 컴퓨터의 예산을 조정하는 루틴은 몇년간 사용되지 않았다.

전체 프로그램이 필요한 경우에도, 그것은 같은 시간동안 필요하지는 않다.

메모리에 일부만 있는 프로그램을 실행하는 능력은 많은 장점을 수여한다.

- 프로그램은 물리 메모리의 크기에 더이상 제한되지 않는다. 유저들은 프로그램을 아주 큰 가상 메모리 공간으로 쓸 수 있게된다. 
- 각 프로그램이 적은 물리 메모리를 차지해서, 더 많은 프로그램들이 동시에 작동할 수 있어서, CPU 효율성, 산출량을 증가 시킬 수있다.(반응시간 또는 턴어라운드 시간은 변화가 없다.)
- I/O가 프로그램의 일부분을 메모리로 로드하거나 스왑할 필요가 적어진다. 그래서 각 프로그램은 더 빠르게 실행할 수 있다.

그러므로, 메모리에 전체 프로그램을 담지 않는 것은 유저와 시스템에게 이득이 된다.

**Virtual memory**는 물리메모리로부터 개발자에게 감지되는 논리 메모리의 분리를 포함한다. 이 분리는 오직 작은 물리 메모리가 존재할 때 프로그래머에게 제공된 매우 큰 물리 메모리를 허용한다. 가상 메모리는 프로그래밍의 업무를 훨씬 쉽게 만드는데, 왜냐하면 프로그래머는 더이상 물리 메모리의 양에 신경쓰지 않아도 되기 때문이다. 그녀는 문제를 해결하는데에만 신경써도 된다.

프로세스의 **Virtual address space**는 어떻게 논리적인 관점에서 프로세스가 메모리에 저장하는지를 언급한다. 일반적으로, 이 과점은 프로세스가 특정 논리 주소에서 시작한다는 것이다. 즉, 주소 0이다. 그리고 연속적인 메모리로 존재한다. 9장을 생각하면, 물리 메모리의 페이지 프레임에서 프레임에 할당된 프로세스는 연속적이지 않다. 그것은 논리 페이지를 물리 페이지 프레임에 매핑하는 MMU에 달려있다.

우리는 프로세스에서 동적 메모리 할당으로 사용되는 힙 메모리가 있다. 유사하게, 우리는 함수콜마다 사용되는 스택메모리가 있다. 힙영역과 스택 영역 사이의 거대한 빈 공간은 가상 주소공간의 일부이지만, 힙과 스택이 커질수록 실제 물리 페이지를 필요로 할 것이다. 가상 메모리 공간에서 구멍을 포함한 공간을 **sparse address space**라고 한다. sparse 주소 공간을 사용하는 것은 스택과 힙이 자랄때마다 채워지기 때문에 이득이다. 또는 우리가 dll을 프로그램 실행중에 사용하고 싶을때도 마찬가지이다.

물리 메모리로부터 논리 메모리를 분리하는 것은, 가상 메모리가 2프로세스간에 페이지 공유를 통해서 메모리와 파일을 공유하게한다. 다음과 같은 이득을 만든다.
- C 라이브러리같은 시스템 라이브러리는 공유 오브젝트의 매핑을 통해서 몇몇 프로세스를 공유할 수 있다. 비록 각 프로세스가 그것의 가상 주소공간의 일부를 라이브러리에 사용해도, 물리 페이지에 있는 라이브러리는 같은 곳에서 공유된 것이다. 일반적으로, 라이브러리는 각 프로세스의 공간에서 읽기전용으로 매핑된다.
- 비슷하게, 프로세스들은 메모리를 공유할 수 있다. 3장에서 말했던 두 프로세스간의 통신을 생각해보자. 가상 메모리는 한 프로세스가 다른 프로세스와 공유할 수 있는 메모리의 리전을 만들게 해준다. 이 리전을 공유하는 프로세스들은 그것을 그들의 가상 메모리 주소라고 고려하고, 실제로는 공유되고 있는 것이다.
- 페이지들은 프로세스 생성중에 fork() 시스템 콜로 공유될수 있고, 그러므로 프로세스 생성을 빠르게한다.

우리는 뒤에서 더 많은 장점을 보겠다. 먼저, 디맨드 페이징을 이용해서 가상메모리를 구현해보겠다.

## 10.2 디맨드 페이징

어떻게 실행가능한 프로그램이 2차 저장소에서 메모리로 로드될까? 한가지 옵션은 전체 프로그램을 물리 메모리에 로드하는 것이다. 그러나, 이 접근은 전체 프로그램이 메모리에 있을 필요가 없기에 필요하지 않다. 프로그램은 유저가 선택한 필요한 옵션만 시작한다. 전체 프로그램을 메모리에 로딩하는 것은 모든 옵션을 로딩하는 것이고, 유저에 의해서 선택되었는지는 전혀 고려하지 않는다. 

다른 전략은 페이지를 필요한 것만 로드하는 것이다. 이 기술이 **demand paging**이고 가상 메모리 시스템에서 보통 사용된다. 디맨드 페이지된 가상 메모리와 함께, 페이지들은 오직 그들이 프로그램 실행에 필요할떄만 로드된다. 디맨드 페이징 시스템은 스와핑과 비슷하다. 디맨드 페이징은 가상 메모리의 주요한 장점을 설명하는데, 오직 프로그램의 일부만을 로딩하고, 메모리는 더욱 효율적이게된다.

### 10.2.1 기본 개념

디맨드 페이징의 일반적인 컨셉은, 페이지를 오직 필요할 때만 로드하는 것이다. 결과적으로, 프로세스가 실행중일때, 몇몇 페이지들은 메모리에 있어야하고, 몇몇은 2차 저장소에 있다. 그러므로, 우리는 이 두가지를 구분할 하드웨어적인 지원이 필요하다. valid-invalid 비트가 이 목적을 위해서 사용된다. 이번에는, 비트가 valid이면 사용가능이고 메모리에 있다는 것이다. 만약에 비트가 invalid이면, 유효하지 않거나, 2차메모리에 있다는 것이다. 페이지를 위한 페이지 테이블 엔트리는 평소대로 세팅되지만, 메모리에 없는 페이지 테이블의 엔트리는 invalid가 된다. 

그러나 메모리에 없는 페이지를 프로세스가 접근하려면 무슨 일이 일어날까? 페이지로의 접근은 **page fault**를 만든다. 페이지 테이블로 번역하는 과정에서, 페이징 하드웨어는 invalid 비트가 세팅되었고, 운영체제로 하여금 트랩하게한다. 이 트랩은 메모리에서 원하는 페이지를 가져오지 못한 운영체제의 실패이다. 페이지 폴트를 해결하는 과정은 직관적이다.

1. 이 프로세스에서의 참조가 valid인지 아닌지 결정하는 내부 테이블을 체크한다.(PCB에 저장되어있음)
2. 만약 invalid라면, 우리는 프로세스를 종료한다. 만약 valid인데 페이지로 가져오지 않았다면, 페이지를 가져온다.
3. 여유있는 프레임을 찾는다.
4. 우리는 2차 저장소 명령어를 원하는 페이지를 새롭게 할당된 프레임에 넣기 위해서 읽는다.
5. 저장소 읽기가 완료되면, 우리는 프로세스가 가진 내부 테이블을 수정하고 페이지 테이블이 메모리에 페이지가 있다고 알린다.
6. 우리는 트랩에 의해서 인터럽트된 명령어를 다시 시작한다. 프로세스는 그것이 원래 메모리에 있던 것처럼 페이지에 접근한다.

극단적인 케이스에서, 우리는 메모리에 페이지 없이 프로세스를 실행할 수 있다. 운영체제가 명령어 포인터를 프로세스의 메모리에 상주하지 않는 페이지의 첫 명령어를 지정하면, 프로세스는 즉각적으로 페이지를 폴트한다. 페이지가 메모리로 가져와진후에, 프로세스는 실행을 모든 페이지가 메모리에 필요할때까지 지속한다. 이 점에서, 그것은 아무런 폴트 없이 실행가능하다. 이 구조는 **pure demand paging**이라고한다. 페이지가 필요할때까지 절대로 메모리에 가져오지 않는다.

이론적으로, 몇몇 프로그램들은 각 명령어 실행마다 메모리의 새로운 페이지에 접근할 수 있는데, 명령어 별로 다중 페이지 폴트를 유발한다. 이 상황을 시스템 성능에서 용납할수없는 성능을 만들어낸다. 다행히도, 실행중인 프로세스의 분석은 이 행동이 매우 예상 밖이라는 것을 보여준다. 프로그램은 **locality of reference**를 하는 경향이 있고, 디맨드 페이징의 의미있는 성능을 결과로 낸다. 

디맨드 페이징을 지원하는 하드웨어는 페이징, 스와핑의 것과 같다. 
- 페이지 테이블 : 이 테이블은 valid-invalid 비트또는 특별한 보호비트의 값을 통해서 엔트리가 invalid인지 마크할 능력이 있다.
- 2차 메모리 : 이 메모리는 메인 메모리에 없는 페이지를 잡고있다. 2차 메모리는 보통 빠른 디스크 또는 NVM 디바이스이다. 이것은 swap device로 알려져 있고, 저장소의 공간은 **swap space**라는 목적으로 사용된다. Swap-space 할당은 11장에서 언급된다.

디맨드 페이징의 필요는 페이지 폴트 이후에 어떤 명령어든 다시 실행할 능력이다. 우리는 페이지 폴트가 일어났을때 인터럽트된 프로세스의 상태(레지스터, 컨디션 코드, 명령어 카운터)를 저장하고, 우리는 반드시 프로세스를 같은 장소와 상태로 재시작해야한다. 페이지 폴트는 메모리 레퍼런스에서 언제든 일어날 수 있다. 만약 페이지 폴트가 명령어 패치에서 일어나면, 우리는 명령어를 다시 패치하면된다. 만약 페이지 폴트가 피연산 함수 패칭 중에 일어나면, 우리는 명령어를 다시 패치하고 디코드하고 피연산 함수를 패치해야한다.

최악의 예제로, 3가지 명령어, ADD A to B한 결과를 C에 배치한다고 고려하겠다. 이 명령어에는 다음과 같은 과정이 필요하다.
1. ADD 명령어를 실행하고 해석한다.
2. A를 가져온다.
3. B를 가져온다.
4. A와 B를 더한다.
5. 합을 C에 저장한다.

만약 페이지 폴트가 C를 저장하는 중에 생기면(왜냐하면 C는 현재 메모리에 없다.), 우리는 원하는 페이지를 가져와야하고, 페이지 테이블을 조정하고, 명령어를 재시작해야한다. 재시작은 명령어 패칭, 디코딩, 연산자 패칭, 더하기를 다시하게 할 수 있다. 그러나, 반복된 일은 많지 않고, 반복은 오직 페이지 폴트가 생겼을떄만 필요하다.

이 문제는 두가지 방법으로 해결이 가능하다. 한가지는 마이크로 코드 컴퓨터와 두 블럭 사이의 두 끝을 연결하려는 시도이다. 만약 페이지 폴트가 일어나려고하면, 그것은 무언가 수정되기전에 이 단계를 거치다.이동이 공간을 차지할 수도 있다. 우리는 모든 관련 페이지가 메모리에 있다면 아무런 페이지 폴트가 일어나지 않은 것을 알고 있다. 다른 방법은 덮어쓰기 당할 공간의 값을 임시 레지스터에서 잡고있는 것이다. 만약 페이지 폴트가 일어나면, 모든 구식 값들은 트랩이 생기기전에 메모리에 덮인다. 이 행동은 명령어가 실행하기 전의 상태로 메모리를 복구한다. 그래서 명령어는 다시 실행한다.

이것은 디맨드 페이징을 허용하기 위해서 존재하는 아키텍처에 페이징을 더한것인데, 많은 어려움이 포함되어있다. 페이징은 컴퓨터 시스템의 CPU와 메모리 사이에 있다. 그것은 프로세스에 완벽히 명백해야한다. 비록 디맨드 페이징이 없는 환경에서의 가정은 사실이지만, 페이지 폴트는 페이탈 에러를 의미한다. 페이지 폴트가 오직 추가적인 페이지를 메모리에 가져오고 프로세스를 재시작하는 것은 사실이 아니다.

### 10.2.2 프리프레임 리스트

페이지 폴트가 일어나면, 운영체제는 반드시 원하는 페이지를 2차 저장소에서 메모리로 가지고 와야한다. 페이지 폴트를 해제하기 위해서, 대부분의 운영체제는 이런 요청을 만족시키는 프리 프레임의 풀인 **free-frame list**를 사용한다.(프리 프레임은 반드시 스택 또는 힙 세그먼트가 확장할때 할당되어야한다.) 운영체제는 일반적으로 **zero-fill-on-demand**라는 기술로 여유프레임을 할당한다. Zero fill on demand frames들은 할당되기전에 "zeroed out" 된다. 그러므로, 그들의 이전 정보를 삭제한다.(이 컨텐츠들을 재할당하기전에 지우지 않으면 생길 보안적인 측면을 고려하자.)

시스템이 시작되면, 모든 가용 메모리는 프리 프레임 리스트에 오른다. 프리 프레임이 요청되면, 그 크기만큼의 리스트는 줄어든다. 몇몇 관점에서, 리스트는 0으로 떨어지거나 특정 스레시홀드로 떨어진다. 우리는 이 전략을 10.4에서 더 자세히 보겠다.

### 10.2.3 디맨드페이징의 성능

디맨드 페이징은 컴퓨터 시스템의 성능에 큰 영향을 끼쳤다. 왜인지 보기위해서 디맨드 페이지 메모리에서의 **effective access time**을 측정해보자. 메모리 액세스 시간(ma)가 10나노초이다. 페이지 폴트가 없다면, 유효 접근 시간은 메모리 접근시간과 같을 것이다. 만약, 페이지 폴트가 일어나면, 우리는 반드시 2차 저장소로부터 페이지를 읽고 원하는 word에 접근해야한다.

페이지 폴트 확률을 p라고 하겠다. 우리는 p가 0에 가깝다고 예상하면 유효 시간은 `effective access time = (1-p)\*ma + p \*page fault time`이 될 것이다. 유효 접근 시간을 측정하기 위해서, 우리는 페이지 폴트에 얼마나 시간이 걸리는지 봐야한다. 페이지 폴트는 다음 순서로 일어난다.
1. 운영체제를 트랩한다.
2. 레지스터와 프로세스 상태를 저장한다.
3. 인터럽트가 페이지 폴트인것을 결정한다.
4. 페이지 레퍼런스가 적합한지 확인하고 2차 저장소에 있는 페이지의 위치를 결정한다.
5. 여유 프레임인 저장소를 읽는다.
   1. 리드 리퀘스트가 서비스 될떄까지 큐에서 기다린다.
   2. 디바이스 찾기와 레이턴시 시간을 기다린다.
   3. 페이지를 프리 프레임에 전송하기 시작한다.
6. 기다리면서, CPU 코어는 다른 프로세스를 할당한다.
7. 저장소 I/O 서브시스템으로부터 완료 인터럽트를 받는다.
8. 다른 프로세스의 레지스터와 프로세스 상태를 저장한다.
9. 인터럽트가 2차 저장소로 온것을 결정한다.
10. 페이지 테이블을 수정하고 원하는 페이지가 메모리에 있다는 것을 보인다.
11. CPU 코어가 이 프로세스를 할당하기를 기다린다.
12. 레지스터, 프로세스 상태, 새로운 페이지 테이블을 복구하고, 인터럽트된 명령어부터 재시작한다.

모든 스텝이 항상 필요하지는 않다. 예를 들어서, 우리는 6번 과정에서, CPU가 I/O 실행시 다른 프로세스를 할당했다고 했다. 이 나열은 CPU 효율을 높이지만 페이지 폴트 서비스 루틴의 시간을 늘린다. 어떤 경우이든, 3가지 페이지 폴트는 반드시 일어난다.
1. 페이지 폴트 인터럽트 서비스
2. 페이지 읽기
3. 프로세스 재시작

첫번째와 세번쨰 태스크는 감소될수 있다. 이런 태스크들은 1~100 마이크로초가 걸린다. HDD가 페이징 디바이스로 쓰인다고 가정하자. 페이지 스위치 시간은 8미리초일 것이다.(하드 디스크는 보통 3미리초의 지연시간, 5미리초의 탐색, 0.05미리초의 전송시간을 가진다.) 우리는 디바이스 서비스 시간만 살펴보았다. 만약 프로세스의 큐가 디바이스를 기다리면, 우리는 우리의 서비스 시간에 페이징 디바이스가 여유로울 떄까지의 큐잉 시간을 더해야하고, 페이지 인에 시간을 더욱 투자한다.
페이지 폴트 시간이 8미리초이고 메모리 엑세스 시간이 200나노초라면, 나노초 기준에서의 유효 접근시간은 다음과 같다.
`effective access time = (1-p) \* 200 + p (8milliseconds) = 200 + 7999800 \* p`
우리는 이로서 유효 접근 시간이 **page fault rage**에 비례함을 알 수 있다. 만약 1000번중에서 1번 페이지 폴트가 일어나면, 8.2 마이크로 초 일 것이다. 기존의 시간보다 무려 40배나 긴것이다. 만약 성능 하락을 10퍼센트 이내로 바꾸려면 우리는 p가 0.0000025의 비율이어야할 것이다. 즉 페이징으로 인한 성능 하락을 없애려면 우리는 사십만번중에 한번의 페이지 폴트를 만들라는 것이다. 디맨드 페이징 시스템에서 페이지 폴트를 낮게하는 것은 중요하다. 그렇지 않으면, 유효 접근 시간은 증가하고, 프로세스 실행은 엄청나게 느려진다.

디맨드 페이징의 추가적인 면은 스왑 공간의 핸들링과 전체 사용이다. I/O에서 스왑 공간은 일반적인 파일 시스템보다 빠르다. 스왑공간이 더욱 큰 블럭에 할당되었고 파일 룩업과 간접 할당 메서드가 사용되지 않아서 더욱 빠르다.(11장) 더좋은 페이징 산출량을 얻기위한 옵션은 프로세스가 시작할때 전체 파일 이미지를 복사하고 스왑공간으로부터 디맨드 페이징을 실행하는 것이다. 이 방법의 단점은 프로그램이 시작할때 파일이미지를 복사하는 것이다. 두번째 옵션이자 윈도우/리눅스에서 사용되는 방법은 디맨드 페이지를 파일 시스템 그대로 사용하다가 그들이 교체되면 스왑 공간으로 쓰는 것이다. 이 접근은 오직 필요한 페이지들을 파일 시스템으로부터 읽게하고 다음 페이징들은 스왑공간에서 하게한다.

몇몇 시스템들은 바이너리 실행 파일의 디맨드 페이징을 통해서 스왑 공간의 양을 제한했다. 이런 파일의 디맨드 페이지는 파일 시스템으로부터 직접 가져왔다. 그러나, 페이지 교체가 불리면, 이런 프레임들은 덮여쓰이고 페이지들은 필요할때 다시 파일시스템에서 읽어온다. 이 접근을 사용하면, 파일 시스템 자체는 배킹 스토어처럼 사용된다. 그러나, 스왑 공간은 반드시 파일(**anonymous memory**로 알려짐)과 관련되지 않은 페이지를 사용해야한다.(이런 페이지들은 프로세스의 힙과 스택을 포함한다.) 이 메서드는 좋은 타협점이고 리눅스와 BSD UNIX에서도 사용된다.

9.5.3에서 설명했듯이, 모바일 시스템은 스와핑을 지원하지 않는다. 대신에, 이러한 시스템들은 메모리가 제한되면 파일 시스템으로 부터 페이지를 요청하고 읽기 전용 페이지를 재정의한다. 이런 데이터는 파일 시스템으로부터 디맨드 페이지드 될수 있다. iOS에서는, 익명 메모리 페이지는 앱이 종료되거나 메모리에서 해제되기 전까지는 결코 재정의되지 않는다. 10.7절에서는, 우리는 모바일 시스템에서 스와핑의 대용으로 쓰이는 압축 메모리를 보겠다.

## 10.3 Copy-on-Write

10.2절에서, 우리는 프로세스가 어떻게 첫 명령어를 포함하는 페이지를 디맨드 페이징으로 빠르게 시작하는지 설명했다. 그러나, fork()시스템 콜을 통한 프로세스 생성은 페이지 쉐어링과 비슷한 기술을 이용해서 디맨드 페이징의 필요를 우회했다. 이 기술은 빠른 프로세스 생성을 제공했고 새롭게 생성된 프로세스에 할당되어야할 새로운 페이지의 수를 감소시켰다.

fork() 시스템콜은 자식 프로세스를 부모의 복제로 생성한다. 전통적으로, fork()는 자식을 위한 부모의 주소공간을 복사한다. 부모가 속한 페이지를 복제하는 것이다. 그러나, 많은 자식 프로세스가 생성된 후에, exec()를 실행하고, 부모의 주소공간의 카피는 필요가 없어진다. 대신에, 우리는 부모와 자식 프로세스가 같은 페이지를 공유하는 **copy on write**기술을 사용한다. 이런 공유된 페이지들은 copy-on-write 페이지로 마크되고, 공유된 페이지가 무언가를 쓰려고하면, 공유 페이지의 카피가 생성된다.

예를 들어서, 자식 프로세스가 스택의 일부를 포함하는 COW로 세팅된 페이지를 수정하려고 한다. 운영체제는 프리 프레임 리스트로부터 다른 프레임을 획득하고 페이지의 복사본을 얻고, 그것을 자식 프로세스의 주소공간에 매핑한다. 자식 프로세스는 복사된 페이지를 수정하고 부모 프로세스에 속한 페이지가 아니게 된다. 명백히, COW 기술이 사용되면, 오직 수정된 페이지만이 복사되는 것이다. 수정되지 않은 페이지들은 부모와 자식에 의해서 공유된다. 오직 수정될수 있는 페이지들만이 COW로서 세팅된다. 수정될 수 없는 페이지(실행 코드)는 부모와 자식에게 공유가 가능하다. COW는 윈도우, 리눅스, 맥에서 쓰이는 일반적인 기술이다.

몇몇 유닉스의 버전은 fork() 시스템콜의 다양한 변형을 제공한다. vfork()(virtual memory fork)는 COW를 지원하는 fork()와 다르게 작동한다. vfork()에서, 부모 프로세스는 정지하고, 자식 프로세스는 부모의 주소공간을 사용한다. vfork()는 COW를 사용하지 않기 때문에, 만약 자식 프로세스가 부모의 주소공간을 바꾸면, 바뀐 페이지는 부모가 다시 시작하면 보일 것이다. 그러므로, vfork()는 반드시 자식 프로세스가 부모의 주소공간을 바꾸지 않도록 조심해서 코딩해야한다. vfork()는 자식 프로세스가 생성된 직후에 exec()를 콜해서 사용하는 경향이 있다. 페이지의 카피가 없기 때문에, vfork()는 프로세스 생성에 매우 효율적인 메서드이고 유닉스 커맨드 라인 쉘 인터페이스를 구현할때 많이 쓰인다.

## 10.4 페이지 교체

페이지 폴트 레이트에 대해서 얘기할때, 각 페이지 폴트는 최대 한번이고, 그것이 처음 언급되었을 때였다. 이 표현은 정확이 옳지는 않다. 만약 10페이지의 프로세스가 오직 그들의 절반만 이용하면, 디맨드 페이징은 결코 쓰이지 않을 5페이지를 로드할때 필요한 I/O를 절약하게 된다. 우리는 두배 가량의 프로세스를 실행함으로서 멀티프로그래밍의 급을 올릴수 있다. 그러므로, 우리가 40개의 프레임을 가지면, 우리는 10개의 프레임을 사용했다면 4개의 프로세스였을 것인데, 8개의 프로세스를 실행할 수 있다.

만약 우리가 멀티프로그래밍의 단계를 올리면, 우리는 메모리를 **over-allocating**하는 것이다. 만약 10페이지지만 실제로 5개의 페이지만 사용하는 6개의 프로세스를 실행하면, 우리는 10개의 프레임을 아끼면서 높은 CPU 효율성과 산출량을 가질 수 있다. 각 프로세스가 특정한 데이터 집합에서, 10개의 페이지를 사용하려고 해서, 40개의 프레임뿐인데, 60개의 프레임이 필요할 수도 있다.

더 나아가서, 시스템 메모리가 프로그램 페이지를 잡는데만 사용되지 않는다. I/O를 위한 버퍼는 상당한 양의 메모리를 소비한다. 이 사용은 메모리 배치 알고리즘에 부담을 늘릴 수도 있다. I/O에 얼마나 많은 메모리를 할당할지와 프로그램 페이지에 얼마나 할당할지 결정하는 것은 어려운 과제이다. 몇몇 시스템은 I/O 버퍼에 대해서 고정된 메모리의 퍼센트를 할당하는데 비해서, 다른 시스템들은 프로세스와 I/O 서브 시스템들이 모든 시스템 메모리를 두고 경쟁하게 한다. 14.6절에서 I/O 버퍼와 가상 메모리 기술 사이의 합쳐진 관계를 설명하겠다.

메모리의 할당은 그 자체로 강화되어져 왔다. 프로세스가 실행중이면, 페이지 폴트가 일어난다. 운영체제는 원하는 페이지가 2차 저장소의 어디에 위치했는지 결정하지만, 프리 프레임 리스트에 프리 프레임이 없다는 것을 알아냈다.(모든 메모리 사용중)

운영체제는 이런 상황에서 여러가지 옵션이 있다. 그것은 프로세스를 종료시킬 수도 있다. 그러나, 디맨드 페이징은 컴퓨터 시스템의 효율성과 산출량을 높이기 위한 시도이다. 유저는 그들의 프로세스가 페이지드 시스템이라는 것을 의식하면 안되고, 페이징은 유저에게 논리적으로 투명해야한다. 그래서 이 옵션은 결코 좋은 선택이 아니다.

운영체제는 프로세스의 프레임을 프리시키고 멀티프로그래밍의 레벨을 낮추는 표준 스와핑과 스왑아웃 프로세스를 사용한다. 그러나 9.5절에서 말했듯이, 표준 스와핑은 전체 프로세스를 메모리와 스왑공간에서 복사하는 간접비 떄문에 더이상 사용되지 않는다. 대부분의 운영체제는 이제 **page replacement**라는 기술을 사용한다.

### 10.4.1 기본 페이지 교체

페이지 교체는 다음과 같은 방식으로 실행된다. 만약 여유 프레임이 없으면, 우리는 현재 사용되지 않는 것을 해제한다. 우리는 페이지의 컨텐츠를 스왑 공간에 쓰고 페이지 테이블을 더이상 메모리에 없다고 바꿈으로서 페이지를 해제한다. 우리는 이제 프로세스가 폴트된 페이지를 여유 프레임에 사용한다. 우리는 페이지 폴트 서비스 루틴으로 페이지 교체를 약간 수정한다.

1. 원하는 페이지의 주소를 2차 저장소에서 찾는다.
2. 여유 프레임을 찾는다.
   1. 만약 프리 프레임이 있으면, 사용한다.
   2. 만약 프리 프레임이 없으면, **victim frame**을 선택하기 위해서, 페이지 교체 알고리즘을 사용한다.
   3. victim 프레임을 2차 저장소에 쓰고, 페이지와 프레임 테이블을 바꾼다.
3. 새롭게 여유로워진 프레임에 원하는 페이지를 읽고, 페이지와 프레임 테이블을 바꾼다.
4. 페이지 폴트가 일어난 곳으로부터 프로세스를 진행한다.

만약, 빈 프레임이 없다면, 두 페이지의 전송(페이지 아웃과 페이지 인)이 필요하다. 이 상황은 페이지 폴트 서비스 시간을 두배로 하고 유효 접근 시간또한 따라 늘어난다.

우리는 **modify bit**(or **dirty bit**)를 이용해서 오버헤드를 줄인다. 이 구조가 사용되면, 각 페이지 또는 프레임은 하드웨어와 관련된 수정 비트를 가진다. 페이지의 수정 비트는 페이지에 어떤 바이트가 적히든 하드웨어에 의해서 설정되고, 페이지가 수정되었다는 것을 알린다. 우리가 페이지를 교체를 위해서 선택하면, 우리는 수정 비트를 확인한다. 만약 비트가 설정되어 있으면, 우리는 페이지가 2차 저장소로부터 읽힌 이후에 수정되었다는 것이다. 이런 경우에, 우리는 반드시 페이지를 저장소에 써야한다. 만약 수정 비트가 설정되어 있지 않으면, 페이지는 읽힌 이후로 수정되지 않은 것이다. 이런 경우에, 우리는 메모리 페이지를 저장소에 쓸 필요가 없다. 이 기술은 읽기 전용에도 쓰이는데, 그들은 원해질때 버려진다. 이 구조는 페이지 폴트를 서비스할때의 시간을 줄여주는데, 페이지가 수정되지 않았을 때, I/O시간을 절반으로 줄여주기 때문이다.

페이지 교체는 디맨드 페이징에 기초이다. 그것은 논리 메모리와 물리 메모리사이의 분리를 완성시킨다. 이 메커니즘과 함꼐라면, 거대한 가상 메모리가 작은 물리메모리에 제공될 수 있다. 디맨드 페이징이 없다면, 논리 주소들은 물리 주소로 매핑되어야하고, 주소의 두 집합은 달라질 수 있다. 프로세스의 모든 페이지들은 여전히 모든 물리 메모리에 있게된다. 디맨드 페이징과 함께라면, 논리 주소 공간의 크기는 더이상 물리 메모리에 제한되지 않는다. 만약 우리가 20페이지의 프로세스가 있으면, 우리는 디맨드 페이징으로 단순히 10개의 페이지로 실행하고 필요해지면 언제든 프리프레임을 찾는 교체 알고리즘을 사용한다. 만약 교체될 페이지가 수정되었으면, 그것의 컨텐츠는 2차 저장소로 카피된다. 또다시 그 페이지가 참조되면 페이지 폴트를 일으킬 것이다. 페이지는 다시 메모리로 돌아오고, 프로세스의 다른 페이지를 교체할 것이다.

우리는 디맨드 페이징을 구현하기 위해서 두가지 주요한 문제를 풀어야한다. 우리는 반드시 **frame-allocation algroithm**과 **page-replacement algorithm**을 개발해야한다. 즉, 만약 메모리에 여러 프로세스가 있으면, 우리는 각 프로세스에 얼마나 많은 프레임을 할당해야하고 페이지 교체가 필요할 때, 우리는 반드시 교체될 프레임을 선택해야한다. 이런 문제를 해결하는 적절한 알고리즘을 디자인하는 것은 중요한 작업인데, 왜냐하면 2차 저장소 I/O가 비싸기 때문이다. 디맨드 페이징의 약간의 성능 향상은 시스템 성능에서 큰 게인을 얻을 수 있다.

그곳에는 많은 페이지 교체 알고리즘이 있다. 모든 운영체제는 그것의 자체적인 교체 구조를 가진다. 어떻게 특정 교체 알고리즘을 선택할까? 일반적으로는, 우리는 가장 낮은 페이지 폴트율을 원한다.

우리는 알고리즘을 특정 메모리 레퍼런스의 스트링에서 실행하고 페이지 폴트의 수를 계산한다. 메모리 레퍼런스의 스트링은 **reference string**이라고 불린다. 우리는 인위적으로 레퍼런스 스트링을 만들 수 있고, 우리는 주어진 시스템을 추적하고 메모리 레퍼런스의 주소를 저장한다. 후자의 경우에는 초당 백만 가량의 주소가 나온다. 이 데이터의 수를 줄이기 위해서 우리는 두가지 사실을 사용한다.

첫번째, 주어진 페이지 크기에서, 우리는 전체 주소가 아니라 오직 페이지 넘버만 고려하면 된다. 두번째, 만약 우리가 페이지 p에 대한 레퍼런스를 가지면, 페이지 p에 대한 어떤 레퍼런스도 페이지 폴트를 일으키지 않을 것이다. 페이지 p는 첫번째 레퍼런스 이후에 메모리에 있을 것이고, 그래서 따라오는 레퍼런스들은 즉시 폴트하지 않을 것이다.

예를 들어서, 우리가 특정 프로세스를 추적하면, 우리는 다음과 같은 주소 시퀀스를 가진다.
` 0100, 0432, 0101, 0601, 0102, 0103, 0104, 0611, 0101, 0103, 0601, 0103, 0104, 0609, 0102, 0105`와 같이 있다면 페이지당 100바이트라면 다음과 같이 줄어들 수 있다. 
`1, 4, 1, 6, 1, 6, 1, 6, 1, 6, 1`

특정 레퍼런스 스트링과 페이지 교체 알고리즘에서 페이지 폴트의 수를 결정하기 위해서, 우리는 가용한 페이지 프레임의 수를 알아야할 필요가 있다. 명백히, 가용한 프레임의 수가 늘어 날수록, 페이지 폴트의 수가 줄어들 것이다. 위의 예시에서 만약 3개보다 많은 프레임이 있으면, 우리는 각 페이지의 첫 시작만 폴트를 일으켜서 3번으로 끝날 것이다. 반대로, 한개의 페이지만 있으면, 매 레퍼런스마다 교체가 일어나서 11번의 폴트가 일어날 것이다. 페이지 폴트의 수는 페이지 프레임의 수에 반비례한다.

### 10.4.2 FIFO 페이지 교체

가장 간단한 페이지 교체정책은 FIFO이다. FIFO 교체 알고리즘은 페이지가 메모리로 왔을때 각 페이지가 메모리로 도달했을때이다. 페이지가 교체되어야하면, 가장 오래된 페이지가 선택된다. 이것은 페이지가 도달했을때 시간을 저장하는 것까지는 아니다. 우리는 FIFO 큐를 통해서 메모리에 모든 페이지를 저장한다. 우리는 큐의 헤드 페이지를 교체한다. 그리고 페이지가 메모리에 들어오면, 우리는 큐의 테일에 삽입한다.

우리의 예시 레퍼런스 스트링에서, 3가지 프레임이 초기에 비어있다. 첫번째 (7,0,1)은 페이지 폴트를 일으키고 이 빈 프레임에 채워진다. 0이 다음 레퍼런스이고 이미 메모리에 있으면, 이 레퍼런스에 대해서 페이지 폴트를 취하지 않는다. 3에 대한 첫번째 참조는 0을 교체하게 한다. 이 프로세스는 아래의 표로 보이겠다. 총 15번의 페이지 폴트가 일어난다.

|reference stirng |7|0|1|2|0|3|0|4|2|3|0|3|2|1|2|0|1|7|0|1|
|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|
|페이지폴트 수 |1|2|3|4|-|5|6|7|8|9|10|-|-|11|12|-|-|13|14|15|
|head |7|7|7|0|0|1|2|3|0|4|2|2|2|3|0|0|0|1|2|7|
|-|-|0|0|1|1|2|3|0|4|2|3|3|3|0|1|1|1|2|7|0|
|tail |-|-|1|2|2|3|0|4|2|3|0|0|0|1|2|2|2|7|0|1|

피포 페이지 교체 알고리즘은 이해와 프로그래밍이 쉽다. 그러나, 그것의 성능은 항상 좋지 않다. 페이지 교체가 오래전에 사용되고 더이상 필요하지 않은 초기화 모듈일 수 있다. 다른 경우에는, 그것은 초기에 선언되었지만 자주 사용되는 것일 수도 있다.

만약 우리가 활동중인 페이지 교체 알고리즘을 사용하면, 모든 것은 여전히 바르게 작동한다. 우리가 활동중인 페이지를 새로운 페이지로 교체하면, 폴트는 활동중인 페이지를 복구하기위해서 즉시 일어난다. 몇몇 다른 페이지들은 활동중인 페이지를 메모리에 넣기위해서 교체될 것이다. 그러므로, 나쁜 교체 선택이 페이지 폴트를 늘리고 프로세스 실행을 지연시킨다. 

피포 알고리즘의 잠재적인 문제를 위해서 다음의 레퍼런스 스트링을 보겠다.
`1,2,3,4,1,2,5,1,2,3,4,5`
이 경우에는 프레임이 3개일때는 9번의 폴트가 일어나는데 비해서, 4개라면 10번의 폴트가 일어난다. 이 결과를 **Belady's anomaly(변칙)**라고 한다. 몇몇 페이지 교체 알고리즘은 할당 가능한 프레임이 증가해도 페이지 폴트가 늘어나기도 한다. 우리는 일반적으로 메모리가 증가하면, 성능이 향상된다고 믿는다. 최근의 연구에서, 이 가정은 항상 옳지는 않다고 알려졌다. 벨라디의 변칙은 결과로 나타났다.

### 10.4.3 Optimal page Replacement

벨라디의 변칙의 반결의 결과는 **optimal page replacement algorithm**을 찾기위한 것도 포함되어있다. 이 알고리즘은 가장 낮은 페이지 폴트를 가지고 벨라디의 변칙도 겪지 않는다. 이런 알고리즘은 보통 OPT or MIN이라고 불린다. 원리는 가장 긴 시간동안 사용되지 않을 페이지를 교체하는 것이다. 이 페이지 교체 알고리즘의 사용은 고정된 프레임 수에서 최저의 페이지 폴트를 보장한다.

예를 들어서, 우리의 샘플 레퍼런스 스트링에서, 오직 9번의 페이지 교체가 일어난다. 처음으로 교체되는 시점에 7이 18번째가 되어야 쓰이기 때문에 2와 교체된다. 

|reference stirng |7|0|1|2|0|3|0|4|2|3|0|3|2|1|2|0|1|7|0|1|  
|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|
|페이지폴트|1|2|3|4|-|5|-|6|-|-|7|-|-|8|-|-|-|9|-|-|
|head |7|7|7|2|-|2|-|2|-|-|2|-|-|2|-|-|-|7|-|-| 
| |-|0|0|0|-|0|-|4|-|-|0|-|-|0|-|-|-|0|-|-| 
|tail|-|-|1|1|-|3|-|3|-|-|3|-|-|1|-|-|-|1|-|-| 

9번의 페이지 폴트로, FIFO보다 좋은 성능의 교체이다. 실제로, 어떤 교체 알고리즘도 이 레퍼런스를 3개의 프레임으로 9번 이하로 실행시킬수 없다.

불행히도, 최적화 페이지 교체 알고리즘은 구현이 힘들다. 왜냐하면 그것은 레퍼런스 스트링의 미래의 정보를 필요로하기 때문이다. 결과적으로, 최적 알고리즘은 비교 연구에만 쓰인다. 예를 들어서, 새로운 알고리즘이 최적이 아니더라도, 그것은 최적의 최악 12퍼센트이거나 평균 4.7퍼센트정도인지로 쓸모를 판명할 수 있다.

### 10.4.4 LRU 페이지 교체

만약 최적 알고리즘이 실현 가능하지 않으면, 최적 알고리즘의 근사화는 가능할 것이다. FIFO와 OPT 알고리즘의 주요한 차이점은 FIFO 알고리즘은 페이지가 온 시간만을 사용하는데 비해서, OPT 알고리즘은 페이지가 사용된 시간을 사용하는 것이다. 만약 우리가 가까운 미래의 근사로 가까운 과거를 사용하면, 가장 긴 기간동안 사용되지 않은 페이지를 교체하는 것이다. 이 접근은 **least recently used(LRU)** 알고리즘이다.

LRU 교체는 가장 마지막에 사용된 페이지의 시간을 기억한다. 페이지가 반드시 교체되어야하면, LRU는 긴 시간동안 사용되지 않은 페이지를 선택한다. 우리는 이 전략을 미래가 아닌 과거를 보는 최적 페이지 교체 알고리즘이라고 한다. (이상하게도, 우리가 레퍼런스 스트링의 리버스를 Sr이라고 하면, S에서의 OPT 알고리즘의 페이지 폴트 레이트는 Sr의 OPT 페이지 값과 같다. 비슷하게 S에서의 LRU 폴트 값은 Sr의 페이지 폴트 값과 같다.)

LRU 교체를 적용한 것의 결과는 아래의 표와 같다. LRU 알고리즘은 12개의 폴트를 만든다. 첫번쨰 5번의 폴트는 최적 교체와 같다. 페이지 4에 대한 참조가 일어나면, LRU는 페이지 2를 교체한다. 그러므로 LRU 알고리즘은 페이지 2가 조만간 쓰일 것을 알지 못한채로 페이지 2를 교체한다. 이러한 문제에도 불구하고 LRU 교체 알고리즘은 FIFO보다 적은 12번의 폴트를 만든다.

LRU 정책은 보통 페이지 교체 알고리즘에 쓰이고 좋게 평가된다. 주요한 문제는 어떻게 LRU를 구현하는 가이다. LRU 페이지 교체 알고리즘은 잠재적인 하드웨어 지원이 필요할 수도 있다. 문제는 마지막에 사용된 프레임을 정하는 것이다. 두가지 구현이 가능하다.

- Counters : 가장 단순한 케이스로서, 우리는 페이지 테이블 엔트리를 사용한 시간의 필드값에 CPU의 논리 클락 또는 카운터를 더하는 것이다. 클락은 매 메모리 참조마다 증가한다. 페이지에 대한 레퍼런스가 만들어질때마다, 클락 레지스터의 카운터는 페이지 테이블 엔트리의 사용 시간 필드에 복사된다. 이런 방법으로, 우리는 항상 각 페이지의 마지막 레퍼런스를 확인할 수 있다. 이 구조는 LRU 페이지를 찾기 위한 검색과 메모리에 쓰는 것이 필요하다. 시간은 페이지 테이블들이 바뀔때에도 유지되어야한다. 클락의 오버플로 또한 반드시 고려되어야한다.
- 스택 : 다른 LRU를 구현하는 방법은 페이지 넘버의 스택을 유지하는 것이다. 페이지가 참조될때마다, 참조된 페이지는 스택에서 제거되고 탑에 배치된다. 이런 방법으로, 가장 최근에 사용된 페이지는 항상 스택의 탑에 배치되고 가장 오래된 것은 바닥에 존재한다. 엔트리가 스택의 중간에서 사라지기 때문에, 이 방법을 헤드 포인터와 테일 포인터를 가진 더블리 링크드 리스트로 구현하는 것이 최고이다. 페이지를 스택의 탑과 바텀에서 제거하는 것은 6번의 포인터 접근으로 해결가능하다. 각 업테이트는 꽤 비싸지만, 교체를 위한 검색은 필요하지 않다. 테일 포인터는 LRU 페이지인 스택의 바닥을 가르킨다. 이 접근은 LRU 교체의 구현을 위한 소프트웨어에 적합하다.

최적 교체와 비슷하게, LRU 교체는 벨라디의 변칙에 영향 받지 않는다. 두 페이지 교체 알고리즘의 클래스는, **stack algorithms**라고 불리고, 벨라디의 변칙을 결코 발생시키지 않는다. 스택 알고리즘은 n프레임에 대한 페이지의 집합이 항상 n+1의 프레임에 있는 페이지의 부분집합이다. LRU 교체에서, 페이지의 집합은 최대 n의 최근 참조한 페이지이다. 만약 프레임의 수가 증가하면, 이러한 n 페이지들은 메모리에 있는 최근의 페이지이고 메모리에 존재한다.

LRU의 구현은 표준 TLB 레지스터 하드웨어 지원 없이는 상상가능하지 않다. 클락 필드 또는 스택의 업데이트는 매 메모리 참조마다 일어난다. 만약 우리가 매 인터럽트마다 소프트웨어가 이런 데이터 구조를 업데이트하면, 메모리 레퍼런스는 10배 정도 느려질 것이다. 몇몇 시스템은 메모리 관리를 위한 오버헤드의 레벨을 참는다.

### 10.4.5 LRU-Approximation Page Replacement

많은 컴퓨터 시스템들은 LRU 페이지 교체를 위한 충분한 하드웨어 지원을 제공하지 못한다. 실제로, 몇몇 시스템들은 하드웨어 지원을 제공하지 않고, 다른 페이지 교체 알고리즘(FIFO알고리즘)을 사용한다. 많은 시스템들은 약간의 도움을 제공하는데, **reference bit**의 형식이다. 페이지의 레퍼런스 비트는 페이지가 참조되었을때 세팅된다. 레퍼런스 비트들은 페이지테이블의 각 엔트리와 관련되어있다.

초기에, 모든 비트는 운영체제에 의해서 0으로 초기화된다. 프로세스가 실행되면, 각 페이지와 연관있는 비트는 하드웨어에 의해서 세팅된다. 그 후에, 우리는 레퍼런스 비트를 통해서 어떤 페이지가 사용되었고 사용되지않았는지를 결정한다. 사용의 순서는 알지 못한다. 이 정보는 LRU 근사화를 한 많은 페이지 교체 알고리즘이다.

#### 10.4.5.1 추가 레퍼런스 비트 알고리즘

우리는 일정 간격마다 레퍼런스비트를 저장함으로서 정보의 추가적인 순서를 알 수 있다. 우리는 메모리의 각 페이지에 8비트를 보관할 수 있다. 균일한 간격(100미리초)으로, 타이머는 운영체제에게 컨트롤을 넘긴다. 운영체제는 각 페이지의 레퍼런스비트를 8비트의 높은 순서로 이동한다. 1비트씩 오른쪽으로 움직이면 가장 낮은 비트를 버리는 것이다. 이런 8비트 쉬프트 레지스터들은 8번의 기간동안 페이지가 사용되었는지를 확인한다. 만약 쉬프트 레지스터가 00000000을 포함하면, 페이지는 8번의 주기동안 사용되지 않은 것이다. 각 주기동안 한번씩 실행된 페이지는 11111111을 가진다. 11000100을 가진 히스토리 레지스터는 01110111보다 더 최근에 쓰였다는 것이다. 만약 우리가 이 8비트를 부호가 없는 정수로 사용하면, 가장 적은 숫자를 가진 페이지가 LRU인 것이고, 교체될 것이다. 숫자들이 유일하지는 않고, 가장 작은 값들끼리는 FIFO의 구조를 가질 수 있다.

쉬프트 레지스터의 히스토리 비트의 숫자는 다양해질 수 있고, 최대한 빠르게 업데이트되도록 만들어진다. 극단적인 에시로는, 0으로 줄여서 한개의 레퍼런스비트 자체로 만들수 있다. 이 알고리즘은 **second chance page-replacement algorithm**이라고 불린다.

#### 10.4.5.2 Second chance algorithm(Clock algorithm)

second chance 교체의 기본적인 알고리은 FIFO 교체 알고리즘이다. 페이지가 선택되면, 우리는 그것의 레퍼런스 비트를 살펴본다. 만약에 값이 0이면, 우리는 이 페이지를 교체한다. 그러나 만약 레퍼런스 비트가 1로 세팅되면, 우리는 페이지에게 두번째 기회를 주고 다음 FIFO 페이지에 선택되게 이동한다. 페이지가 두번째 기회를 얻으면, 그것의 참조 비트는 비워지고, 그것의 도착시간은 현재로 리셋된다. 그러므로, 두번째 기회를 받은 페이지는 다른 모든 페이지가 교체되기전까지 교체되지 않는다. 추가로, 만약 페이지가 그것의 레퍼런스 비트를 유지될만큼 사용되면, 그것은 결코 교체되지않는다.

이것을 구현하는 방법은 환형 큐이다. 포인터는 교체될 다음 페이지를 가르킨다. 프레임이 필요해지면, 포인터는 레퍼런스비트가 0인 비트를 찾을때까지 이동한다. 한번 피해 페이지가 찾아지면, 페이지는 교체되고, 새로운 페이지가 환형큐에 들어가게된다. 최악의 경우에는, 모든 비트가 세팅되있어서, 전체 큐의 포인터 사이클은 각 페이지에 모든페이지에 두번째 기회를 준것이다. 그것은 모든 레퍼런스 비트를 다음 페이지 교체전에 초기화한다. 만약 모든 비트가 세팅되어있으면, 세컨드 찬스는 FIFO 교체를 악화시킨것 뿐이다.

#### 10.4.5.3 Enhanced Second-Chance Algorithm

우리는 세컨드 찬스 알고리즘을 레퍼런스비트와 modify 비트의 정렬된 페어를 통해서 강화시킬수 있다.  2개의 비트로 우리는 4가지 클래스를 만들 수 있다.
1. (0, 0) 최근에 사용되지도 수정되지도 않은 페이지 - 교체하기에 최고의 페이지
2. (0, 1) 최근에 사용은 안되었지만, 수정은 된 페이지 - 썩 좋지는 않은데, 왜냐하면 페이지가 교체이전에 쓰여야하기 때문이다.
3. (1, 0) 최근에 사용되었지만 깔끔함 - 최근에 다시 쓰일 수 있음
4. (1, 1) 최근에 사용되고 수정됨 - 조만간 쓰일 수 있고, 페이지가 2차저장소에 교체되기전에 쓰여야할 필요가 있음

각 페이지는 4개의 클래스중 하나이다. 페이지 교체가 불리면, 우리는 클락 알고리즘과 같은 구조를 사용한다. 그러나 우리의 레퍼런스 비트가 1을 가르키기보다는 어떤 클래스에 속해있는지를 확인한다. 우리는 가장 낮은 빈 클래스의 첫번째 페이지를 교체한다. 우리는 페이지가 교체되기위해서 환형 큐를 몇번 스캔할 수도 있다. 주요한 차이는 우리는 수정 여부를 따짐으로서 I/O 필요를 줄였다는 것이다.

### 10.4.6 Counting-Based Page Replacement

페이지 교체를 위한 다양한 알고리즘이 있다. 예를 들어서, 우리는 레퍼런스의 수를 유지하는 카운터를 사용하는 두가지 구조를 볼수있다.

- **Least Frequently used(LFU)** 페이지 교체 알고리즘은 가장 적은 카운트의 페이지가 교체된다. 이 선택의 이유는 활발하게 사용된 페이지가 큰 레퍼런스 카운터를 가지기 때문이다. 문제는, 페이지가 초기에만 활발하게 쓰이고 쓰이지 않을때이다. 그것이 많이 사용된 직후에, 그것은 큰 카운트를 가지고 더 이상 필요가 없어도 남아있는다. 이것을 해결하는 방법은 카운트를 매 주기마다 1비트씩 옮기는 것이다. 그래서 부패가 지수적으로 형성된다.
- **Most Frequently used(MFU)** 페이지 교체 알고리즘은 가장 적은 카운트를 가진 것은 최근에 가져온 것이고 아직 사용되지 않았다고 가정한 것이다.

너가 예상했듯이, MFU와 LFU는 일반적이지 않다. 구현에 드는 알고리즘이 비싸고, 그들은 OPT의 근사화또한 되지 않는다.

### 10.4.7 페이지 버퍼링 알고리즘

다른 절차는 보통 특정 페이지 교체 알고리즘을 사용한다. 예를 들어, 시스템은 보통 여유 프레임의 풀을 유지한다. 페이지 폴트가 일어나면, 피해자 프레임은 그전에 선택된다. 그러나, 원하는 페이지는 피해자가 쓰기전에 여유프레임에 읽힌다. 이 절차는 프로세스가 피해 페이지가 쓰이기전에 가능한 빠르게 재시작하게 한다. 피해자가 쓰여지면, 그것의 프레임은 여유 프레임 풀에 들어간다.

이 아이디어의 확장은 수정된 페이지의 리스트를 유지하는 것이다. 페이징 디바이스가 유휴상태이면, 수정된 페이지는 선택되고 2차 저장소로 쓰여진다. 그것의 수정비트는 리셋된다. 이 구조는 페이지가 깔끔할(수정이 되어서 2차메모리에 쓰지않아도 되는 상태) 가능성을 높여준다.

다른 수정은 여유 프레임의 풀을 유지하지만, 각 프레임에 어떤 페이지인지 기억하는 것이다. 프레임 컨텐츠가 수정되지 않으면, 프레임은 2차 저장소에 쓰이고, 늙은 페이지는 여유 프레임 풀에 다시 사용된다. 이 경우에는 I/O가 필요 없다. 페이지 폴트가 일어나면, 우리는 우선 프리 프레임 풀에서 원하는 페이지를 찾는다. 만약에 그렇지 않으면, 여유프레임을 선택해서 읽어드려야한다.

유닉스 시스템의 몇가지 버전은 세컨드 찬스와 이것을 섞어서 쓴다. 이 것은 잘못된 피해 페이지를 선택해서 생기는 손해를 줄여주는 페이지 교체 알고리즘이다. 더 자세한 것은 10.5.3절에서 말해주겠다.

### 10.4.8 앱과 페이지 교체

특정 상황에서, 운영체제의 가상 메모리를 통해서 데이터를 접근하는 앱은 만약 운영체제가 버퍼링을 전혀 제공하지 않으면 더 느리게 작동한다. 전형적인 예시는 데이터 베이스이고, 그것의 고유한 메모리 관리와 I/O 버퍼링을 제공한다. 이런 앱들은 그들의 메모리 사용과 저장소 사용을 이해하고 구현된 알고리즘들은 특정 목적 사용을 위해서 운영체제보다 잘 구현되어있다. 더 나아가, 만약 운영체제가 I/O를 버퍼링하고 앱또한 그런다면, 메모리의 두배가 I/O 세팅에 사용된다.

다른 예시는, 데이터 웨어하우스는 보통 거대한 저장 리드를 실행한다. LRU 알고리즘은 낡은 페이지를 제거하고 새로운 것을 보존한다.(앱들은 새로운 것을 읽는 것보다 낡은 것을 더 선호하고 있다.) 여기서는 MFU가 LRU보다 효율 적이다.

이런 문제때문에, 몇며 운영체제들은 2차 저장소 파티션을 파일 시스템의 데이터 구조 없이 거대한 논리 블럭의 큰 연속 행렬로 사용할 능력을 프로그램에게 준다. 이 행렬은 **raw disk**라고 부르는데, 이 행렬의 I/O는 raw I/O라고 불린다. raw I/O는 파일 I/O 디맨드 페이징, 파일 라킹, 프리패칭, 공간 할당, 파일 이름, 데이터 같은 모든 파일 시스템 서비스를 우회한다. 비록 그들이 그들만의 공간에서 특수한 목적으로 효율적으로 사용될 수있지만, 대부분의 앱들은 일반적인 파일 시스템을 사용할때 더욱 잘 작동한다.

## 10.5 프레임 할당

우리는 이제 할당에 대한 이슈를 살펴보겠다. 우리가 어떻게 다양한 프로세스 사이에서 고정된 여유메모리를 할당할수 있을까? 만약 93개의 여유 프레임과 2개의 프로세스가 있다면, 각 프로세스는 몇개의 프레임을 가질까?

128개의 프레임을 가진 간단한 예시를 들겠다. 운영체제가 35개를 가지고, 93개는 유저 프로세스를 위한 프레임으로 남긴다. 순수한 디맨드 페이징에서, 93개의 프레임은 모두 여유로운 상태로 초기화 된다. 유저 프로세스가 실행을 시작하면, 그것은 페이지 폴트의 시퀀스를 만들 것이다. 첫번째 93개의 페이지 폴트는 모든 프레임을 여유 프레임 리스트에서 가져온다. 프리 프레임 리스트가 소진되면, 페이지 교체 알고리즘이 93개의 메모리중에 하나를 골라서 94번쨰 페이지를 교체할 것이다. 프로세스가 끝나면, 93개의 프레임은 다시 여유 프레임 리스트로 돌아온다.

단순한 전략에는 많은 변형이 있다. 우리는 운영체제가 여유 프레임으로부터 그것의 버퍼와 테이블을 모두 할당하기를 원한다. 이 공간이 운영체제에 의해서 쓰이지 않으면, 그것은 유저 페이징을 지원하기 위해서 쓰일수도 있다. 우리는 3개의 여유 프레임을 항상 유지할 수도 있다. 그러므로, 페이지 폴트가 일어나면, 페이지 인 할 프레임이 3개는 항상 있다는 것이다. 페이지 스왑하는 도중에, 교체는 선택될 수 있고, 유저 프로세스가 실행할때 저장 기기에 쓰일 수도 있다. 다른 변형들도 가능하지만, 기초적인 전략은 명료하다. 유저 프레임은 어느 여유 프레임에 할당된다.

### 10.5.1 Minimum Number of Frames

프레임의 할당을 위한 전략은 다양한 방법으로 제한된다. 우리는 가용한 프레임 이상을 할당할 수 없다. 우리는 반드시 최소한의 프레임을 할당해야한다. 여기서, 우리는 후자의 필요를 더욱 살펴보겠다. 최소한의 프레임을 할당하는 이유는 성능을 포함한다. 명백하게, 각 프로세스에 할당되는 프레임의 수가 감소하면, 페이지 폴트 비율은 늘어나고, 프로세스 실행은 느려진다. 추가로, 페이지 폴트가 실행 멸령어 완료 이전에 일어나면, 그 명령어는 반드시 재시작되어야한다. 결과적으로 우리는 어느 명령어가 참조할 수 있도록 다른 모든 페이지를 붙잡아야한다.

예를 들어서, 메모리 참조 명령어가 오직하나의 메모리 주소를 참조하는 기계를 고려하자. 이런 경우에, 우리는 명령어를 위한 프레임과 메모리 레퍼런스를 위한 프레임이 필요하다.(에를 들어서, 16번 프레임에서 실행되는 `load` 명령어는 0번 프레임을 참고하지만, 23은 간접참조를 한다.) 그러면 페이징은 프로세스당 최소 3개씩 필요하다. 

프레임의 최소수는 컴퓨터 구조에 의해서 정의된다. 예를 들어서, 만약 이동 명령어가 몇몇 주소 모드에서 하나 이상의 워드가 포함된다면, 명령어는 두개의 프레임을 가로지를 수 있다. 만약 그것의 두 피연산자가 간접 참조이면, 총 6개의 프레임이 필요한 것이다. 다른 예로는, 인텔 32와 64비트 구조는 데이터가 레지스터와 레지스터, 레지스터와 메모리 사이에만 움직이게 한다. 그것은 직접적인 메모리-메모리 이동을 지원하지 않고, 최소한의 프레임이 필요해진다.

프로세스당 최소 프레임수는 구조에 정해지는 것에 비해서 최대 숫자는 가용한 물리 메모리에 의해서 결정된다. 두 사항 모두, 우리에게 프레임 할당에 대한 큰 선택으로 남겨져 있다.

### 10.5.2 할당 알고리즘

n개의 프로세스에서 m개의 프레임으로 분할하는 가장 쉬운 방법은 같은 지분을 모두에게 주는 것이다. 예를 들어서 93개의 프레임과 5개의 프로세스가 있다면, 각 프로세스는 18개의 프레임을 얻고 나머지 3개는 프리 프레임 버퍼 풀에서 사용한다. 이런 구조는 **equal allocatoin**이라고 한다.

다른 대안은 다양한 프로세스가 다른 양의 메모리가 필요하다는 발상에서 시작한다. 1KB의 프레임 크기를 가진 시스템이다. 만약 작은 학생 프로세스가 10KB이고 상호작용 데이터베이스가 127KB이다. 이 두개의 프로세스가 62개의 여유 프레임에서 돌아간다. 각 프로세스에게 31개를 배분하는 것은 일리가 없다. 학생프로세스는 10개 이상이 필요하지 않고 21개는 결국 낭비되게한다.

이 문제를 해결하기 위해서 우리는 **proportional allocation**을 사용할 수 있는데, 가용 메모리를 그것의 사이즈에 맞게 배분하는 것이다. `62 * 10/(127+10)`과 같이 할당을 할수 잇다.

이 방법에서는, 평등보다는 필요에 의해서 가용한 프레임을 공유한다.

eqaul and proportional 할당에서, 할당은 멀티프로그래밍 레벨에 따라서 달라질 수도 있다. 멀티프로그래밍 레벨이 증가하면, 각 프로세스는 새로운 프로세스를 위해서 메모리를 잃을 것이다. 반대로, 감소하면, 떠나는 프로세스에 할당된 프레임을 남은 프로세스에 나눠진다.

eqaul 또는 proportional 할당에서, 우선 순위 프로세스는 낮은 순위 프로세스와 같게 다뤄진다. 정의에 따라서, 우리는 우선 순위의 프로세스에 많은 메모리를 주기를 원할 수 있다. 이 방법은 비례 할당을 프로세스의 상대적인 크기가 아니라 사이즈와 우선도의 혼합으로 바꿀수 있다.

### 10.5.3 Global versus Local 할당

프레임이 다양한 프로세스에 할당되는 다른 중요한 요소는 페이지 교체이다. 다중 프로세스가 프레임을 두고 경쟁할떄, 우리는 페이지 교체 알고리즘을 두가지 거대한 카테고리로 나눌 수 있다. **global replacement**와 **local replacement**이다. 전역 교체는 프로세스가 모든 프레임에서 교체를 할 수 있는 것이고, 그 프레임이 다른 프로세스에 의해서 할당되어있어도 교체한다. 즉, 한 프로세스가 다른 프로세스에서 가져올 수 있다. 지역 교체는 각 프로세스가 그것의 할당된 프레임 자체에서만 교체가능한 것이다.

예를 들어서, 높은 우선순위 프로세스가 낮은 우선 순위 프로세스에서 교체를 위해서 선택되는 할당 구조를 생각해보자. 프로세스는 그것의 프레임 또는 낮은 순위의 프로세스의 프레임에서 교체를 위해서 선택가능하다. 이 접근은 높은 우선순위 프로세스이 그것의 프레임 할당을 낮은 순위의 프로세스의 비용에 비해서 상승시킨다. 반면에 지역 교체 전략에서는, 프로세스에 할당된 프레임의 수는 변하지 않고, 전역 교체에서는, 프로세스는 다른 프로세스에서만 프레임을 선택하고, 할당된 프레임의 수를 늘린다.

전역 교체 알고리즘의 문제는 프로세스의 메모리 안 페이지의 집합이 프로세스의 페이징 행동뿐만 아니라, 다른 프로세스의 페이징 행동에도 영향을 받는 것이다. 그러므로, 같은 프로세스는 왜냐하면 외부환경때문에 꽤 다르게 작동할 수 있다.(한개의 실행에 0.5초 다음 실행은 4.3초) 이런 상황은 지역 교체 알고리즘에서는 생기지 않는다. 지역 교체에서, 페이지의 집합은 오직 그것의 페이징 행동에만 영향을 받는다. 지역 교체는 프로세스를 저해할 수 있지만, 다른 것에 의해서 가용하지 않아서, 메모리의 페이지를 덜 쓴다. 그러므로, 전역 교체는 더 큰 시스템 산출량을 만들어낸다. 이것이 우리가 더 주로 쓰는 이유이다.

다음으로는, 우리는 전역 페이지 교체 정책을 구현할 수 있는 전략을 보겠다. 이 접근으로, 우리는 프리 프레임 리스트로부터 모든 메모리 요청을 만족시키고, 우리가 페이지 교체를 시작하기 이전에 리스트가 0으로 떨어지는 것을 기다리기보다는, 우리는 리스트가 특정 스레시홀드로 떨어질때 페이지 교체를 일으킨다. 이 전략은 항상 새로운 요청에 충분한 여유 메모리가 있게 해준다.

이런 전략의 목적은 메모리가 최소한의 스레시홀드를 여유롭게 가지는 것이다. 그것이 스레시홀드 밑으로 떨어지면, 커널 루틴은 시스템의 모든 프로세스가 페이지를 재정의하게 시작한다. 이런 커널 루틴은 **reapers**라고 불리고, 그들은 10.4절에서한 페이지 교체 알고리즘을  적용한다. 여유 메모리의 양이 최대 스레시 홀드에 도달하면, 리퍼는 중단되고, 여유 메모리가 스레시 홀드 밑으로 떨어지면 재시작한다. 

위에서 언급했듯이, 커널 리퍼 루틴은 어느 페이지 교체 알고리즘을 선택했고, 보통은 LRU 근사화를 사용한다. 만약 리퍼가 최소 스레시홀드의 여유 프레임을 못 유지한다면 무슨 일이 일어날까? 이런 상황에서, 리퍼 루틴은 페이지를 공격적으로 교채하기 시작할 거싱다. 예를 들어서, 그것은 세컨드 찬스 알고리즘을 사용하지 않고 순수 FIFO를 사용한다. 다른 극단적인 예시로는, 예시는 리눅스이다. 여유 메모리의 양이 매우 적어지면, **out of memory(OOM) killer**가 프로세스를 종료시키고, 메모리를 풀어준다. 리눅스는 어떻게 종료될 프로세스를 정할까? 각 프로세스는 OOM 점수가 있고, 높은 점수는 OOM 킬러에게 종료될 가능성이 높다. OOM 스코어는 프로세스가 사용하는 메모리의 퍼센트에 따라서 계산되고, 높으면 높을 수록 OOM 점수가 높다.(이 점수는 /proc에서 볼수 있다.)

일반적으로, 리퍼 루틴이 얼마나 공격적으로 메모리를 재정의할지 다양할뿐 아니라, 최소/최대 스레시 홀드 또한 다양할 수 있다. 이런 값들은 디폴트 값으로 지정되고, 몇몇은 물리 메모리의 양에 따라서 시스템 관리자가 설정하는 것을 허용한다.

### 10.5.4 Non-Uniform Memory Access

가상 메모리의 범위에서, 우리는 메인 메모리가 항상 일정하거나 적어도 일정하게 접근된다고 가정했다. **non unifrom memory access(NUMA)** 시스템에서는 이런 경우가 아니다. 이러한 시스템에서는 주어진 CPU는 다른 CPU들보다 빠르게 접근가능한 부분이 있다. 이런 성능 차이는 어떻게 CPU가 연결되어 있는지에 야기된다. 이런 다중 CPU로 만들어진 시스템에서, 각각은 그것의 로컬 메모리를 가지고 있다. CPU는 공유 시스템 연결을 사용해서 구성되어있고, 예상 햇듯이 자신의 메모리가 다른 로컬 메모리보다 빠르게 된다. NUMA 시스템도 예외없이 모든 메인 메모리가 동일하게 취급되는 접근보다 느리다. 그러나, 1.3.2절에서 말했듯이 NUMA 시스템들은 많은 CPU를 가지고 있고 그러므로 더 높은 산출량과 병행성을 가진다.

어떤 페이지 프레임이 어디에 저장될지 관리하는것은 NUMA 시스템에서 성능에 큰 영향을 미친다. 만약 우리가 메모리를 동일하게 취급하면, CPU들은 만약 우리가 메모리 할당 알고리즘을 수정해서 메모리 접근에 더 긴 시간이 걸릴수도 있다. 우리는 이러한 수정을 5.5.4절에서 언급했다. 그들의 목표는 메모리 프레임을 최대한 작동하는 프로세스와 가깝게 가지는 것이다.(가깝다는 최소한의 레이턴시를 의미한다.) 그러므로, 프로세스가 페이지 폴트를 일으키면, NUMA용 가상 메모리 시스템은 현재 프로세스가 실행중인 CPU에 최대한 가깝게 할당하려고 할 것이다.

NUMA를 쓰기위해서, 스케쥴러는 프로세스가 작동한 마지막 CPU를 기록해야한다. 만약 스케쥴러가 각 프로세스를 이전의 CPU에 스케쥴하려면, 가상 메모리 시스템은 현재 스케쥴된 프로세스가 CPU와 가깝게 할당하려고 할것이고, 캐시 히트가 증가하고 메모리 접근 시간은 감소한다.

스레드가 추가되면 더욱 복잡해진다. 예를 들어서 많은 스레드를 가진 프로세스가 다른 시스템 보드에 스케쥴될 수 있다. 이럴때는 어떻게 메모리를 할당해야할까?

5.7.1절에서 언급했듯이, 리눅스는 스케쥴링 도메인의 계보를 가지고 있다. 리눅스 CFS 스케쥴러는 스레드가 다른 도메인에 이주하지 못하게 하고 그러므로 메모리 엑세스 패널티가 발생한다. 리눅스는 다른 프리 프레임 리스트가 각 NUMA 노드에 있고 그러므로 스레드가 현재 진행중인 노드에 할당되게 한다. 솔라리스는 비슷하게 **lgroups(locality groups)**를 운영한다. 각 그룹은 같은 CPU와 메모리에 모이고, 각 CPU는 그 그룹의 메모리에 접근을 빠르게할 수 있다. 솔라리스는 프로세스의 모든 스레드가 같은 그룹에서 할당되게 한다. 만약 불가능하면 가까운 그룹을 나머지 리소스를 위해서 고른다. 이런 노력은 전체 메모리 레이턴시를 줄이고 CPU 캐시 히트를 높였다.

## 10.6 Thrashing

만약 프로세스가 충분한 프레임을 가지지 못했다고 고려하겠다. 즉, 그것은 워킹 셋에 페이지를 지원한 최소의 프레임을 가지지 않았다는 것이다. 프로세스는 재빨리 페이지 폴트를 일으킬 것이다. 이 지점에서, 그것은 반드시 몇가지 페이지를 교체해야할 것이다. 그러나, 모든 페이지가 활동중이기에, 그것은 반드시 당장 다시 필요할 것을 교체하게된다.  결과적으로, 다시 페이지 폴트가 일어나고 또 일어나고, 다시 불러와야하는 모든 페이지를 교체한다.

이런 높은 페이징 활동을 **Thrashing**이라고 한다. 프로세스는 만약 실행시간보다 페이징에 많은 시간을 투자하면 스레싱이라고 한다. 너가 예측할 수 있듯이, 스래싱은 심각한 성능문제를 일으킨다.

### 10.6.1 스레싱의 원인

최근 페이징 시스템 실제 행동에 기반한 시나리오를 가정하겠다. 운영체제는 CPU 효율성을 모니터링한다. 만약 CPU 효율성이 너무 낮다면, 우리는 시스템에 새로운 프로세스를 소개함으로서 멀티프로그래밍의 단계를 높인다. 전역 페이지 교체 알고리즘이 사용된다. 이제 프로세스가 실행을 위해서 새로운 페이즈에 들어갔고 더 많은 프레임이 필요하다. 그것은 다른 프로세스로부터 폴팅하고 프레임을 가져온다. 뺏긴 프로세스들은 페이지가 필요하고, 그래서 그들 또한 폴트하고 다른 프로세스로부터 프레임을 가져온다. 폴팅 과정은 반드시 페이지를 스왑 인/아웃하기 위해서 페이징 디바이스를 사용한다. 큐가 페이징 디바이스를 위해서 준비되면, 레디큐는 비어진다. 프로세스가 페이징 디바이스를 기다리면, CPU 효율성은 내려간다.

CPU 스케쥴러는 CPU 효율성이 떨어지는 것을 확인하고 다시 멀티프로그래밍의 단계를 높인다. 새로운 프로세스는 실행 프로세스로부터 프레임을 뺏기 시작하고, 페이지 폴트를 일으키고 페이징 디바이스의 큐 시간을 길게한다. 결과적으로 CPU 효율성은 다시 떨어지고, CPU 스케쥴러는 더욱 멀티프로그래밍의 단계를 높인다. 스레싱이 일어나는 것이고, 시스템 산출량이 거꾸러진다. 페이지 폴트 비율은 심각하게 증가한다. 결과적으로 effective memory access time이 증가한다. 어떠한 일도 실행되지않고, 프로세스는 그들의 시간을 페이징하는 것에만 투자한다.

멀티 프로그래밍의 단계가 증가하면, CPU 효율성이 늘어나야지만, 최대치가 존재한다. 만약 멀티 프로그래밍이 최대치를 넘어서면, 스레싱이 일어나고, CPU 효율성은 급감한다. 이 시점에서, CPU 효율성을 증가시키고 스레싱을 중단하기 위해서는, 우리는 반드시 멀티프로그래밍의 단계를 낮춰야한다.

우리는 지역 교체 알고리즘(또는 **priority replacement algorithm**)을 사용해서 스레싱의 효과를 줄일 수 있다. 앞서 말했듯이, 지역 교체는 그것의 자체적인 할당된 프레임을 가진다. 그러므로, 만약 한 프로세스가 스레싱을 시작하면, 그것은 다른 프로세스로부터 프레임을 훔치지 못하고 계속 스레싱을 유발한다. 그러나, 문제는 완벽히 해결되지 않았다. 만약 프로세스가 스레싱이면, 그들은 페이징 기기의 큐에서 대부분의 시간을 보낼 것이다. 페이징 기기의 큐에 머무는 평균시간이 길어져서 페이지 폴트의 평균 서비스 시간이 증가한다. 그러므로, 스레싱이 아닌 프로세스의 effective access time마저 증가한다.

스레싱을 예방하기 위해서, 우리는 반드시 프로세스가 최대한 많은 프레임을 가지도록 해야한다. 그러나 어떻게 우리가 프레임을 얼마나 필요로하는지 앐수 있을까? 한가지 전략은 얼마나 많은 프레임이 프로세스에서 사용중인지 확인하는 것이다. 이 방법은 프로세스 실행의 **locality model**이다.

지역성 모델은, 프로세스가 실행중 일때, 그것은 locality에서 loacality로 이동한다. 로컬리티는 함께 사용중인 페이지의 집합이다. 실행중인 프로그램은 다양한 다른 로컬리티로 구성되어있고, 겹칠수도 있다. 예를 들어서, 함수가 불리면, 그것은 새로운 로컬리티를 정의한다. 이 로컬리티에서, 메모리 참조는 함수 콜의 명령어, 그것의 지역변수, 전역변수의 부분집합으로 만들어진다. 그것이 함수를 탈출하면, 프로세스는 로컬리티를 떠나고, 로컬 변수와 함수의 명령어는 더이상 쓰이지 않는다. 우리는 이 로컬리티를 리턴할 수도 있다.

로컬리티는 프로그램 구조와 그것의 데이터 구조에 의해서 정의된다. 로컬리티 모델은 모든 프로그램이 이 기초 메모리 참조 구조를 보인다고 말한다. 로컬리티 모델은 캐시의 일종이다. 만약 특정 데이터에 접근이 랜덤보다는 패턴화되있으면, 캐싱을 쓸모없어진다.

우리가 프로세스에게 그것의 로컬리티를 반영할 충분한 프레임을 제공했다고 가정하자. 페이지 폴트는 이 페이지들이 메모리에 있을떄까지 페이지 폴트를 일으킨다. 그때, 그것은 로컬리티를 바꾸기 전까지는 폴트하지 않는다. 만약 우리가 현재 로컬리티의 크기를 할당할만큼 충분한 프레임을 할당하지 않으면, 사용중인 페이지를 메모리에 둘수 없을 것이다.

### 10.6.3 Working Set Model

**Working-set model**은 로컬리티의 가정에 기반한다. 이 모델은 **Working-set window**를 정의하기 위한 델타 파라미터를 사용한다. 아이디어는 최근의 델타 페이지 참조를 측정하는 것이다. 가장 최근의 델타 페이지 레퍼런스에 있는 페이지의 집합은 **working set**이다. 만약 페이지가 현재 사용중이면, 그것은 워킹 집합에 들어갈 것이다. 만약 더이상 사용되지 않으면, 그것은 그것의 마지막 참조후 델타 타임 유닛 후에 워킹 셋으로부터 버려진다. 그러므로, 워킹 셋은 프로그램 로컬리티의 근사화이다.

예를 들어서, 델타 타임이 10이라고 가정하겠다. 그러면, 최근의 10초간의 워킹셋만이 있을 것이다.

워킹 셋의 정확도는 델타 시간의 선택에 달려있다. 만약 델타 시간이 너무 작으면, 그것은 전체 로컬리티를 어우르지 못할 것이다. 만약 델타 시간이 너무 길면, 그것은 몇가지 로컬리티를 겹칠 것이다. 극단적으로는, 만약 델타 시간이 무한대이면, 워킹 셋은 프로세스 실행중에 만져진 모든 페이지의 집합이다. 

워킹 셋의 가장 중요한 성질은 그것의 크기이다. 만약 우리가 워킹 집합 사이즈를 시스템의 각 프로세스에서 계산후 모두 더한 값을 D라고 하겠다. 그리고 그 D는 프레임에 필요한 총 수요이다. 각 프로세스는 그것의 워킹셋에서 페이지를 사용한다. 만약 총 수요가 가용한 프레임보다 크면, 스레싱이 일어난다.

한번 델타가 정해지면, 워킹셋 모델의 사용은 간편해진다. 운영체제는 각 프로세스의 워킹 셋을 모니터링하고 그것의 워킹셋 사이즈를 만족하는 충분한 프레임을 할당한다. 만약 워킹셋의 사이즈 합이 증가하면, 운영체제는 프로세스를 중단하는 것을 선택한다. 프로세스의 페이지들은 스왑되고, 그것의 프레임은 다른 프로세스들에게 재할당된다. 중지된 프로세스는 후에 재시작된다.

이 워킹 셋 전략은 멀티프로그래밍의 단계를 최대한 높이면서 스레싱을 예방한다. 그러므로, 그것은 CPU 효율성을 최적화한다. 워킹셋 모델의 힘든 점은 워킹셋을 추적하는 것이다. 워킹셋 윈도우는 움직이는 윈도우이다. 매 메모리 참조마다, 새로운 레퍼런스가 끝에 생기고, 가장 오래된 레퍼런스는 다른 끝에서 떨어진다. 페이지는 만약 그것이 워킹셋 윈도우에 있다면 언제든지 참조된다.

우리는 워킹셋 모델은 고정된 인터벌 타이머 인터럽트와 레퍼런스 비트로 근사화 가능하다. 예를 들어서 델타가 10000의 레퍼런스와 같고 타이머 인터럽트가 매 5000 레퍼런스마다 일어난다. 우리가 타이머 인터럽트가 일어나면, 우리는 각 페이지로부터 레퍼런스 비트를 복사하고 초기화한다. 그러므로, 만약 페이지 폴트가 일어나면, 우리는 현재의 레퍼런스 비트를 측정하고 페이지가 최근 10000에서 15000의 참조 사이에서 일어났는지 결정한다. 만약 그것이 사용되었으면, 최소한 하나의 비트는 켜져있을 것이다. 만약 사용되지 않았으면, 비트들은 꺼져있다. 한가지라도 켜져있는 비트는 워킹셋에 있다고 가정한다.

이 정렬은 완벽히 정확하지는 않은데, 왜냐하면 우리는 5000의 간격에서, 레퍼런스가 일어났는지 말할 수 없다. 우리는 이 불확실성을 히스토리 비트의 증가와 잦은 인터럽트로 줄일수는 있다. 그러나 이 서비스의 코스트는 꽤나 비쌀 것이다.

### 10.6.3 페이지 폴트 빈도

워킹셋 모델은 성공적이고, 워킹셋의 지식은 prepaging(10.9절)에도 유용하지만, 스레싱을 조절하기에는 서툴러보인다. **Page-fault frequency(PFF)**를 사용하는 전략은 더욱 직접적인 접근을 사용한다. 

특정한 문제는 어떻게 스레싱을 예방하는 것이다. 스레싱은 높은 페이지 폴트 확률을 가진다. 그러므로, 우리는 페이지 폴트율을 조절하고 싶다. 그것이 너무 높으면, 우리는 프로세스가 더 많은 프레임을 필요로 한 것을 안다. 반대로는, 만약 페이지 폴트가 낮으면, 프로세스는 너무 많은 프레임을 가진 것이다. 우리는 원하는 페이지 폴트 율을 위해서 upper와 lower 바운드를 설정할 수 있다. 만약 실제 페이지 폴트율이 upper limit를 초과하면, 우리는 프로세스에 다른 프레임을 할당한다. 만약 페이지폴트 율이 lower limit보다 떨어지면, 우리는 프로세스로부터 프레임을 삭제한다. 그러므로, 우리는 페이지 폴트율을 측정하고 조절함으로서 스레싱을 예방한다.

워킹셋 전략과 함께, 우리는 프로세스를 스왑해야 할 수 있다. 만약 페이지 폴트율이 증가하고 가용 프레임이 없으면, 우리는 반드시 배킹스토어에 스왑 아웃할 프로세스를 선택해야한다. 여유로워진 프레임은 높은 페이지 폴트율을 가진 프로세스에게 나눠진다.

### 10.6.4 Current practice

실직적으로 말하면, 스레싱과 스와핑의 결과는 퍼포먼스에 수용할수 없는 큰 임팩트를 가진다. 컴퓨터 시스템에 가장 최고의 구현은 충분한 물리 메모리를 포함해서, 스레싱과 스와핑을 방지하는 것이다. 스마트폰부터 큰 서버까지, 동시에 메모리에 있는 워킹셋을 보관할 충분한 메모리를 제공하는 것은 최고의 유저 경험을 제공한다.

