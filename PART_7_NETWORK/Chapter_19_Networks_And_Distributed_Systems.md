## What we gonna study

분산 시스템은 메모리 또는 클락을 공유하지 않는 프로세서의 집합이다. 대신에, 각 노드는 그것의 고유 로컬 메모리를 가진다. 노드는 가속 버스같은 다양한 네트워크를 통해서 다른 것과 소통한다. 분산된 시스템들은 더욱 관련이 있고, 너는 몇가지 분산된 서비스를 사용한다. 분산 시스템의 앱은 조직내에서 파일에 투명한 접근을 제공하는 것부터 대규모 클라우드 파일과 사진 저장소 서비스, 큰 데이터셋에서 사업 트렌드의 분석, 과학적 데이터의 평행적 프로세싱등이 있다. 실제로, 가장 일반적인 분산 시스템의 예시는 인터넷이다.

이 장에서, 우리는 일반적인 분산 시스템의 구조와 그들을 연결하는 네트워크를 알아보겠다. 우리는 또한 현재 분산 시스템 디자인의 타입과 역할에서의 차이를 대조하겠다. 마지막으로, 우리는 분산 파일 시스템에서의 기본 디자인과 어려운점을 보겠다.

## Objectives

- 네트워크, 분산 시스템의 장점을 설명한다.
- 분산 시스템을 연결하는 네트워크의 높은 레벨 개요를 제공하겠다.
- 오늘날 사용되는 분산시스템의 종류와 역할을 정의하겠다.
- 분산 파일 시스템의 디자인에 관련된 이슈를 논의하겠다.

## 19.1 분산 시스템의 장점

**distributed system**은 통신 네트워크에 의해서 연결된 loosely coupled nodes의 집합이다. 분산 시스템의 특정 노드의 관점에서, 나머지 노드와 그들의 각각의 리소스들은 원격이고, 반면에 그것의 리소스는 로컬이다. 분산 시스템의 노드는 사이즈와 함수에서 다양하다. 그들은 작은 마이크로 프로세서, pc, 큰 범용 컴퓨터 시스템을 포함할 수 있다. 이런 프로세서들은 그들이 언급하는 문맥에 따라 *processors, sites, hosts, machine*과 같이 다양한 이름으로 언급된다. 우리는 주로 머신의 위치를 말하기 위해서 *site*를 사용하고 사이트에서의 특정 시스템을 언급하기 위해서 *node*를 사용한다. 노드들은 *client-server*, *peer-to-peer*와 같은 상태로 존재할 수 있다. 일반적인 클라이언트-서버에서, one node at on site, the *server*는 다른 노드인 *client*가 사용하기를 원하는 리소스를 가지고 있다. peer-to-peer에서, 서버나 클라언트가 없다. 대신에, 노드들은 클라이언트와 서버로서 동등한 책임을 공유한다.

몇가지 사이트들은 통신 네트워크에 의해서 연결되어 있을때, 다양한 sites에 있는 유저들은 정보를 교환할 기회를 가진다. 낮은 레벨에서, **messages**들은 시스템 사이에서 전달되고, 3.4절에서 얘기한 단일 컴퓨터 메시지 시스템에서 프로세스간에 메시지가 교환되는 것과 비슷하다. 메시지 패싱이 주어지면, 단일 시스템에서 발견되는 모든 높은 레벨 기능은 분산 시스템을 포함하게 확장된다. 이런 함수는 파일 저장소, 앱의 실행, RPCs를 포함한다.

분산 시스템을 짓는 주요한 3가지 이유가 있다 : 리소스 공유, 연산 가속, 신뢰성. 이 절에서, 우리는 간단하게 이를 살펴보겠다.

### 19.1.1 리소스 공유

만약 많은 수의 다른 사이트들이 다른 것에 연결되면, 한 site에 있는 유저는 다른 곳에 가용한 리소스를 사용할 수 있다. 예를 들어서, site A의 유저는 site B에 있는 데이터베이스에 문의한다. 그 동안에, site B의 유저는 site A에 존재하는 파일에 접근한다. 일반적으로, 분산 시스템에서 **resource sharing** 원격 site에서 파일 공유, 분산 데이터 베이스에서의 데이터 처리, 원격 site의 파일 인쇄, 슈퍼 컴퓨터나 GPU같은 특화 하드웨어를 원격에서 사용하기 위한 메커니즘을 제공한다.

### 19.1.2 Computation SpeedUp

만약 특정 연산이 동시에 작동하는 서브 컴퓨테이션으로 분할될 수 있으면, 분산 시스템은 다양한 site사이에서 subcomputation을 분산하게 허용한다. subcomputation은 동시에 작동이 가능하고 그러므로 **computation speedup**을 제공한다. 이것은 근 데이터 셋의 처리를 진행할때 사용된다. 추가로, 만약 특정 site가 현재 요청이 과적되면, 그들의 일부가 덜 적재된 site로 이동하거나 재설정된다. 이런 행동을 **load balancing**이라고 부르고 분산 시스템 노드와 인터넷에서 제공되는 다른 서비스에서는 일반적이다.

### 19.1.3 Reliability

만약 분산 시스템에서 한개의 사이트가 실패하면, 나머지 사이트는 계속 작동하고, 시스템은 나은 신뢰성을 얻는다. 만약 시스템이 다중 대형 자율적인 설치로 구성되면, 그들중 하나의 실패는 나머지에게 영향을 주지 않는다. 만약 시스템이 몇몇 중요한 시스템 함수를 책임지는 다각화된 머신으로 구성되면, 한개의 실패는 전체 시스템을 멈추게한다. 일반적으로, 충분한 낭비와 함께라면, 시스템은 그것의 노드가 몇개 실패해도 작동을 지속한다.

노드 또는 site의 실패는 시스템에 의해서 발견되고 적절한 행동은 실패로부터 회복되는데 필요하다. 시스템은 반드시 그 사이트의 서비스를 사용하지 말아야한다. 추가로, 만약 실패한 사이트의 함수가 다른 사이트로 넘어가면, 시스템은 반드시 함수의 전달이 알맞게 일어났는지 확인해야한다. 마지막으로, 실패한 사이트가 회복되거나 수리되면, 그것을 시스템으로 부드럽게 다시 합병하는 메커니즘이 있어야한다.

## 19.2 Network Structure

오늘 날 사용하는 분산 시스템의 종류와 역할을 완벽히 이해하기 위해서, 우리는 그들을 연결하는 네트워크를 이해할 필요가 있다. 이 절은 기본적인 네트워킹 컨셉과 그들이 분산 시스템과 관련된 문제를 소개하는 네트워크 입문서로서 다루겠다. 장의 나머지는 분산 시스템을 특별히 다루겠다.

네트워크에는 두가지 타입이 있다 : **Local-area networks(LAB)**과 **Wide-area networks(WAN)**이다. 두개의 가장 큰 차이점은 그들이 지리적으로 분산된 방식이다. LAN은 작은 공간(단일 건물 또는 인접한 빌딩)에 분산된 호스트들로 구성되어있고, 반면에 WAN은 큰 공간(미국)에 부산된 시스템으로 구성되어 있다. 이런 차이가 통신 네트워크의 속도와 신뢰성에서의 주요한 변형을 내포하고, 그들은 분산 시스템 디자인에도 반영된다.

### 19.2.1 LAN

LAN은 메인프레임 컴퓨터 시스템의 대체로 1970년대 초기에 부상했다. 많은 기업체가, 각자의 앱을 가진 작은 컴퓨터들을 가지는 것 단일 큰 시스템을 가지는 것보다 경제적이었다. 각 작은 컴퓨터가 부속 디바이스의 전체 보완을 필요로했고, 몇가지 데이터 공유가 단일 기업체에서 공유되어야했다. 그것은 이런 작은 시스템을 네트워크로 연결하는 자연스러운 단계이다.

LANs 말했듯이, 작은 지리적 영역에 디자인되고 그들은 오프시 또는 가정 환경에서 쓰였다. 이런 시스템에서의 모든 사이트는 다른 것과 가까웠고, 그래서 통신 링크는 빠른 속도와 wan에 비해서 적은 에러를 가졌다.

일반적인 LAN은 다른 컴퓨터(워크스테이션, 서버, 랩탑, 태블릿, 스마트폰)를 포함하고, 다양한 공유 부속 디바이스(프린터, 저장소행렬), 하나 또는 이상의 **router**를 포함한다. 이더넷과 **WiFi**는 LAN을 건설하느데 사용되었다. *Wireless access points*가 LAN에 디바이스를 무선으로 연결하고 그들은 그들자체로 라우터가 되거나 안되었다.

이더넷 네트워크들은 일반적으로 컴퓨터와 부속기기가 이동이 불가능한 사업과 조직에서 사용되었다. 이런 네트워크들은 *coaxial, twisted pair, fiber optic*케이블을 시그널을 보내기 위해서 사용한다. 이더넷 네트워크는 중앙 컨트롤러가 없는데, 왜냐하면 그것은 다중 엑세스 버스이고 그래서 새로운 호스트들은 네트워크에 쉽게 추가될 수 있다. 이더넷 프로토콜은 IEEE 802.3 표준에 의해서 정의된다. twisted-pair 케이블링을 사용하는 일반적인 이더넷 속도는 10Mbps~10Gbps이고 다른 케이블링은 100Gbps이다.

와이파이는 이제 유비쿼터스이고 전통적인 이더넷 네트워크를 보충하거나 그 자체로 존재한다. 특히, 와이파이는 우리가 네트워크를 물리적 케이블 없이 건설하게 허용한다. 각 호스트는 네트워크에 참석하는데 사용하는 무선 전송기와 수신기를 가진다. 와이파이는 IEEE 802.11 표준으로 정의된다. 무선 네트워크는 가정과 사업에서 유망하다. 와이파이 속도는 11Mbps에서 400Mbps로 다양하다.

IEEE 802.3과 802.11 표준은 지속적으로 진화중이다. 다양한 표준과 속도에 관한 최근의 정보에서는 장의 마지막에 살펴보겠다.

### 19.2.2 WAN

WAN은 1960년 후반에 부상했고, 사이트 사이에서 하드웨어와 소프트웨어가 넓은 범위의 유저에 의해서 간편하고 경제적으로 공유되는 것을 허용하는 효율적인 소통을 제공하기 위한 학술적인 연구 프로젝트로서 발전했다. 첫번째로 디자인되고 개발된 WAN은 ARPANET이다. 1968년에 시작해서, ARPANET은 4가지 사이트 실험적인 네트워크에서 WWW인 수백만개의 컴퓨터 시스템을 구성하는 **Internet**으로 자랐다.

WAN의 사이트는 물리적으로 넓은 장소에 분산되어있다. 일반적인 링크들은 전화선, leased llines, optical cable, microwave link, radio waves, satellite channel이다. 이런 통신 링크들은 다른 라우터와 네트워크의 교통을 지시하고 다양한 사이트 사이에서 정보를 전송하는 라우터에 의해서 제어된다.

예를 들어서, 인터넷 WAN은 멀리 떨어진 호스트가 다른 것과 통신하게 한다. 호스트 컴퓨터들은 일반적으로 통신하는 것과 속도, CPU 종류, 운영체제 등이 다르다. 호스트들은 LAN위에 있고, 지역 네트워크를 통해 인터넷에 연결되어있다. 지역 네트워크들은 라우터와 함께 worldwide network에 연결되어 있다. 거주자들은 전화, 케이블 또는 거주자를 중앙 서비스와 연결하는 라우터가 설치된 특화된 인터넷 서비스 제공자와 연결되어있다. 물론, 인터넷 사이에는 WAN이 있다. 회사는 그것의 사적인 WAN을 증가된 보안, 성능, 신뢰성을 위해서 만들 수 있다.

WANs들은 일반적으로 LANs보다 느린데, 비록 주요한 도시를 연결하는 근간 WAN 연결은 광학 케이블을 통해서 빠른 속도의 전송속도를 가질 수 있다. 실제로, 많은 근간 제공자들은 40Gbps 또는 100Gbps의 광학 케이블을 가진다.(그것은 보통 로컬 **Internet Service Providers(ISPs)**로부터의 링크이다.) 그러나, WAN 링크들은 빠른 속도를 원하는 수요가 늘어나면서 빠른 기술로 발전하고 있다.

자주, WANs와 LANs는 연결되어있고, 그것은 어떤게 시작이고 어떤게 끝인지 말하기 어렵다. 핸드폰 데이터 네트워크를 고려하겠다. 핸드폰들은 음성과 데이터 통신에 사용된다. 주어진 영역에 있는 핸드폰은 수신기와 송신기를 가진 무선기지국에서 나오는 라디오파를 통해서 연결한다. 네트워크의 부분은 셀폰이 다른 것과 통신하지 않는다는 것을 제외하면 LAN과 비슷하다. (비록 두 사람이 데이터를 주고받거나 통화해도 같은 타워에 연결되어있다고 한다.) 오히려, 기지국이 다른 기지국과 타워 통신을 랜드 라인 또는 다른 통신 매체에 연결하고 그들의 목적지로 라우트하는 허브에 연결되어있다. 이 네트워크의 부분은 WAN과 비슷하다. 한번 적절한 타워가 패킷을 받으면, 그것은 송신기를 이용해서 정확한 수신인이 있는 곳으로 보낸다.

## 19.3 Communication Structure

이제 우리는 네트워킹의 물리적인 측면을 살펴보고, 우리는 내부적인 일을 보겠다.

### 19.3.1 Naming and Name Resolution

네트워크 통신에서 첫번째 이슈는 네트워크에서의 시스템 이름을 포함한다. site A의 프로세스는 사이트 B의 프로세스와 정보를 교환하고, 각각은 반드시 서로를 명시할 수 있어야 한다. 컴퓨터 시스템 내에서, 각 프로세스는 프로세스 식별자가 있고 메시지들은 프로세스 식별자로 지정된다. 네트워크 시스템이 메모리를 공유하지 않아서, 시스템의 호스트는 다른 호스트들의 프로세스에 대한 정보가 전혀 없다.

이 문제를 해결하기 위해서, 원격 시스템의 프로세스들은 <host name, identifier> 쌍으로 구별되고, **host name**은 네트워크 안의 유일한 이름이고 **identifier**는 호스트 안의 프로세스 식별자 또는 유일한 숫자이다. 호스트 이름은 일반적으로 알파벳+숫자 식별자여서 유저가 명시하기 쉽다. 예를 들어서, site A는 *program, student, faculty, cs*라는 이름을 가진 호스트를 가진다. 호스트 이름 *program*은 호스트 주소 `128.148.31.100`보다 훨씬 기억하기 쉽다.

이름들은 사람이 사용하기 편하지만, 컴퓨터는 속도와 단순성 때문에 숫자를 선호한다. 이 이유때문에, 호스트이름을 도착 시스템에서 네트워킹 하드웨어를 설명할 **host-id**로 **resolve**할 메커니즘이 필요하다. 이 메커니즘은 컴파일, 링킹, 로딩, 실행에서 나타나는(9장) name-to-address binding과 비슷하다. 호스트 이름의 경우에는, 두가지 가능성이 존재한다. 첫번쨰로, 모든 호스트는 네트워크에서 도달할 수 있는 이름과 숫자 주소를 포함한 데이터 파일을 가진다.(컴파일 시의 binding과 비슷하다.) 이 모델과의 문제는 네트워크로부터 호스트를 추가하고 삭제하는 것은 모든 호스트에서 데이터 파일을 업데이트할 필요가 있다는 것이다. 실제로, ARPRANET의 초기에는, 모든 시스템을 주기적으로 복사하는 고전적인 호스트 파일이 있었다. 네트워크가 성장하자, 이 메서드는 지지할 수 없는 메서드이다.

네트워크 상에서 시스템 사이에 정보를 배분하는 대안이 있다. 네트워크는 정보를 분산하고 되찾기 위해서 프로토콜을 사용했다. 이 구조는 excution time binding과 비슷하다. 인터넷은 호스트 이름 해결책으로 **domain-name system(DNS)**를 사용했다.

DNS는 호스트의 이름 구조 뿐만 아니라 name-to-address resolution을 명시했다. 인터넷의 호스트는 논리적으로 IP주소라고 알려진 multipart names로 보내졌다. IP 주소의 일부는 가장 상세한 것에서 일반적인것으로 필드를 분할하는 주기와 함께 진행했다. 예를 들어서, `eric.cs.yale.edu`는 탑 레벨 도메인 `edu`안의 예일 대의 컴퓨터 공학 부서의 호스트 eric이다.(다른 탑레벨 도메인은, com for commercial sites and org for organizations, 시스템에 연결된 각나라의 주소일 수 있다.) 일반적으로, 시스템은 호스트 이름 구성요소를 역순으로 검사해서 주소를 resolve한다. 각 컴포넌트는 **name server**를 가지고- 간단히 시스템의 프로세스- 이름을 수락하고 이름에 책임있는 네임 서버의 주소를 리턴한다. 마지막 단계로, 호스트를 위한 네임 서버가 연결되고, 호스트-id가 리턴된다. 예를 들어서 `eric.cs.yale.edu`와 소통하려는 시스템 A의 프로세스에 의해서 만들어진 요청은 다음의 과정을 밟는다.

1. 시스템 A의 시스템 라이브러리 또는 커널이 `edu` 도메인을 위해서 네임 서버를 요청하고, `yale.edu`의 네임 서버의 주소를 물어본다. `edu` 도메인 네임 서버는 반드시 알고 있는 주소여야하고, 그래서 그것은 쿼리될 수 있다.
2. `edu` 네임 서버가 `yale.edu` 네임서버가 거주하는 호스트의 주소를 리턴한다.
3. 시스템 A는 그러면 주소에서의 네임서버를 쿼리하고 `cs.yale.edu`를 물어본다.
4. 주소가 리턴된다. 이제, 마지막으로 `eric.cs.yale.edu`를 위한 주소 요청이 호스트를 위한 인터넷 주소 host-id를 리턴한다.(예를 들어서, `128.148.31.100`)

이 프로토콜은 비효율적이어 보이지만, 개인 호스트들은 이미 해결한 IP 주소를 캐시해서 프로세스의 속도를 증가시킨다.(물론, 이런 캐시의 컨텐츠는 네임 서버가 이동하거나 그것의 주소가 바뀌면 반드시 다시 채워야한다.) 실제로, 프로토콜은 그것이 여러번 최적화되고 많은 세이프가드가 추가될 정도로 너무 중요하다. 만약 초기 `edu` 네임서버가 충돌했다면 무슨일이 일어날지 고려해보아라. `edu`라는 이름을 가진 모든 호스트들은 resolve될수 없고, 그들은 닿을수 없어진다. 해결책은 2차 선을 두는 것인데, 백업 네임서버가 초기 서버의 컨텐츠를 복제해둔다.

dns가 소개되기 전에, 인터넷의 호스트는 네트워크에서 각 호스트의 이름과 주소를 포함한 파일의 카피를 가질 필요가 있었다. 이 파일에 모든 변화는 한가지 사이트(SRI-NIC)에 등록되었고, 주기적으로 모든 호스트들은 SRI-NIC로 부터 파일을 업데이트해서 새로운 시스템에 접근하고 주소가 바뀐 호스트들을 찾아야했다. DNS에서는, 각 이름 서버 사이트는 그 도메인에 대해서 호스트 정보를 업데이트할 책임만 있다. 예를 들어서, 예일 대학교에서의 호스트 변화들은 yale.edu 네임 서버에게 책임이 있고 어디에도 보고되지 않는다. DNS 룩업은 그들이 yale.edu에 직접 연결되기 떄문에 자동으로 업데이트된 정보를 되찾는다. 도메인들은 호스트 이름과 호스트아이디 변화에 책임을 분산하는 독자적인 서브 도메인을 가진다.

자바는 IP 이름을 IP 주소로 매핑하는 필수 API를 제공한다. `InetAddress`는 IP 이름 또는 주소를 대표하는 자바 클래스이다. `InetAddress` 클래스의 정적 메서드 `getByName()`은 IP 이름의 대표를 문자열 대표에 전달한다. 그리고 그것은 상응하는 `InetAddress`를 리턴한다. 프로그램은 그리고 `getHostAddress()`메서드를 실행하고, 내부적으로 DNS를 이용해서 원하는 호스트의 IP 주소를 룩업한다.

일반적으로, 운영체제는 그것의 프로세스로부터 <host name, identifier>를 목적지로한 메시지를 수락하고 적절한 호스트에게 메시지를 전달하는 책임이 있다. 목적지 호스트의 커널은 식별자로 이름지어진 프로세스에 메시지를 전송할 책임이 있다.(이것은 19.3.4에서 더 다루겠다.)

### 19.3.2 Communication Protocols

우리가 통신 네트워크를 디자인할 때, 우리는 반드시 잠재적으로 느리고 에러가 자주 나오는 환경에서 조직화된 비동기 명령어의 내부적 복잡성을 처리해야한다. 추가로, 네트워크의 시스템은 호스트 이름을 결정하고, 네트워크에 호스트를 위치시키고, 연결을 성사시키는 프로토콜의 집합을 수락해야한다. 우리는 다중 레이어로 문제를 분할함으로서 디자인 문제를 단순화시켰다. 시스템의 각 레이어는 다른 시스템에서의 같은 레이어와 통신한다. 일반적으로, 각 레이어는 그것의 고유 프로토콜을 가지고, 통신은 특정 프로토콜을 이용해서 피어 레이어 사이에서 자리한다. 프로토콜들은 하드웨어 또는 소프트웨어에 구현된다. 예를 들어서, 두 컴퓨터 사이에서의 논리적 통신을 가진다.

국제 표준 조직은 Open System Interconnection(OSI) 모델을 네트워킹의 다양한 계층을 표현하기 위해서 만들었다. 이런 레이어들이 실제로는 구현되지는 않지만, 그들은 어떻게 네트워킹이 논리적으로 작동하는지 이해하는데에 유용하고, 우리는 그들을 소개하겠다.

- Layer 1 : Physical layer. 물리 계층은 비트스트림의 물리적 통신의 기계/전자적인 상세를 핸들링한다. 물리 계층에서, 통신 시스템은 반드시 0과 1에서만 허용해야하고, 그래서 데이터가 전기적인 신호의 스트림으로 보내지면, 수락자는 데이터를 바이너리 데이터로 적절히 해석할 수 있다. 이 레이어는 네트워킹 디바이스의 하드웨어로 구현된다. 이것은 비트를 전송하는 역할을 한다.
- Layer 2 : Data-link layer. 데이터 링크 레이어는 물리 계층에서 일어난 에러 감지와 회복을 포함한 *frames* 또는 고정된 길이의 패킷을 다룬다. 그것은 물리적 주소사이에서 프레임을 보낸다.
- Layer 3 : Network layer. 네트워크 레이어는 메시지를 패킷으로 쪼개는 역할을 하고, 논리적 주소와 떠나는 패킷의 주소 통신를 다루고, 들어오는 패킷의 주소를 디코딩하고 라우팅 정보를 변하는 로드 레벨에 따라서 적절히 유지하는 네트워크에서 라우팅 패킷을 포함한다. 라우터는 이 계층에서 일한다.
- Layer 4 : Transport layer. 전송 계층은 노드 사이의 메시지의 전달, 패킷 순서를 유지, 혼잡을 피하기 위해서 플로우를 통제하는 역할을 한다.
- Layer 5 : Session layer. 세션 계층은 세션 또는 프로세스-프로세스 통신 프로토콜을 구현하는 역할을 한다.
- Layer 6 : Presentation layer. 표현 계층은 네트워크의 다양한 사이트 사이에서의 포맷 차이를 해결하는 역할을 한다. 하프 듀플렉스와 풀 듀플렉스 모드와 캐릭터 컨버전을 포함한다.
- Layer 7 : Application layer. 앱 계층은 유저와 직접 소통하는 역할을 한다. 이 레이어는 파일 전송, 원격 로그인 프로토콜, 전자 메일, 분산 데이터를 위한 구조를 처리한다.

**OSI protocol stack**은 프로토콜의 집합이고 데이터의 물리적 흐름을 나타낸다. 말했듯이, 논리적으로 프로토콜 스택의 각 레이어는 다른 시스템의 같은 계층과 통신한다. 그러나 물리적으로, 메시지는 앱 계층에서 시작하고 낮은 레벨에 전달된다. 각 레이어는 메시지를 수정하고 받는 쪽에서 같은 계층을 위한 메시지 헤더 데이터를 포함한다. 궁극적으로, 메시지는 데이터 네트워크 계층에 도달하고 하나 또는 이상의 패킷으로 전송된다. 데이터 링크 계층의 타겟 시스템은 이런 데이터를 받고 메시지는 프로토콜 스택을 통해서 위로 이동한다. 그것은 분석되고, 수정되고, 쪼개진다. 그것은 마침내 앱 계층으로 도달해서 프로세스에 수락되서 사용된다.

OSI 모델은 네트워크 프로토콜에서 생긴 최근의 연구를 정규화했지만 1970년대 후반에 개발되었고 현재는 널리 쓰이지 않는다. 아마도 대부분의 널리 채택된 플토콜 스택은 TCP/IP 모델(*Internet mode*이라고도 불림)인데, 가상적으로 모든 인터넷 사이트에 채택되었다. TCP/IP 프로토콜 스택은 OSI 모델보다 적은 계층을 가진다. 이론적으로, 그것이 각 레이어의 몇가지 함수를 합치기 때문에, 그것은 구현하기는 어렵지만 OSI 네트워킹에는 더욱 효율적이다.

TCP/IP 앱 계층은 HTTP, FTP, SSH, DNS, SMTP를 포함한 인터넷에서 널리 쓰이는 몇가지 프로토콜을 식별한다. 전송 계층은 신뢰할 수 없고, 연결이 없는 **user datagram protocol(UDP)**와 신뢰할 수 있고, 연결에 기반한 **transmission control protocol(TCP)**로 구별한다. **Internte protocol(IP)**는 인터넷을 통해서 IP **datagrams** 또는 패킷을 라우팅하는 역할을 한다. TCP/IP 모델은 링크 또는 물리 레이어를 정식으로 구별하지 않고, TCP/IP 트래픽이 어느 물리 네트워크로든지 달려가게 허용한다. 19.3.3절에서, 우리는 이더넷 네트워크를 건너서 작동하는 TCP/IP 모델을 고려하겠다.

보안은 반드시 어느 현대 통신 프로토콜에서 디자인되고 구현되어야한다. 강한 인증과 암호화는 보안 통신을 위해서 필요하다. 강한 인증은 보낸이와 받는이의 통신을 누가 어떻게 그들이 가정한대로 하는지 확신한다. 암호화는 감청으로부터 통신의 컨텐츠를 보호한다. 약한 인증과 정확한 텍스트 통신이 여전히 일반적이지만, 여러가지 이유가 있다. 대부분의 일반적인 프로토콜이 디자인되면, 보안은 성능, 단순성 효율성보다 덜중요하게 여겨진다. 이런 유산은 여전히 남아있고, 존재하는 인프라스트럭쳐에 보안을 더하는 것은 어렵고 복잡하다고 증명되었다.

강한 인증은 멀티스텝 handshake 프로토콜 또는 인증 기기를 필요로하고, 프로토콜에 복잡성을 추가한다. 보안이 필요해짐에 따라서, 현대 CPU들은 효율적으로 보안을 수행하고, 자주 암호학 가속 명령어를 포함해서 그래서 시스템 성능은 깍이지 않는다. 원거리 통신은 엔드포인트를 인증하고 가상 개인 네트워크의 패킷의 스트림을 암호화함으로서 보안을 만든다. LAN 통신은 대부분의 사이트에서 암호화되지 않은채 남아있지만, NFS 버전 4와 같은 프로토콜은 강한 native 인증과 암호화를 포함한다. 이들은 LAN 보안마저도 향상시킨다.

### 19.3.3 TCP/IP 예제.

다음으로, 우리는 인터넷에서 TCP/IP 프로토콜 스택에 따라서 이름 해석을 지시하고 그것의 명령어를 평가하겠다. 우리는 다른 이더넷 네트워크사이에서 패킷을 전송하는데 필요한 프로세싱을 고려하겠다. 우리는 우리의 설명을 IPV4 프로토콜에 기반해서하겠다.

TCP/IP 네트워크에서, 모든 호스트는 이름과 관련된 IP 주소를 가진다. 두 스트링은 반드시 유일해야한다. 그래서 이름 공간이 관리되고, 그들은 분리된다. 앞서 설명했듯이, 이름은 계층적이고, 호스트이름을 설명하고, 호스트가 연관된 조직을 말해준다. 호스트 아이디는 네트워크 번호와 호스트 번호로 분리된다. 스플릿의 범위는 네트워크의 사이즈에 따라서 다양하다. 한번 인터넷 관리자가 네트워크 번호를 할당하면, 그 숫자의 사이트는 호스트 아이디를 할당하기위해서 해제된다.

보내는 시스템은 그것의 라우팅 테이블을 그것의 길로 프레임을 보내기 위해서 체크한다. 이 라우팅 테이블은 시스템 관리자에 의해서 수동으로 설정되거나 **Border Gateway Protocol(BGP)**같은 라우팅 프로토콜에 의해서 위치된다. 라우터는 호스트 id의 네트워크 파트를 패킷을 그것의 소스 네트워크에서 도착지 네트워크로 전송하기 위해서 사용한다. 목적지 시스템은 그러면 패킷을 받는다. 패킷은 완벽한 메시지가 될 수 있거나, 그것은 목적 프로세스로의 전송을 위해서 메시지가 뭉쳐지고 TCP/UDP(transport) 게층으로 전달되기전에 메시지의 일부일 수도 있다.

네트워크에서, 어떻게 패킷이 보낸이(호스트 또는 라우터)에서 받는이로 갈까? 모든 이더넷 디바이스는 유일한 바이트 넘버를 가지고 **medium access control(MAC) address**라고 불리고, 주소를 위해서 할당된다. LAN의 두 디바이스는 이 이름으로 각자와 통신한다. 만약 시스템이 다른 시스템에 데이터를 보낼려면, 네트워킹 소프트웨어는 목적 시스템의 IP 주소를 포함한 **address resolution protocol(ARP)** 패킷을 생성한다.

브로드캐스트는 모든 호스트가 받고 프로세스해야할 패킷을 시그널하는 특별한 네트워크 주소를 사용한다. 브로드캐스트는 다른 네트워크 사이에서 라우터에 의해서 재전송되지 않고, 그래서 오직 로컬 네트워크 시스템이 받는다. IP 주소가 ARP 요청의 IP주소와 매치하는 시스템은 반응하고 그것의 MAC 주소를 쿼리를 시작한 시스템에 돌려 보낸다. 효율성을 위해서, 호스트는 IP-MAC 주소 페어를 내부 테이블에 캐시한다. 캐시 엔트리가 오래되면, 그래서 만약 시스템의 접근이 주어진 시간에 필요하지 않으면 엔트리가 캐시에서 마침내 제거된다. 이 방법으로, 네트워크에서 제거된 호스트들은 마침내 잊혀진다. 추가된 성능으로, 자주 사용되는 호스트를 위한 ARP 엔트리는 ARP 엔트리에 고정될 수 있다.

한번 이더넷 디바이스가 그것의 호스트id와 주소를 선언하면, 통신은 시작한다. 프로세스는 통신할 호스트의 이름을 명시할 수 있다. 네트워킹 소프트웨어는 DNS 룩업 또는 번역이 수동으로 저장된 로컬 호스트 파일의 엔트리을 사용해서 이름을 가져가고 타켓의 IP 주소를 결정한다. 메시지는 앱 계층에서 전달되고 소프트웨어 계층을 통해서 하드웨어 계층으로 간다. 하드웨어 계층에서, 패킷은 그것의 시작에 이더넷 주소를 가진다. 트레일러는 패킷의 끝을 가르키고 패킷 데미지의 발견을 위한 **checksum**을 포함한다. 패킷은 이더넷 디바이스에 의해서 네트워크에 위치한다. 패킷의 데이터 섹션은 원본 메시지의 데이터의 일부 또는 전체를 포함하지만, 그것은 메일을 구성하는 높은 레벨의 헤더의 일부또한 포함한다. 다른 말로는, 원본 메시지의 모든 파트는 소스에서 목적지로 보내지고, 802.3 계층(data-link layer)위의 헤더는 이더넷 패킷안의 데이터로 포함된다.

만약 목적지가 소스와 같은 로컬 네트워크에 있으면, 시스템은 그것의 ARP 캐시를 살펴보고, 호스트의 이더넷 주소를 찾고, 와이어에 패킷을 담는다. 목적지 이더넷 디바이스는 패킷의 그것의 주소를 보고 패킷을 읽고, 프로토콜 스택위로 전달한다.

만약 목적지 시스템이 소스와 다른 네트워크에 위치하면, 소스 시스템은 그것의 네트워크에서 적절한 라우터를 찾고 그곳으로 패킷을 보낸다. 라우터는 그러면 WAN을 따라서 패킷을 그것이 목적지 네트워크에 도달할떄까지 보낸다. 목적지 네트워크와 연결된 라우터는 그것의 ARP 캐시를 체크하고, 목적지의 이더넷 번호를 찾고, 패킷을 호스트에게 보낸다. 이런 전송을 통해서, 데이터 링크 레이어 헤더는 다음 라우터의 사용된 체인안의 이더넷 주소에 따라서 변할 수 있다. 그러나 패킷의 다른 헤더들은 패킷이 도달할떄까지 같게 유지되고 프로토콜 스택에 의해서 프로세스되고 마침내 커널에 의해서 받는 프로세스에 전달된다.

### 19.3.4 Transport Protocols UDP and TCP

한번 특정 IP 주소의 호스트가 패킷을 받으면, 그것은 반드시 그것을 정확한 대기 프로세스에 전달해야한다. 전송 프로토콜 TCP와 UDP는 **port number**의 사용을 통해서 받는(보내는) 프로세스를 식별한다. 그러므로 단일 IP 주소를 가진 호스트는 작동하고 각 프로세스가 다른 포트넘버를 명시하는 동안 패킷을 기다리는 다중 서버 프로세스를 가진다.  기본으로, 많은 서비스가 *well-known* 포트 넘버를 사용한다. 예를 들어서, FTP(21), SSH(22), SMTP(25), HTTP(80)을 포함한다. 예를 들어서, 만약 니가 브라우저를 통해서 "http" 웹사이트에 접속하기를 원하면, 너의 브라우저는 자동으로 TCP 전송 헤더의 포트 숫자를 80으로 사용함으로서 서버의 포트 80에 접속하려고 시도할 것이다. 잘 알려진 포트의 확장된 리스트에서, 너의 리눅스 또는 유닉스에 로그인하고 파일 `/etc/services`를 확인해라.

전송 계층은 네트워크 패킷을 실행하는 프로세스에 연결하는 것보다 많은 일을 한다. 그것은 또한, 원한다면 네트워크 패킷 스트림에 신뢰성을 추가할 수 있다. 설명하기 위해서, 우리는 UDP와 TCP 전송 프로토콜의 행동을 강조하겠다.

#### 19.3.4.1 User Datagram Protocol

전송 프로토콜 UDP는 그것이 포트넘버의 추가와 함께 IP의 말라빠진 확장이기 때문에 *unreliable*하다. 실제로 UDP 헤더는 단순하고 오직 4가지 필드를 포함한다. 소스 포트 넘버, 목적지 포트 넘버, 길이, 체크섬이다. 패킷은 UDP를 이용해서 목적지에 빠르게 보내질 수 있다. 그러나, 네트워크 스택의 낮은 계층에서의 배달을 보장할 수 없기 때문에, 패킷을 잃을 수도 있다. 패킷들은 또한 수신자에게 순서없이 갈 수 있다. 그것은 에러를 찾고 수정하는것을 앱에게 맞긴다.

UDP 패킷이 데이터를 손실하는 시나리오를 보겠다. 이 프로토콜은 *connectionless* 프로토콜이라고 불리는데 왜냐하면 상태를 설정하는 전송의 시작에 어떠한 연결 셋업이 없다. 클라이언트는 오직 데이터를 보내기 시작한다. 비슷하게, 연결 teardown도 없다.

클라이언트가 서버에 몇가지 정보를 요청하는 것을 시작한다. 서버는 4가지 데이터그램 또는 패킷을 클라이언트에게 보낸다. 불행히도, 패킷의 하나는 라우터의 오작동으로 손실되었다. 클라이언트는 3가지 패킷으로 만들거나 잃어버린 패킷을 요청하는 앱에 구현된 로직을 사용할 것이다. 그러므로, 우리는 만약 네트워크에 의해서 조절되는 신뢰성을 보장하는 것을 원한다면 다른 전송 프로토콜을 사용해야한다.

#### 19.3.4.2 Transmission Control Protocol

TCP는 *reliable*하고 *connection oriented*한 전송 프로토콜이다. 다른 호스트에서 받고 보내는 프로세스를 구별하는 포트넘버를 명시해서, TCP는 한 호스트의 보내는 프로세스가 네트워크를 통해서 다른 호스트의 받는 프로세스에 순차적으로, 방해받지 않는 *byte stream*을 보내는 것을 허용하는 추상화를 제공한다. 그것은 다음의 메커니즘으로 이를 해결한다.

- 호스트가 패킷을 보내면, 받는이는 반드시 **acknowledgment packet**을 보낸이가 패킷이 받아졌다고 알리기 위해서 보내야한다. 만약 ACK를 타이머가 만료되기전에 받지 못하면, 보낸이는 패킷을 다시 보낸다.
- TCP는 모든 패킷의 TCP 헤더에 **sequence numbers**를 제공한다. 이런 숫자들은 받는이가 (1) 요청한 프로세스에 준비된 데이터를 보내기전에 패킷을 순서대로 두고 (2) 바이트 스트림으로 부터 잃은 패킷에 대해서 지각하는 것을 허용한다.
- TCP 연결은 보낸이와 받는이(*three-way handshake*라고 불림) 사이의 컨트롤 패킷의 연속으로 시작되고 연결을 끊는 책임을 가진 컨트롤 패킷과 함께 우아하게 닫힌다.

TCP가 정보를 교환하는 방법을 보겠다.(연결 셋업과 끊기는 생략한다.) 연결이 생기면, 클라이언트는 요청 패킷을 서버에 시퀀스 넘버 904로 보낸다. UDP 예제의 서버와 달리, 서버는 반드시 ACK를 클라이언트에게 보낸다. 다음으로, 서버는 그것의 ACK 패킷을 클라이언트에게 보낸다. 다음으로, 서버는 다른 시퀀스 넘버로 시작하는 데이터 패킷의 스트림을 보내기 시작한다. 클라이언트는 받은 데이터 패킷에 대한 ACK 패킷을 보낸다. 불행히도, 시퀀스 127의 데이터 패킷을 잃었고, 클라이언트로부터 ACK 패킷이 돌아오지 않는다. 보낸이는 ACK 패킷을 기다리는 시간이 만료되고, 그래서 데이터 패킷 127을 다시보낸다. 시간이 흘러서, 서버는 데이터 패킷 128을 보낸지만, ACK를 잃는다. 서버가 ACK를 받지 못했기에 패킷 128은 다시 보내진다. 클라이언트는 중복된 패킷을 받는다. 클라이언트는 시퀀스 넘버를 통해서 받은 패킷이 이미 받은 것을 알고 버린다. 그러나, 그것은 서버에 ACK를 다시 보내서 서버가 진행하게 한다.

실제 TCP 상세에서, ACK는 각각에 필요하지 않는다. 대신에, 수신자는 패킷의 시리즈를 ACK하기 위해서 *cumulative ACK*를 보낸다. 서버는 또한 ACK를 기다리기전에 수많은 데이터 패킷을 보내고, 네트워크 산출량의 강점을 얻는다.

TCP는 또한 *flow control*과 *congestion control*을 이용해서 패킷의 플로우를 제제한다. **Flow control**은 수신인의 용량을 초과하지 않도록 보낸이를 예방한다. 예를 들어서, 수신인이 느린 연결을 가지거나 느린 하드웨어를 가졌다. 플로우 컨트롤은 보낸이가 느리게하거나 빠르게하라고 경고하는 리시버의 ACK 패킷으로 리턴된다. **Congestion control**은 보낸이와 받는이의 네트워크(일반적으로 라우터)의 상태를 근사화하게한다.  만약 라우터가 패킷에 의해서 꽉차면, 그것은 그들을 떨어뜨리는 경향이 있다. 패킷을 떨어뜨리면 ACK 타임아웃이 생기고, 더 많은 패킷이 네트워크에 포화된다. 이런 컨디션을 예방하기 위해서, 센더는 얼마나 많은 패킷이 인지되지 않았는지 아는 것으로서 드랍된 패킷으로 연결을 모니터링한다. 만약 너무 많은 드랍된 패킷이 있으면, 보낸이는 그들을 보내는 속도를 늦춘다. 이것은 TCP 연결이 같은 시간에 생기는 다른 연결과 공평하게 보장하는 것을 돕는다.

TCP같은 reliable 전송 프로토콜을 실행함으로서, 분산 시스템은 패킷의 손실이나 잘못된 순서를 교정할 로직이 필요하지 않다. 그러나, TCP는 UDP보다 느리다.

## 19.4 Network and Distributed Operating Systems

이 절에서, 우리는 두가지 종류의 네트워크 기반 운영체제를 설명하겠다.: 네트워크 운영체제와 분산 운영체제이다. 네트워크 운영체제는 구현하기는 간편하지만 많은 기능을 제공하는 분산 운영체제보다 유저가 접근하고 사용하기 더 어렵다.

### 19.4.1 네트워크 운영체제

**network operating system**은 유저가 적절한 원격 머신에 로그인하거나 그들의 머신으로 원격 머신에 있는 데이터를 전송함으로서 원격 리소스에 접근할 수 있는 환경을 제공한다. 최근에, 모든 범용 운영체제, 심지어는 안드로이드와 iOS같은 임베디드 운영체제도 네트워크 운영체제이다. 

#### 19.4.1.1 Remote Logini

네트워크 운영체제의 중요한 기능은 유저가 원격에서 로그인하게 허락하는 것이다. 인터넷은 이 목적을 위해서 `ssh`를 제공한다. 설명하자면, 웨스트민스터 대학의 유저가 예일 대학교에 위치한 컴퓨터, `kristen.cs.yale.edu`에서 연산하기를 바란다. 이걸 하기 위해서, 유저는 반드시 머신에 적절한 계정을 가지고 있어야한다. 원격에서 로그인하기 위해서, 유저는 다음 커맨드를 발행한다.

`ssh kristen.cs.yale.edu`

이 커맨드는 웨스트민스터 대학의 로컬 컴퓨터와 `kristen.cs.yale.edu` 컴퓨터사이를 암호화된 소켓 연결의 형식으로 얻는다. 이 연결이 성립된 이후에, 네트워킹 소프트웨어는 유저가 입력한 모든 문자가 `kristen.cs.yale.edu`의 프로세스로 보내지고 프로세스로부터의 모든 아웃풋이 유저에게 다시 보내지는 transparent, bidirectional 링크를 생성한다. 한번 정확한 정보를 얻으면, 프로세스는 원격 머신을 로컬 머신처럼 연산가능한 유저에게 프록시처럼 행동한다.

#### 19.4.1.2 Remote File Transfer

네트워크 운영체제의 다른 주요한 함수는 한 머신에서 다른 머신으로 **remote file transfer**하는 메커니즘을 제공하는 것이다. 이런 환경에서, 각 컴퓨터는 그것의 로컬 파일 시스템을 유지한다. 만약 사이트(Kurt at albion.edu)에서의 유저가 다른 컴퓨터(colby.edu)의 베카에 의해서 소유된 파일에 접근을 원하면, 파일은 반드시 Colby에 있는 컴퓨터에서 Albion에 있는 컴퓨터로 명시적으로 카피되어야한다. 이 통신은 다른 유저가 파일을 전송하기를 원하면 이런 커맨드의 집합을 발행해야하므로 단방향이고 개인적이다.

인터넷은 File transfer protocol(FTP)와 더욱 개인적인 secure file transfer protocol(SFTP)같은 전송을 위한 메커니즘을 제공한다. `wesleyan.edu`에 있는 카를라라는 유저가 `kzoo.edu`에 있는 오웬에 의해서 소유된 파일을 카피하기를 원한다고 가정하겠다. 유저는 반드시 `sftp owen@kzoo.edu`라는 프로그램을 실행함으로서 `sftp` 프로그램을 깨워야한다. 프로그램은 그리고 유저에게 로그인 이름과 패스워드를 묻는다. 한번 정확한 정보를 받으면, 유저는 파일을 업로드하고 다운로드하고, 파일 시스템 구조를 탐색하기 위해서 다음과 같은 커맨드를 사용할 수 있다.

- get - 원격 머신에서 로컬 머신으로 파일을 전송한다.
- put - 로컬 머신에서 원격 머신으로 파일을 전송한다.
- ls or dir - 리모트 머신의 현재 디렉토리의 파일을 리스트한다.
- cd - 리모트 머신의 현재 디렉토리를 변경한다.

#### 19.4.1.3 Cloud Storage

기본적인 클라우드 베이스 저장소 앱은 유저가 FTP를 이용해서 파일을 전송하게 허용한다. 유저는 클라우드 서버에 파일을 업로드하고, 로컬 컴퓨터에 파일을 다운로드하고 웹링크 또는 그래픽 인터페이스를 통한 다른 공유 메커니즘을 통해서 클라우드 시스템으로 파일을 공유한다. 일반적인 예시는 드롭박스와 구글 드라이브이다.

SSH, FTP와 클라우드 기반 저장소 앱에 관한 중요한 포인트는 그들은 유저가 패러다임을 바꾸도록 한다는 것이다. 예를 들어서, FTP는 기본 운영체제 시스템 커맨드와 확실히 다른 커맨드셋을 알도록 한다. SSH의 경우에는, 유저는 반드시 원격 시스템의 적절한 커맨드를 알아야한다. 예를 들어서, 윈도우의 유저가 유닉스 머신에 접근하면, SSH 세션동안에는 반드시 유닉스 커맨드를 사용해야한다.(네트워킹에서, **session**은 통신의 완벽한 라운드이고, 인증에 고르인하는 것부터 통신을 종료하는 로그오프까지를 의미한다.) 클라우드 기반 저장소 앱에서, 유저는 클라우드 서비스(웹 브라우저를 통해서)에 로그인하고 앱을 탐색하고 업로드, 다운로드, 공유를 위해서 그래픽 커맨드를 사용한다. 명백히, 유저들은 다른 커맨드의 집합을 필요롸지 않는 것을 더욱 편하다고 생각할 것이다. 분산 운영체제는 이 문제를 해결하기 위해서 디자인되었다.

### 19.4.2 분산 운영체제

분산 운영체제에서, 유저들은 그들이 로컬 리소스에 접근하는 것과 같은 방식으로 원격 리소스에 접근한다. 한 사이트에서 다른 사이트로의 프로세스와 데이터 이전은 분산 운영체제의 컨트롤 아래에 있다. 시스템의 목적에 따라서, 그것은 데이터 전송, 연산 전송, 프로세스 전송을 구현한다.

#### 19.4.2.1 Data Migrration

사이트 A의 유저가 사이트 B의 데이터에 접근하기를 원한다고 가정하겠다. 시스템은 두가지 기본적인 메서드 중 한가지로 전송할 수 있다. **data migration**을 통한 한가지 접근은 사이트 A에 전체 파일을 전송하는 것이다. 그 포인트로부터, 파일에 모든 접근은 로컬이다. 유저가 파일에 더이상 접근할 필요가 없으면, 파일의 카피가 사이트 B에 다시 보내진다. 오직 일부의 적당한 변화가 큰 파일에 만들어져도, 모든 데이터는 전송되어야한다. 이 메커니즘은 자동화된 FTP 시스템으로 생각하면 된다. 이 접근은 앤드류 파일 시스템(15장)에서 사용되었지만, 비효율적이었다.

사이트 A에 오직 현재의 태스크에 실제로 필요한 파일의 일부만 전송하는 다른 접근이다. 만약 다른 부분이 후에 필요하면, 다른 전송이 생긴다. 유저가 더 이상 필요로 하지 않으면, 수정된 일부분만이 사이트 B로 돌아간다.(디맨드 페이징과 비슷하다.) 대부분의 현대 분산 시스템은 이 방법을 사용한다.

어떤 메서드가 사용되든지, 데이터 전송은 데이터의 전송이 포함된다. 시스템은 만약 두사이트가 직접적으로 호환되지 않으면 반드시 다양한 데이터 번역을 수행해야한다.(예를 들어서, 만약 다른 캐릭터 코드 대표를 사용하거나 다른 숫자 또는 다른 비트의 순서의 정수를 사용하는 것이다.)

#### 19.4.2.2 Computaiton Migration

몇몇 상황에서, 우리는 시스템을 통해서 데이터보다는 연산을 전송하기를 원할수 있다. 이 프로세스는 **computation migration**이라고 불린다. 예를 들어서, 파일들의 요약을 얻기 위해서, 다른 사이트에 거주하는 큰 파일들에 접근할 필요가 있는 일을 고려해보자. 파일이 거주하는 사이트에서 접근하고 원하는 값을 연산을 시작한 사이트에서 리턴하는 것이 더욱 효율적이다. 일반적으로, 만약 데이터를 전송할 시간이 리모트 커맨드를 실행할 시간보다 길면, 리모트 커맨드가 사용되어야한다.

이런 연산을 다른 방식으로 실행된다. 프로세스 P가 사이트 A에 있는 파일에 접근하기를 원한다고 가정하겠다. 사이트 A에서 파일의 접근이 실행되고 RPC에 의해서 시작된다. RPC는 리모트 시스템에 있는 루틴을 실행할 네트워크 프로토콜을 사용한다. 프로세스 P는 사이트 A에서 미리 정의된 프로시저를 실행한다. 프로시저는 적절히 실행하고 P에 결과를 리턴한다.

대안으로, 프로세스 P는 사이트 A에 메시지를 보낸다. 사이트 A의 운영체제는 지정된 태스크를 실행하는 프로세스 Q를 새롭게 생성한다. 프로세스 Q가 그것의 실행을 마치면, 그것은 메시지 시스템을 통해서 P에 결과를 보낸다. 이 구조에서, 프로세스 P는 프로세스 Q와 동시에 실행한다. 실제로, 몇가지 사이트에서 동시에 작동하는 몇가지 프로세스가 있을 수 있다.

다양한 사이트에 거주하는 몇가지 파일에 접근하기 위해서 메서드가 사용될 수 있다. 한가지 RPC는 다른 RPC의 실행또는 다른 사이트로의 메시지의 전송에 결과할 수 있다. 비슷하게, 프로세스 Q는 그것의 실해웅에, 다른 사이트에 메시지를 보내고, 다른 프로세스를 생성한다. 이 프로세스는 Q에 메시지를 돌려보내거나 사이클을 반복한다.

#### 19.4.2.3 Process Migration

연산 이전의 논리적 확장은 **process migration**이다. 프로세스가 실행을 위해서 제출되면, 그것은 시작한 사이트에서 항상 실행되지 않는다. 전체 프로세스 또는 일부분이 다른 사이트에서 실행될 수 있다. 이런 구조는 다음의 이유로 사용된다.

- Load balancing 프로세스들은 사이트에서 부담을 분산할 수 있다.
- Computation speedup 만약 단일 프로세스가 다른 사이트 또는 노드에서 동시에 작동하는 서브 프로세스로 쪼개지면, 전테 프로세스 턴어라운드 시간이 줄어든다.
- Hardware preference 프로세스는 특정 프로세서에 적합한 실행을 요구하는 특정을 원할 수 있다.(매트릭스 인버전은 GPU에서)
- Software preference 프로세스는 특정 사이트에서 가용한 소프트웨어를 필요로하고, 소프트웨어는 이동될 수 없거나 프로세스를 이동하는게 싸다
- Data access 연산 이전에서 처럼, 만약 연산에 필요한 데이터가 크면, 그것은 원격에서 프로세스를 가지는 것이 데이터를 전송하는 것보다 효율적이다.

우리는 컴퓨터 네트워크에서 프로세스를 이동하기 위해서 두가지 상호보완적인 테크닉을 사용한다. 첫번쨰는, 시스템은 클라이언트로부터 프로세스가 이전되었다는 사실을 숨긴다. 클라이언트는 그러면 이전을 위해서 그녀의 프로그램을 코딩할 필요가 없다. 이 메서드는 보통 같은 시스템 사이에서 로드 밸런싱과 computation speedup을 실현함으로서 구현되고, 그들은 프로그램을 원격으로 실행하기 위해서 유저 인풋을 필요로 하지 않는다.

다른 접근은 유저가 어떻게 프로세스가 이전되는지 명시하는 것을 허용한다.(또는 필요) 이 메서드는 프로세스가 소프트웨어 또는 하드웨어 선호를 필요로할 때 사용된다.

너는 WWW가 분산 컴퓨팅 환경의 측면을 가진 것을 알 것이다. 확실히, 그것은 데이터 전송을 제공한다. 그것은 또한 computation migration도 제공한다. 예를 들어서, 웹 클라이언트는 쉡서버에서 데이터 베이스 명령어를 실행한다. 마지막으로, 자바, 자바 스크립트에서, 그것은 프로세스 이전의 형태를 제공한다. 자바 애플릿과 자바 스크립트 스크립츠는 그들이 실행되는 클라이언트로 보내진다. 네트워크 운영체제는 대부분의 이런 기능을 제공하지만, 분산 운영체제는 그들을 seamless하고 쉽게 접근하게 만든다. 강력하고 사용하기 쉬운 점이 WWW의 큰 성장의 이유이다.


## 19.5 Design Issues in Distributed Systems

분산 시스템의 디자이너들은 반드시 다양한 디자인 챌린지를 염두해야한다. 시스템은 그것이 실패를 견디도록 견고해야한다. 시스템은 파일 위치와 유저 이동성에대해서 유저에게 투명해야한다. 마지막으로, 시스템은 반드시 추가적인 연산 파워, 더 많은 공간, 더 많은 유저로 확장되게끔 허용되어야한다. 우리는 간단하게 이런 이슈들을 보겠다. 다음 절에서는, 우리는 우리가 말한 특정 분산시스템의 디자인을 설명하겠다. 

### 19.5.1 Robustness

분산 시스템은 다양한 타입의 하드웨어 실패로 고통받는다. 링크, 호스트, 사이트의 실패와 메시지의 손실이 가장 일반적이다. 시스템이 견고하다는 것을 확신하려면, 우리는 이런 실패를 탐지해야하고, 연산이 지속되게끔 시스템을 재설정하고, 실패가 수리되면 회복해야한다.

시스템은 그것이 실패의 레벨을 참아내고 함수를 일반적으로 실행함으로서 **fault tolerant**할 수 있다. fault tolerance의 정도는 분산 시스템의 디자인과 특정 실패에 달려있다. 명백히, 더 많은 fault tolerance가 낫다.

우리는 *fault tolerance*를 다양한 범위에서 사용한다. 통신 실패, 특정 머신 실패, 저장 디바이스 크래시, 저장 미디어의 부패는 모두 몇가지 확장으로 tolerated할 수 있다. **fault tolerant system**은 반드시 함수를 지속하고, 아마 이런 실패를 겪으면 디그레이드된 형태가 된다. 디그레이션은 성능, 기능에 영향을 줄 수 있다. 그것은 일부여야만하고, 실패를 발생했다. 한가지 컴포넌트가 실패해서 정지를 생산한 시스템은 fault tolerant가 아니다.

불행히도, 폴트 톨레런스는 구현하기 비싸고 힘들다. 네트워크 계층에서, 다양한 추가적인 통신 경로와 네트워크 디바이스(스위치, 라우터)가 통신 실패를 피하기 위해서 존재한다. 저장소 실패는 운영체제, 앱, 데이터의 손실을 초래한다. 저장소 유닛은 자동으로 이런 실패를 극복하기 위해서 추가적인 하드웨어 컴포넌트를 포함한다. 추가적으로 레이드 시스템은 하나 또는 그 이상의 저장소 디바이스 실패 상황에서도 데이터에 연속적인 접근을 보장한다.

#### 19.5.1.1 실패 탐지

공유 메모리가 없는 환경에서, 우리는 링크 실패, 사이트 실패, 호스트 실패, 메시지 손실를 구별할 수 없다. 우리는 보통 이 중에서 한가지가 생겼다고 탐지한다. 한번 실패가 탐지되면, 적절한 액션이 취해진다. 어떤 액션이 적절한지는 특정 앱에 따라 달려있다.

링크와 사이트 실패를 탐지하려면, 우리는 **heartbeat** 프로시저를 사용한다. 사이트 A와 B가 그들 사이에 물리적 직접 연결이 있다. 고정된 간격에서, 그 사이트들은 서로에게 `I-am-up` 메시지를 보낸다. 만약 사이트 A가 이 메시지를 미리 정한 주기 내에 받지 못하면, 그것은 사이트 B가 실패했다고 가정하고, A와 B사이의 링크가 실패하거나 B로부터의 메시지가 손실되었다고 한다. 이 시점에, 사이트A는 두가지 선택이 있다. 그것은 B로부터 `I-am-up` 메시지를 다른 시간 기간동안 기다리거나, B에게 `Are-you-up?`메시지를 보낸다.

만약 시간이 흐르고 사이트 A가 `I-am-up` 메시지를 받지 못하거나 `Are-you-up?`에 대한 답장을 받지 못했다. 다시, 사이트 A가 내리는 결론은 실패가 일어났다는 것디아.

사이트 A는 링크 실패와 사이트 실패를 다른 루트로 `Are-you-up?` 메시지를 보냄으로서 구별할 수 있다. 만약 이 메시지를 B가 받으면, 그것은 즉시 답장할 것이다. 이 긍정적 답장은 A에게 B가 준비되어 있고 그들 사이의 링크가 실패했다는 것이다. 우리가 메시지가 A에서 B까지 얼마나 걸릴지 알수 없기 때문에, 우리는 time-out 구조를 사용해야한다. A가 `Are-you-up?`을 보낸시간에, 그것은 B로부터 답변을 기다리길 원하는 시간 간격을 정한다. 만약 A가 답장 메시지를 시간 간격안에 받으면, 그것은 B가 준비되었다고 할 수 있다. 그렇지 않으면, A는 다음 상황을 가정한다.
- Site B is down
- Direct link from A to B is down
- Alternative path from A to B is down
- The message has been lost.(Although the use of reliable TCP)

사이트 A는 이런 것중 무엇이 일어났는지는 모른다.

#### 19.5.1.2 Reconfiguration

사이트 A가 이전에 말한 메커니즘으로 실패가 일어났다고 발견되었다고 가정하자. 그것은 반드시 시스템이 그것의 명령의 일반적인 모드를 재설정하고 진행하게 허용하는 프로시저를 시작해야한다.

- 만약 A에서 B의 직접 링크가 실패하면, 이 정보는 시스템의 모든 사이트에 방송되어서, 다양한 라우트 테이블이 이에 맞게 업데이트된다.
- 만약 시스템이 사이트가 실패했다고 믿으면(왜냐하면 사이트가 더 이상 닿지 않는다.), 그떄 시스템의 모든 사이트에 알려지고, 더이상 실패한 사이트의 서비스를 사용하려고 시도하지 않는다. 몇몇 활동을 위한 중신 중재자로 역할하는 사이트의 실패는 새로운 중재자를 필요로한다. 만약 사이트가 실패하지 않으면, 그때 우리는 두가지 서버가 중재자로서 취급하는 원치 않는 상황을 가진다. 네트워크가 파티션되면, 두 중재자는 충돌하는 액션을 시작한다. 예를 들어서, 만약 중재자가 상호배제를 구현하는 역할이면, 우리는 두 프로세스가 그들의 임계영역을 실행하는 상황을 보게된다.

#### 19.5.1.3 Recovery from failure

실패한 링크 또는 사이트가 고쳐지면, 그것은 반드시 시스템에 우아하고 부드럽게 통합되어야한다.

- A와 B사이의 링크가 실패했다고 가정하자. 그것이 고쳐지면, A와 B에게 알려져야한다. 우리는 19.5.1.1에서 설명한 하트비트 절차를 반복해서 이 정보를 알린다.
- 사이트 B가 실패했다고 가정하자. 그것이 고쳐지면, 그것은 반드시 다른 사이트에게 그것이 준비되었다고 알려야한다. 사이트 B는 그것의 로컬 테이블을 업데이트하기 위해서 다른 사이트로부터 정보를 얻어야한다. 예를 들어서, 그것은 라우팅 테이블 정보, 다운된 사이트의 리스트, 배달되지 않은 메시지, 실행되지 않은 트랙잭션의 트랜잭션 로그를 필요로한다. 만약 사이트가실패하지 안았고 도달만 안되어도, 그것은 이 정보를 필요로한다.

### 19.5.2 Transparency

분산 시스템에서 다중 프로세스와 저장 디바이스를 유저에게 **transparent**하게 만드는 것은 많은 디자이너에게 주요한 챌린지이다. 이상적으로, 분산 시스템은 반드시 그것의 유저를 평범하고 중심화된 시스템으로 보아야한다. transparent 분산 시스템의 유저 인터페이스는 로컬과 리모트 리소스간에 구별되지 않아야한다. 즉, 유저는 반드시 리소스가 로컬인 것처럼 리모트 리소스에 접근할 수 있어야하고 분산 시스템은 반드시 리소스를 위치하고 적절한 인터섹션에 배열하는 책임을 가져야한다.

transparency의 다른 측면은 user mobility이다. 그것은 유저가 그들이 특정 머신을 사용하는 것보다 시스템의 어떤 기기에도 로그인하게 허용하기 편한 것이다. 투명한 분산 시스템은 유저 모빌리티를 유저의 환경을 그가 로그인 한 곳으로 가져다 줌으로서 가능하게한다. LDAP같은 프로토콜은 로컬, 리모트, 모바일유저에게 인증 시스템을 제공한다. 한번 인증이 완료되면, 데스크탑 가상화 같은 기능은 유저가 그들의 데스크탑 세션을 원격 퍼실리티에서 보게 허용한다.

### 19.5.3 Scalability

여전히 다른 이슈는 **scalability**이다. 증가된 서비스 로드에 적응하는 시스템의 능력이다. 시스템은 제한된 리소스를 가지고 증가된 로드에 의해서 완벽히 포화된다. 예를 들어서, 파일 시스템에서, 포화는서버의 CPU가 높은 실행율에서 실행되거나 디스크의 I/O 요청이 I/O 서브시스템을 압도할 때 생긴다. Scalability는 상대적인 특징이고 그러나 그것은 정확히 측정된다. scalable 시스템은 그렇지 않은 것보다 증가하는 로드에 우아하게 대응한다. 첫번째, 그것의 성능은 더욱 온화하게 감소한다. 두번째, 그것의 리소스는 포화된 상태에 늦게 도착한다. 아무리 완벽한 디자인도 무한히 자라는 로드를 감다앟지 못한다. 새로운 리소스를 더하는 것은 문제를 해결하지만, 그러나 그것은 다른 리소스에 간접적인 로드를 추가하는 것을 생성한다. 더 최악으로, 시스템을 확장하는 것은 이런 문제 없이 자라날 가능성을 가진다. 분산 시스템에서, 스케일업 할 능력은 특별히 중요한데, 새로운 머신을 추가해서 네트워크를 확장하거나 두 네트워크를 연결하는 것이 일반적이기 때문이다. 짧게 말해서, 증가하는 디자인은 높은 서비스 로드를 감당하고, 유저 커뮤니티의 증가를 수용하고, 추가된 리소스의 간단한 통합을 허용해야한다.

증가는 fault tolerance에도 연관이 있다. 높게 로드된 컴포넌트는 마비되거나 faulty 컴포넌트처럼 행동한다. 추가로, 폴티 컴포넌트로부터 컴포넌트의 백업으로 로드를 바꾸는 것은 후자를 포화시킨다. 일반적으로, 스패어 리소스를 가지는 것은 신뢰성을 보장하는 것에 주요하다. 그러므로, 분산 시스템에서의 다중 리소스는 내부적인 장점을 대표하고, 시스템이 faulty tolerance와 scalabilty에 큰 가능성을 준다. 그러나, 부적절한 디자인은 이 가능성을 애매하게한다. 폴트 톨레랑스와 확장성 고려는 데이터와 컨트롤의 분산을 정의하는 디자인을 콜한다.

확장성은 또한 효율적인 저장소 구조와 관련되어 있다. 예를 들어서, 많은 클라우드 저장소는 사용하는 저장소의 양을 줄이기 위해서 **compression** 또는 **deduplication** 사용을 제공한다. *Compression*은 파일의 크기를 줄인다. 예를 들어서, `zip` 아카이브 파일은 `zip`커맨드를 이용해서 파일을 생성한다. 결과는 압축된 것이 더 적은 용량이다. 원래 상태로 돌리려면, 유저는 unzip 커맨드를 사용하면 된다. *Deduplication*은 데이터 저장 필요를 추가적인 데이터를 지움으로서 낮춘다. 이 기술과 함꼐, 데이터의 한가지 인스턴스만이 전체 시스템에 저장된다. 압축과 중복방지는 파일 레벨 또는 블럭레벨에서 사용되고, 그들은 함께 사용된다. 이런 기술들은 자동으로 분산 시스템에 유저가 특정 커맨드를 명시적으로 발행하지 않고 정보를 압축하게 지어지고, 그러므로 저장소 공간을 아끼고 유저 복잡도를 추가하지 않고 네트워크 통신 비용을 절약한다.

## 19.6 Distributed File Systems

비록 WWW가 오늘 날에 지배적인 분산 시스템이지만, 그것만 있지는 않다. 다른 중요하고 인기있는 분산 컴퓨팅의 사용은 **distributed File Systems**이다.

DFS의 구조를 설명하면, 우리는 DFS 문맥에서의 *service*, *server*, *client*를 정의해야한다. **Service**는 하나 이상의 머신에서 작동하고 클라이언트에게 특정 타입의 함수를 제공하는 소프트웨어 엔티티이다. **Server**는 단일 머신에서 작동하는 서비스 소프트웨어이다. **Client**는 **client interface**를 구성하는 명령어의 집합을 사용해서 서비스를 실행하는 프로세스이다. 가끔 로우 레벨 인터페이스가 실제 크로스 머신 교환을 위해서 정의된다. 그것은 **intermachine interface**이다.

이 용어를 이용해서, 우리는 파일 시스템이 클라이언트에게 파일 서비스를 제공한다고 한다. 파일 서비스를 위한 클라이언트 인터페이스는 파일 생성, 삭제, 읽기, 쓰기와 같은 원시적인 파일 명령어의 집합으로 구성된다. 파일 서버 컨트롤하는 주요한 하드웨어 컴포넌트는 파일이 저장되고 그들이 클라이언트의 요청에 따라 탐색하는 로컬 2차 저장소 디바이스의 집합이다.

DFS는 클라이언트, 서버, 저장 디바이스의 시스템이 분산된 시스템의 머신 사이에 흩어진 파일 시스템이다. 따라서, 서비스 행동은 네트워크를 통해서 실행되야한다. 단일 중앙화 데이터 레포지터리 대신에, 시스템은 자주 다중과 독립적인 저장 디바이스를 가진다. 너가 보듯이, 구체적인 설정과 DFS의 구현은 시스템에 따라서 다르다. 몇몇 설정에서, 서버들은 특정 머신에서만 작동한다. 다른 경우에, 머신은 서버와 클라이언트가 될 수 있다. 

DFS의 확정적인 기능은 시스템에서 클라이언트와 서버의 다양성과 자립성이다. 이상적으로, 비록 DFS가 그것의 클라이언트에게 평범하고 중앙화된 파일 시스템으로 나타나야한다. 즉, DFS의 클라이언트 인터페이스는 로컬과 원격 파일 사이에서 구별되지 않아야한다. 그것은 DFS가 파일을 위치시키고 데이터의 전송을 위해서 정렬하는 것에 달려있다. *transparent* DFS는 이전에 말했듯이, 유저의 환경을 유저가 로그인하는 곳에 옮김으로서 가능하게 한다.

DFS의 가장 중요한 성능 평가는 서비스 요청을 만족하는데 걸리는 시간이다. 기존의 시스템에서, 이 시간은 저장소 접근 시간과 약간의 CPU 프로세싱 시간이었다. DFSㅇ서, 원격 접근은 분산된 구조에 의해서 추가적인 오버헤드이다. 이 오버헤드는 요청을 서버로 전송하고, 네트워크로부터 클라이언트에게 다시 받는 시간을 포함한다. 각 방향마다, 정보의 전송에 덧붙여서, 통신 프로토콜 소프트웨어를 실행하는 CPU 오버헤드도 존재한다. 즉, 이상적인 DFS의 성능은 기존의 파일 시스템의 성능과 차이가 난다.

DFS의 기본적인 구조는 궁극적인 목표에 달려있다. 우리가 여기서 논의한 두가지 널리 사용되는 아키텍처 모델은 **client-server model**과 **cluster-based model**이다. 전자 아키텍처의 목적은 하나 이상의 클라이언트가 파일을 공유할때 파일이 클라이언트 머신에 개인적으로 저장되어있는 것같은 transparent를 제공하는 것이다. 분산 시스템 NFS와 OpenAFS가 주요한 예시이다. NFS는 유닉스 베이스의 일반적인 DFS이다. 그것은 몇가지 버전이있고 NFS 버전 3를 말하겠따.

만약 많은 앱들이 높은 가용성과 확장성에서 큰 데이터 셋에서 병렬로 작동할 필요가 있으면, 클러스터 베이스 모델은 클라이언트-서버 모델에서 더욱 정확해야한다. 구글 파일 시스템과 하둡 프레임워크의 일부로 작동하는 오픈 소스 HDFS가 널리 알려진 예제이다.

### 19.6.1 The client-server DFS model

서버는 부착된 저장소에 파일과 메타데이터를 둘다 저장한다. 몇몇 시스템에서, 하나 이상의 서버가 다른 파일을 저장하기 위해서 사용될 수 있다. 클라이언트들은 서버를 통해서 네트워크에 연결되어있고 잘 알려진 NFS 버전 3와 같은 프로토콜을 통해서 서버에 연결함으로서 DFS의 파일에 접근 요청을 할 수 있다. 서버는 인증을 가지고 올 필요가 있고, 요청한 파일 허가를 체크하고 인가되면 요청하는 클라이언트에게 파일을 배송할 책임이 있다. 클라이언트가 파일에 변화를 만들면, 클라이언트는 반드시 이런 변화를 서버에 전달해야한다. 파일의 클라이언트와 서버의 버전은 네트워크 트래픽과 서버의 워크로드를 최소화하는 방법으로 일관성을 유지해야한다.

**network file system(NFS)** 프로토콜은 다양한 아키텍처와 시스템을 통해서 채택되는 오픈 프로토콜로서 선 마이크로 시스템즈에서 개발되었다. 시작부터, NFS의 주의는 서버가 실패하면 빠르고 간편한 충돌 회복이었다. 이 목적을 구현하기 위해서, NFS 서버는 상태가 없도록 디자인되었다. 그것은 어떤 클라이언트가 파일에 접근하는지 또는 오픈 파일 기술자와 파일 포인터를 추적하지 않았다. 이것은 클라이언트가 파일 명령어를 발행하면, 그 명령은 서버 충돌의 상황에서도 멱등이다. **Idempotent**는 한번 이상 이슈되었지만 같은 결과를 리턴하는 명령어이다. 읽기의 경우에는, 클라이언트는 상태(파일 포인터)를 추적하고 만약 서버가 충돌하고 온라인 상태가 되면 다시 명령어를 발행한다. 너는 NFS 구현에 관한 추가적인 것은 15.8에서 볼 수 있다.

**Andrew file system(OpenAFS)**는 확장성에 집중해서 개발되었다. 특히, 연구자들은 서버가 최대한 많은 클라이언트를 지원하게 디자인했다. 이것은 서버에게 요청과 트래픽을 최소화하는 것을 의미한다. 클라이언트가 파일을 요청하면, 파일의 컨텐츠들은 서버로부터 다운로드되고 클라이언트의 로컬 저장소에 저장된다. 파일로의 업데이트들은 파일이 닫히면 서버에 보내지고, 파일의 새로운 버전은 파일이 열리면 클라이언트에게 보내진다. 반면에, NFS는 chatty하고 파일이 클라이언트에게 사용되면 블럭 읽기/쓰기 요청을 서버에게 보낼 것이다.

OpenAFS와 NFS는 로컬 파일 시스템에 사용된다. 다른 말로는, 너는 NFS 파일 시스템으로 하드 드라이브 파티션을 포맷하지 않는다. 대신에, 서버에서, 너는 너의 선택에 따라서 로컬 파일 시스템 파티션을 포맷하고 DFS를 통해서 공유된 디렉토리를 전한다. 클라이언트에서, 너는 간단히 전해진 디렉토리를 너의 파일 시스템 트리에 붙인다. 이 방법에서, DFS는 로컬 파일 시스템에서 책임을 분리하고 분산된 태스크에 집중할 수 있다.

DFS 클라이언트 서버 모델은 만약 서버가 충돌하면 실패의 단일 포인트에서 고통받을 수 있다. 컴퓨터 클러스터링은 이 문제를 추가적인 컴포넌트와 실패를 찾는 클러스터링 메서드와 실패에도 컴포넌트가 서버 명령어를 진행함으로서 문제를 해결하겠다. 추가로, 서버는 데이터와 메타데이터에 대한 모든 요청을 위한 버틀넥을 대표한다.

### 19.6.2 The Cluster-Based DFS Model

데이터, I/O 워크로드, 프로세싱의 양이 확장됨에 따라서, DFS가 fault-tolerant하고 scalable할 필요가 생겼다. 큰 버틀넥은 용납되지 않았고 시스템 컴포넌트 실패가 예측되었다. 클러스터 기반 아키텍처는 이런 필요를 만족하기 위해서 개발되었다.

이 모델은 **Google file system**과 **Hadoop distributed file system(HDFS)**를 대표하기 위한 기본 모델이다. 하나 이상의 클라이언트가 마스터 메타 테이터 서버와 몇가지 데이터 서버를 통해서 연결되어있다. 메타데이터 서버는 어떤 파일의 청크를 어떤 데이터 서버가 가지고 있는지의 매핑을 유지하고, 전통적인 디렉토리와 파일의 계층적 매핑도 있다. 파일 청크는 데이터 서버에 저장되어있고 컴포넌트 실패와 데이터에 빠른 접근을 위해서 특정 숫자로 복제되어있다.

파일에 접근하려면, 클라이언트는 반드시 먼저 메타데이터 서버에 접속한다. 메타데이터 서버는 클라이언트에게 요청한 파일 청크를 가지고있는 데이터 서버의 신원을 준다. 다른 파일의 청크는 만약 그들이 다른 데이터 서버에 저장되어있으면 읽거나 쓰이고 메타데이터 서버는 전체 프로세스에서 단한번 접속되면 된다. 이것이 메타 데이터 서버가 성능 버틀넥에 처하지 않게한다. 메타 데이터 서버는 데이터 서버에서 파일 청크를 재분배하고 밸런싱하는 역할이다. GFS는 2003년에 큰 분산 데이터 특화 앱을 지원하기 위해서 출시했다. GFS의 디자인은 4가지 주요한 관측에의해서 영향을 받았다.

- 하드웨어 컴포넌트 실패는 예외보다 일반적이고 주기적으로 예측되어야한다.
- 이런 시스템에 저장된 파일은 크다.
- 대부분의 파일들은 덮어쓰기보다는 새로운 데이터를 파일의 끝에 추가함으로서 바뀐다.
- 앱과 파일 시스템 API를 재디자인하는 것은 시스템의 유연성을 증가시킨다.

4가지 관측과 일치해서, GFS는 그것의 고유 API를 전하고 이 API로 앱이 프로그램되기를 필요로했다.

GFS가 개발된 후에, 구글은 GFS의 탑에 앉힐 **MapReduce**라고 불리는 모듈화된 소프트웨어 계층을 개발해다. MapReduce는 개발자가 큰 규모의 병렬 연산을 쉽게 실행하고 낮은 레이어 파일 시스템의 이점을 활서오하했다. 후에, HDFS와 하둡 프레임워크가 구글의 연구에 기반해서 생성되었다. GFS와 MapReduce처럼, 하둡은 분산 컴퓨팅 환경에서 큰 데이터 셋의 처리를 지원했다. 일전에 제안했듯이, 이런 프레임워크의 드라이브는 전통 시스템이 빅데이터 프로젝트에 필요한 성능과 능력으로 확장되지 않아서이다. 빅데이터 프로젝트는 소셜 미디어, 고객 데이터, 큰 과학적 데이터를 분석하고 크롤링하는 것을 포함한다.

## 19.7 DFS Naming and Transparency

**Naming**은 논리적 객체와 물리적 객체사이를 매핑하는 것이다. 예를 들어, 유저가 파일 이름으로 대표된 논리적 데이터 객체를 다루는 반면에, 시스템은 디스크 트랙에 저장된 물리적 데이터의 블럭을 조작한다. 보통, 유저는 텍스트 이름으로 파일을 언급한다. 후자는 디스크 블럭에 매핑되는 많은 로우레벨 식별자에 매핑된다. 멀티레벨 매핑은 유저에게 파일이 어디에 어떻게 저장되는지에 관한 디테일을 숨기는 파일의 추상화를 제공한다.

transparent DFS에서, 새로운 차원이 추상화에 추가되었다. 네트워크 안에있는 파일을 숨기는 것이다. 전통적인 파일 시스템에서, 네이밍 매핑의 범위는 디스크 안의 범위였다. DFS에서, 이 범위는 파일이 저장된 디스크의 특정 머신을 포함하게 확장되었다. 파일을 추상화로 다루는 컨셉은 한단계 더나아가서 **file replication**의 가능성을 이끌었다. 파일이름이 주어지면, 매핑은 파일 레플리카의 위치의 집합을 리턴했다. 이 추상화에서, 다중 카피의 존재와 그들의 위치는 숨겨졌다.

### 19.7.1 Naming structures

우리는 DFS에서 네임 매핑에 관련된 두가지 관련된 개념을 구분해야한다.

1. **Location transparency** 파일의 이름은 파일의 물리적인 저장 공간에 대한 힌트를 공개하지 않는다.
2. **Location independence** 파일의 이름은 파일의 물리적 저장소 위치가 바뀌어도 변하지 않는다.

파일이 다른 레벨에서 다른 이름을 가지기 때문에, 두 정의는 이전에 말한 네이밍의 레벨과 관련있다.(즉, 유저 레벨 텍스트 이름과 시스템 레벨 식별자) 그것이 같은 파일 이름에 다른 시점에 다른 위치를 매핑하기 떄문에 위치 독립적인 네이밍 구조는 동적 매핑이다. 그러므로, 위치 독립성은 위치 투명성보다 강한 성질이다.

실제로, 대부분의 현대 DFS는 정적, 위치 투명성 매핑을 유저레벨 이름에 제공한다. 몇몇은 **file migration**을 지원하는데, 즉, 파일의 위치를 자동으로 바꾸고, 위치 독립성을 제공한다. OpenAFS는 위치 독립성과 파일 모빌리티를 지원한다. HDFS는 file migration을 포함하지만 POSIX 표준을 따르지 않고, 구현과 인터페이스에 더 많은 유연성을 제공한다. HDFS는 데이터의 위치를 추적하지만 클라이언트로부터 이 정보를 숨긴다. 이 동적 위치 투명성은 셀프 튠이라는 기본 메커니즘을 허용한다. 다른 예제에서, 아마존의 S3 클라우드 저장소 기능은 APIs를 통해서 저장소의 블럭을 제공하고, 성능, 신뢰성, 능력 요구사항을 만족하면서 데이터를 움직인다.

몇가지 측면이 위치 독립성과 정적인 위치 투명서을 구별한다.

- 위치로부터 데이터를 분리하면, 위치 독립성에 의해서 전시되고, 파일을 위한 나은 추상화를 제공한다. 파일 이름은 파일의 그것의 컨텐츠보다는 그것의 위치인 가장 중대한 성질을 나타낸다. 위치 독립성 파일은 특정 저장소 위치에 부착되지 않은 논리적 데이터 컨테이너로 보일 수 있다. 만약 오직 정적 위치 투명성이 제공되면, 파일 이름은 여전히 비록 숨겨져있지만 특정한, 물리 디스크 블럭의 집합을 가진다.
- 정적 위치 투명성은 유저에게 데이터를 공유할 간단한 방법을 제공한다. 유저들은 위치 투명 상태인파일을 단순히 네이밍 함으로서 리모트 파일을 공유할 수 있다. 드롭박스와 다른 클라우드 베이스 저장소 솔루션이 이렇게 작동한다. 위치 독립성은 저장소 공간 자체를 공유하게 한다. 파일이 이동가능해지면, 시스템 와이드 저장소 공간은 단일 가상 리소스로 보인다. 가능한 이득은 시스템을 통해서 저장소의 활용을 밸런스맞추는 것이다.
- 위치 독립성은 저장소 디바이스 계층과 컴퓨터 내부 구조로부터 계층을 네이밍하는 것을 분리한다. 반대로, 만약 정적 위치 투명성이 사용되면, 우리는 간편히 컴포넌트 유닛과 머신 사이에 관계를 노출시킨다. 머신들은 네이밍 구조와 비슷하게 사용된다. 이 설정은 불필요한 시스템의 구조를 제한하고 다른 고려들과 부딪힌다. 루트 디렉토리에 책임을 가지는 서버는 네이밍 계층에 의해서 지시하고 중싱화되지 않은 가이드라인에 모순된다.

한번 이름과 위치의 분리가 완료되면, 클라이언트들은 원격 서버 시스템에 거주하는 파일에 접근할 수 있다. 실제로, 이런 클라이언트들은 **diskless**이고 운영체제 커널을 포함한 모든 파일을 제공하기 위해서 서버에 의존한다. 특별한 프로토콜들이 부트 과정에 필요하다. 디스크가 없는 워크스테이션에 커널을 가져오는 문제를 고려해보게다. 디스크 없는 워크스테이션은 커널이 없고, 그래서 그것은 커널을 탐색하기 위한 DFS 코드를 사용할 수 없다. 대신에, 특별한 부팅 프로토콜이 클라이언트의 ROM에 저장되고, 실행된다. 그것은 네트워킹을 가능하게하고 고정된 위치로부터 특별한 파일하나를 탐색한다. 한번 커널이 네트워크를 통해서 카피되고 로드되면, 그것의 DFS는 모든 다른 운영체제 시스템 파일을 가용하게 한다. 디스크 없는 클라이언트의 장점은 많은데, 저비용과 큰 간편성이다. 단점은 부트 프로토콜의 복잡성과 로컬 디스크보다 네트워크의 사용으로 인해 생기는 성능 로스이다.

### 19.7.2 Naming Schemes

DFS에서 네이밍 구조에는 3가지 주요한 접근이 있다. 가장 단순한 접근은, 파일이 그것의 호스트 이름과 로컬 이름에 의해서 구별되는 것이고, 시스템 전역에 유일한 이름을 보장한다. Ibis에서, 예를 들어, 파일은 *host:local-name*에 의해서 유일하게 구별되었고, *local-name*은 유닉스 패스와 유사하다. 인터넷 URL 시스템 또한 이 접근을 사용한다. 이 네이밍 구조는 위치 투명도 위치 독립도 아니다. DFS는 독립된 컴포넌트 유닛의 집합이고, 각각은 전체의 전통적인 파일 시스템이다. 컴포넌트 유닛은 여전히 고립되어있지만, 비록 이 수단은 리모트 파일을 언급하기 위해 제공되었다. 우리는 이 구조를 더 이상 알아보지 않겠다.

두번쨰 접근은 NFS에서 사용된다. NFS는 로컬 디렉토리에 원격 디렉토리를 붙이기 위한 수단을 제공하고, 그러므로 일관성 있는 디렉토리 트리의 형태를 준다. 초기의 NFS 버전은 투명하게 접근하기 위해서 이전에 마운트된 원격 디렉토리를 허용한다. **automount** 기능의 출현은 마운트 포인트의 테이블과 파일 구조 이름에 기반한 수요에 따라서 마운트를 허용했다. 컴포넌트들은 투명한 쉐어링을 위해서 통합되었고, 그러나 이 통합은 제한되고 일정하지 않았다. 왜냐하면 각 머신은 그것의 트리에 다른 리모트 디렉토리를 붙일 수 있었다. 결과된 구조는 다재다능했다. 

우리는 컴포넌트 파일 시스템의 통합을 3번쨰 접근을 통해서 이루어냈다. 여기서, 단일 전역 네임 구조는 시스템의 모든 파일에 돌았다. OpenAFS는 파일을 위한 단일 글로벌 이름 공간과 디렉토리를 제공하고, 다른 클라이언트 머신에서 비슷한 유저 경험을 제공했다. 이상적으로, 통합된 파일 시스템 구조는 전통적인 파일 시스템과 같았다. 실제로, 그러나, 많은 특별한 파일이 이 목적을 얻기 힘들게 했다.

이름 구조를 측정하기 위해서, 우리는 관리적인 복잡도를 보았다. 유지하기가 가장 복잡하고 어려운 것은 NFS 구조이다. 왜냐하면 어떠한 원격 디렉토리도 로컬 디렉토리 트리에 어디든지 부착될 수 있기 때문에, 결과된 계층은 높게 구조화되지 않을 수 있다. 만약 서버가 가용하지 않으면, 임의의 디렉토리 집합이 어떤 머신을 디렉토리에서 그것의 트리에 붙일지 허용한다. 그러므로, 유저는 디렉토리 트리에 접근할 수있고 그러나 다른 클라이언트에게 접근을 거절받을 수 있다. 

### 19.7.3 Implementation Techniques

투명한 네이밍의 구현은 파일을 관련된 위치에 매핑할 공간을 필요로한다. 매핑을 관리가능하게 유지하려면, 우리는 반드시 파일의 집합을 컴포넌트 파일로 종합하고 단일 파일 기초보다는 컴포넌트 유닛 기초로 매핑해야한다. 이 종합은 관리적인 목적또한 제공한다. 유닉스 같은 시스템은 이름-공간 매핑을 제공하고 디렉토리에서 파일을 재귀적으로 종합하기 위한 계층적 디렉토리 트리를 사용한다.

중요한 매핑 정보의 가용성을 증가시키기 위해서, 우리는 복제, 로컬 캐싱을 사용한다. 말했듯이 위치 독립성은 매핑이 시간을 거쳐서 바뀐다. 따라서, 매핑을 복제하는 것은 단순하지만, 정보의 일관된 업데이트를 불가능하게 한다. 이 장애물을 극복하기 위해서, 우리는 로우레벨, *location-independent file identifiers*를 제공한다. 텍스트 파일 이름들은 낮은 레벨 파일 어떤 컴포넌트 유닛이 지시하는 식별자로 매핑된다. 그들은 컴포넌트 유닛의 이전에 의한 무효화없이도 자유롭게 복제되고 캐시된다. 2레벨의 매핑에는 피할수 없는 비용이 필요하다. 컴포넌트 유닛을 위치에 매핑하고 단순하지만 일관된 업데이트 메커니즘이 필요하다. 이런 로우레벨을 사용하는 UNIX 디렉토리 트리 구현하는 것은, 위치 독립 식별자를 컴포넌트 유닛 이전에 전체 계층을 만든다. 실제로 바뀌는 유일한 측면은 컴포넌트 유닛 위치 매핑이다.

로우레벨 식별자를 구현하는 일반적인 방법은 구조화된 이름을 사용하는 것이다. 이런 이름들은 두가지 부분을 가진 비트 문자열이다. 첫번째 부분은 파일이 속한 컴포넌트 유닛을 식별한다. 두번쨰 부분은 유닛 안에 있는 특정파일을 식별한다. 더 많은 파트를 가진 변형이 있다. 구조화된 이름의 변함없음은 이름의 개별적인 부분이 모든 시간동안 유일하다는 것이다. 우리는 현재 사용중인 이름을 재사용하지 않게 신경씀으로서 유일성을 얻는다. 이 방법은 충분한 비트를 추가하거나 이름의 일부에 타임스탬프를 찍는 것이다. 이 프로세스를 보는 다른 방법은 공간 투명한 시스템에서, 위치 독립성을 생산하기 위해서 다른 레벨의 추상화를 추가하는 것이다.

## 19.8 Remote File Access

다음으로, 리모트 파일에 접근을 요청하는 유저를 고려해보겠다. 파일을 저장하는 서버는 네이밍 구조에 의해서 위치되어고 이제 실제 데이터 전송이 일어나야한다. 

이 전송을 성사시키는 한가지 방법은 **remote-service mechanism**을 통해서이고, 접근을 위한 요청들은 서버에 배달되고, 서버 머신은 접근을 수행하고 그들의 결과는 유저에게 포워드백된다. 원거리 서비스를 구현하는 가장 일반적인 방법은 RPC 패러다임이다. 직접 비유는 전통적인 파일 시스템내의 디스크 접근 메서드와 DFS안의 원격 서비스 메서드 사이에서 일어난다. 원격 메서드를 사용하는 것은 각 접근 요청에 디스크 접근을 실행한다.

원격 서비스 메커니즘의 납득가능한 성능을 보장하기 위해서, 우리는 캐싱의 형태를 사용한다. 전통적인 파일 시스테에서, 캐싱을 하는 이유는 디스크 I/O를 줄이고, 반면에 DFS에서는, 네트워크 트래픽과 디스크 I/O 둘다를 줄이기 위해서이다. 다음 절에서, 우리는 DFS에서의 캐싱의 구현을 설명하고 그것을 기본 원격 서비스 패러다임에 대조하겠다.

### 19.8.1 Basic Caching Scheme

캐싱의 개념은 단순하다. 만약 접근 요청을 만족하는 데이터가 캐시되지 않으면, 데이터의 카피가 서버로부터 클라이언트 시스템에 전달된다. 접근들은 캐시된 카피에서 수행된다. 이 아이디어는 캐시에 있는 최근에 접근한 디스크 블럭을 유지하고, 그래서 같은 정보에 반복된 접근은 지역적으로 네트워크 트래픽 없이 핸들된다.

교체 정책(LRU같은 알고리즘)은 제한된 캐시 사이즈를 유지한다. 직접적인 연관성이 접근과 서버에 트래픽 사이에 없다. 파일들은 여전히 서버 머신에 있는 한가지 마스터 카피로 구별되지만, 파일의 카피들은 다른 캐시에서 흩어져있다. 캐시된 카피가 수정되면, 변경은 연관된 일관성 시맨틱을 유지하기 위해서 마스터 카피에 반영된다. 캐시된 카피을 마스터 파일과 일관되게 유지하는 문제는 **cache consistency problem**이다. DFS 캐싱은 **network virtual memory**라고 불린다. 그것은 배킹 공간이 보통 로컬 디스크보다는 리모트 서버라는 것을 제외한다면, 디맨드 페이지 가상 메모리와 비슷하다. NFS는 스왑공간이 원격으로 마운트되는 것을 허용하고, 그래서 그것은 네트워크를 건너서 가상 메모리를 구현하고, 성능 페널티를 결과한다.

캐시된 데이터의 입자화는 파일의 블록에서 전체 파일까지 다양하다. 보통, 더 많은 캐시가 단일 접근을 만족하기 위해서 필요하고, 그래서 많은 저븐이 캐시된 데이터에 의해서 서브될 수 있다. 이 프로시저는 디스크 리드 어헤드와 비슷하다. OpenAFS은 64KB의 청크로 파일을 캐시한다. 다른 시스템은 클라이언트 수요에 따라서 개별 블럭의 캐싱을 지원한다. 캐싱 유닛의 증가는 히트율을 증가시키지만, 그것은 또한 미스 패널티를 증가시킨다. 왜냐하면 각 미스는 더 많은 데이터가 전송되게 하기 때문이다. 그것은 일관성 문제에 대한 염려도 증가시킨다. 캐싱의 유닛을 선택하는 것은 네트워크 전송 유닛과 RPC 프로토콜 유닛같은 파라미터를 고려한다. 네트워크 전송 유닛은 1.5KB이고, 그래서 캐시된 데이터의 큰 유닛은 배달을 위해서 해제되고 접수되면 합체된다.

블록 사이즈와 전체 캐시 사이즈는 블록 캐싱 구조에서 중요하다. 유닉스 같은 시스템에서, 일반적인 블록 사이즈는 4KB또는 8KB이다. 큰 캐시에서(1MB를 넘는), 큰 블록 사이즈가 이득이다. 작은 캐시에서는 큰 블럭 사이즈는 덜 유용한데 왜냐하면 그들은 적은 블럭을 캐시하고 히트율을 낮춘다.

### 19.8.2 캐시 위치

어디에 캐시된 데이터를 저장해야할까. 디스크일까 메인 메모리일까? 디스크 캐시는 메인 메모리 캐시보다 한가지 명백한 장점이 있다. 그들은 신뢰성이 있다. 캐시된 데이터에 수정은 만약 캐시가 휘발성 메모리에 있으면 잃을 수 있다. 더욱이, 만약 캐시된 데이터가 디스크에 보관되면, 그들은 회복중에도 여전히 그곳에 있고, 그들을 다시 실행할 필요가 없다. 메인 메모리 캐시는 그들의 장점을 가진다.
- 메인 메모리 캐시는 워크스테이션이 diskless하게 한다.
- 데이터는 디스크보다 메인 메모리에서 더 빠르다.
- 기술은 크고 비용이 적은 메모리를 향해간다. 성능의 속도 향상은 디스크 캐시의 장점을 압도할 것이다.
- 서버 캐시는 유저 캐시가 어디에 있는지에 상관없이 메인메모리에 있다. 만약 우리가 유저 머신에서 메인 메모리 캐시를 사용하면, 우리는 서버와 유저를 모두 사용하는 단일 캐싱 메커니즘을 만들 수 있다.

많은 원격 접근 구현은 캐싱과 리모드 서비스의 혼합으로 생각될 수 있다. NFS에서 예를 들면, 구현은 리모트 서비스에 기반을 했지만 클라이언트와 서버 사이드 메모리 캐싱으로 논쟁한다. 그러므로, 두가지 방법을 평가하려면,우리는 어떤 메서드가 강조되었는지의 정도를 평가한다. NFS 프로토콜과 대부분의 구현은 디스크 캐싱을 제공하지 않는다.

### 19.8.3 Cache-Update Policy

서버의 마스터 카피에 수정된 데이터 블럭을 적는데 사용하는 정책은 시스템의 성능과 신뢰성에 큰 영향을 가진다. 가장 단순한 정책은 그들이 어떤 캐시에 위치하자마자 디스크를 통해 데이터를 쓰는 것이다. **write-through policy**의 장점은 신뢰성이다. 적은 정보는 클라이언트 시스템이 충돌할때 손실된다. 그러나, 이 정책은 정보가 서버에 보내질떄까지 각 쓰기 접근을 기다려야하고, 그래서 그것은 낮은 성능을 보인다. 이 정책으로 캐싱하는 것은 쓰기 접근을 위해서 리모트 서비스를 사용하고 읽기 접근을 위해서만 캐싱을 이용하는 것이다.

다른 대안은 **delayed-write policy**인데, **write-back caching**이라고 알려져 있고, 우리는 마스터 카피에 대한 업데이트를 지연시킨다. 수정은 캐시에 쓰이고 시간이 지나고나서 서버를 통해서 쓰인다. 이 정책은 두가지 장점을 가진다. 첫번쨰, 쓰기가 캐시에 만들어지고, 쓰기 접근이 더 빨리 완료된다. 두번쨰, 데이터는 그들이 쓰이기전에 덮어쓰일수있고, 마지막 업데이트한번으로 필요한 것을 모두 쓴다. 불행히도, 이 정책은 신뢰성 문제가 있는데, 쓰이지 않은 데이터가 유저 머신이 충돌하면 날라가는 점이다.

delayed-write policy의 변형은 수정된 데이터 블럭이 서버에 플러시되는 것이다. 한가지 대안은 그것이 클라이언트의 캐시로부터 배출될때, 블럭을 플러시하는 것이다. 이 옵션은 좋은 성능을 결과로 내지만, 그러나 몇몇 블럭은 서버에 쓰이기전에 클라이언트의 캐시에 오래동안 거주할 수 있다. 이 정책과 이전의 정책의 타협은 일정 주기마다 캐시를 스캔하고 수정된지 오래된 블럭을 플러시하는 것이다. NFS는 파일 데이터에 정책을 사용하고, 그러나 한번 쓰기가 캐시 플러시 중에 이슈되면, 쓰기는 반드시 그것이 완료되기전에 서버의 디스크에 도달해야한다. NFS는 메타데이터를 다르게 다룬다. 어떤 메타 데이터 변화도 서버에 동기적으로 발행된다. 그러므로, 파일 구조 손실과 디렉토리 구조 손상은 클라이언트 또는 서버가 충돌하면 피해진다.

그러나 다른 변형은 파일이 닫혔을때 데이터를 쓰는 것이다. 이 정책은 OpenAFS에서 쓰이는 **write-on-close policy**이다. 짧은 주기 동안 열리고 거의 수정되지 않는 경우에는, 이 정책은 효과적으로 네트워크 트래픽을 줄이지 못한다. 추가로, write-on-close 정책은 파일이 쓰이는 도중에 프로세스를 닫아서 딜레이 시킬 필요가 있고, 지연된 쓰기의 성능 장점을 감소시킨다. 오랜 기간 열리고 자주 바뀌는 파일에서, 이 정책의 장점은 명백하다.

### 19.8.4 Consistency

클라이언트 머신은 가끔 마스터 카피와 캐시된 데이터의 복사본이 일치하는지 판단해야한다. 만약 클라이언트 머신이 캐시된 데이터가 기간이 만료되었다고 판단하면, 그것은 데이터의 최근 카피를 캐시해야한다. 이 캐시된 데이터의 유효성을 찾는 두가지 방법이 있다.

1. Client initiated approach 클라이언트는 유효성 검사를 시작한다. 그것은 서버에 연결하고 로컬 데이터가 마스터 카피와 일치하느지 체크한다. 유효성 검사의 빈도가 이 접근의 중요한 부분이고 일치성 시맨틱을 결정한다. 그것은 모든 접근이전에 체크하거나 첫번째 접근 이전에만 체크하는 범위를 가진다. 유효성 확인을 가진 모든 접근은 지연되고, 캐시에 의해서 즉시 처리되는 접근과 비교된다. 대안으로 확인은 고정된 시간 간격으로 시작된다. 그것의 빈도에 따라서, 유효성 확인은 네트워크와 서버에 로드될 수 있다.
2. Server-initiated approach 서버는 캐시하는 클라이언트, 파일을 위해서 기록한다. 서버가 잠재적 비일치를 감지하면, 그것은 반응한다. 비일치의 잠재성은 두 다른 클라이언트가 파일을 캐시할때 생긴다. 만약 유닉스 시맨틱이 구현되면, 우리는 서버가 활성화된 역할을 줌으로서 잠재적인 비일치를 막는다. 서버는 파일이 열릴때마다 알려주고, 원하는 모드는 반드시 매 열림마다 지시되어야한다. 서버는 특정 파일에 대한 캐싱을 비활성화함으로서 충돌하는 모드에 동시에 열릴수있다. 실제로, 캐싱을 비활성화하는 것은 리모트 서비스 모드의 명령어로 바꾸는 것을 결과로 한다.

클러스터 기반 DFS에서, 캐시 일치성 발행은 메타데이터 서버의 실재와 몇몇 복사된 파일 데이터 청크에 의해서 더욱 복잡해진다. 우리의 HDFS와 GFS예시를 사용하면, 두 차이를 비교할 수 있다. HDFS는 append-only write operations와 싱클 파일 라이터를 허용하고 GFS는 동시성 라이터와 함께 random writes를 지원한다. 이 복잡한 쓰기 일치성은 GFS를 보장했고 HDFS에서 간단해졌다.

## 19.9 Final Thoughts on DFS

클라이언트 서버와 클러스터 베이스 사이의 구분은 모호해지고 있다. NFS 버전 4.1 명세는 pNFS라고 불리는 NFS의 병렬 버전을 위한 프로토콜을 포함하지만, 그러나 이 쓰기, 채택은 느리다.

GFS, HDFS, 와 다른 대규모 DFS는 non-POSIX API를 전하고, 그래서 그들은 투명하게 디렉토리를 NFS와 OpenAFS처럼 일반적인 유저 머신에 디렉토리를 매핑한다. 오히려, 이런 DFS에 접근하는 시스템에서, 그들은 설치된 클라이언트 코드를 필요로한다. 그러나, 다른 소프트웨어 계층들은 DFS의 탑에 마운트된 NFS를 개발하게 허용한다. 이것은 매력적인데, 그것이 확장성의 장점과 다른 클러스터 기반 DFS의 장점을 가지면서 여전히 기존 운영체제 유틸리티를 허용하고 유저가 DFS에서 파일에 직접 접근하게 해준다.

오픈소스 HDFS NFS 게이트웨이는 NFS 버전3를 지원하고 HDFS와 NFS 서버 소프트웨어 사이에서 프록시로 일한다. HDFS가 현재 랜덤 라이트를 지원하지 않기 떄문에, HDFS NFS 게이트웨이는 또한 이 능력을 지원하지 않는다. 이것은 파일이 오직 한가지 바이트가 바뀐 스크래치로도 반드시 삭제되고 재생성된다는 것을 의미한다. 상업 기업과 연구자들은 이 문제를 해결하고 DFS의 스태킹을 허용하는 스택가능한 프레임워크를 만들고 있다.

파일 시스템의 다른 종류로서, 클러스터 기반 DFS보다 덜 복잡하지만 클라이언트 서버보다 더 복잡한 **clustered file system(CFS)** 또는 **parallel file system(PFS)**가 있다. CFS는 LAN에서 작동한다. 이런 시스템들은 중요하고 널리 사용되고 그러므로 여기서 언급될 가치가 있다. CFS는 **Lustre**와 **GPFS**를 포함한다. CFS는 근본적으로 데이터를 저장하는 N개의 시스템과 단일 클라이언트 서버 인스턴스로서 데이터에 접근하는 Y개의 시스템으로 취급된다. 반면에, NFS는 서버 네이밍을 가지고 두가지 분리된 NFS 서버들은 일반적으로 두가지 다른 네이밍 구조를 가진다. CFS는 다양한 저장 디바이스에서 저장소 컨텐츠를 가진다.

분산 파일 시스템은 오늘날 일반적으로 쓰이고, LANs, 클러스터 환경, WAN에서 파일 공유를 제공한다. 이런 시스템 구현의 복잡성은 낮게 잡으면 안되고, DFS는 반드시 운영체제에 독립적이고 반드시 가용성과 먼 거리, 하드웨어 실패, 같은 상황에서도 좋은 성능을 제공해야한다.