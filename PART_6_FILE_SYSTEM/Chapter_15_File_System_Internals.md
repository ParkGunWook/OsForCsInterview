## What we gonna learn

13장에서 보았듯이, 파일 시스템은 데이터와 프로그램을 포함한 온라인 저장소와 파일 컨텐츠 접근을 위한 메커니즘을 제공한다. 이 장에서 내부 구조와 파일 시스템의 명령어를 주로 고려하겠다. 우리는 파일 사용을 구조화하고 저장공간을 할당하고, 여유 공간을 회복하고, 데이터의 위치를 추적하고, 운영체제의 다른 부분과 상호작용하는 상세한 방법을 살펴보겠다. 

## Objectives

- Delve into the details of file systems and their implementation
- Explore booting and file sharing
- Describe remote file systems, using NFS as an example

## 15.1 File Systems

확실히, 어떠한 범용 컴퓨터도 하나의 파일만 저장하지 않는다. 보통은 컴퓨터안에는 수천 수만개의 파일이 있다. 파일들은 하드 디스크 드라이브, 광학 디스크와 비휘발성 메모리 디바이스를 포함한 랜덤 액세스 저장 디바이스에 저장된다. 볼륨 매니저에 따라서, 볼륨은 다중 파티션을 거친다. 

컴퓨터 시스템들은 다양한 수의 파일 시스템을 가지고, 파일 시스템들은 다양한 타입을 가진다. 예를 들어서, 일반적인 솔라리스 시스템은 여러개의 타입의 파일 시스템을 가진다.

이 책에서, 우리는 오직 범용 파일 시스템을 고려하겠다. 알아두면 좋은 것은, 특수목적을 위한 파일 시스템이 다양하다는 것이다. 솔라리스 예시로 위의 파일 시스템의 타입을 보이겠다.
- `tmpfs` - "temprary" 파일 시스템은 휘발성 메인 메모리에서 생기고 만약 시스템이 리부트되거나 충돌을 일으키면 그것의 컨텐츠를 삭제한다.
- `objfs` - "virtual" 파일 시스템(파일 시스템 같이 생긴 커널 대상 인터페이스)은 디버거에 커널 심벌에 접근을 허용한다.
- `ctfs`  - 시스템이 부트하거나 명령어 중에 실행을 지속하는 어떤 프로세스가 시작할때 "contract" 정보를 관리하기 위한 가상 파일 시스템이다.
- `lofs`  - 하나의 파일 시스템이 다른 파트에 접근하는 것을 허용하는 "loop back" 파일 시스템
- `procfs`- 파일 시스템처럼 모든 프로세스에 정보를 보여주는 가상 파일 시스템
- `ufs, zfs` - 범용 파일 시스템

컴퓨터의 파일 시스템은 광범위할 수 있다. 파일 시스템안이어도, 그것은 파일을 그룹으로 분리하고 그 그룹에서 관리와 활동하는데 유용하다. 이 구성은 디렉토리이 사용을 포함한다.

## 15.2 File System Mounting

파일이 사용되기전에 반드시 열려야하듯이, 파일 시스템은 반드시 시스템의 프로세스에 가용하기 전에 마운트되어야한다. 더 특정하게, 디렉토리 구조는 파일 시스템 네임 스페이스 안에서 가용할수있도록 마운트되는 볼륨을 포함한 다중 파일 시스템에 생겨야한다.

마운트 과정은 직관적이다. 운영체제는 디바이스의 이름과 파일 시스템이 부착된 파일 구조안의 장소인 **mount poing**가 주어진다. 몇몇 운영체제는 다른 것들이 디바이스의 구조를 관찰하고 파일 시스템의 타입을 결정하는 파일 시스템 타입이 제공되는 것을 필요로한다. 일반적으로, 마운트 포인트는 빈 디렉토리이다. 예를 들어서, 유닉스 시스템에서, 유저의 홈 디렉토리를 포함한 파일 시스템은 파일 시스템 안의 디렉토리 구조에 접근하기 위해서 `/home`으로 마운트되고, 우리는 디렉 토리 이름은 `home/jane`과 같이 `/home`으로 지정할 수 있다.

다음으로, 운영체제는 디바이스가 유효한 파일 시스템을 포함했는지 입증한다. 그것은 디스크 드라이버에게 디바이스 디렉토리를 읽도록 물어보고 디렉토리가 예상한 포맷인지 검증한다. 마지막으로, 운영체제는 파일 시스템이 특정 마운트 포인트에서 마운트되었다고 쓴다. 이 구조는 운영체제가 그것의 디렉토리 구조를 탐색하게 하고, 파일 시스템, 다양한 타입의 파일 시스템간에 바꾼다.

시스템은 기능을 명료하게하기위해서 시맨틱을 내포한다. 예를 들어서, 시스템은 파일을 포함한 디렉토리를 넘어서 마운트를 거부한다. 또는 그것은 디렉토리에서 마운티드 파일 시스템을 유효하게 만들고 파일 시스템이 언마운트될떄까지 디렉토리의 존재파일을 애매하게하고, 파일 시스템의 사용을 종료하고 디렉토리에서 원본 파일 접근을 허용한다. 다른 예시로, 시스템은 파일 시스템이 반복적으로 마운트하게 허용하거나 파일 시스템마다 마운트할 수 있다.

macOS 운영체제를 살펴보겠다. 시스템이 디스크를 처음 마주할때, macOS 운영체제는 디바이스에서 파일 시스템을 검색한다. 만약 한가지를 찾게되면, 그것은 자동으로 `/Volumes` 디렉토리 아래에 파일 시스템을 마운트하고, 파일 시스템의 이름으로 라벨링된 폴더 아이콘을 추가한다. 유저는 그러면 아이콘을 클릭하고 새롭게 마운트된 파일 시스템을 보인다.

마이크로소프트 윈도우 패밀리의의 운영체제는 확장된 two-level 디렉토리 구조를, 디바이스와 드라이브 레터에 할당된 볼륨으로 구현한다. 각 볼륨은 범용 디렉토리 구조를 가진다. 특정 파일로의 패스는 `드라이브 레터:\path\to\file`를 가진다. 최근의 윈도우 버전은 파일 시스템이 디렉토리 트리의 어디든지 마운트되게 허용한다. 윈도우 운영체제는 부트시점에 자동으로 모든 디바이스를 찾아내고 모든 파일 시스템을 마운트한다. 몇몇 운영체제에서는, 마운트 커맨드가 확실하다. 시스템 설정 파일은 디바이스의 리스트를 포함하고 부트 시점에 자동 마운팅 포인트를 마운트하지만, 다른 마운트들은 수동으로 실행된다.

## 15.3 Partitions and Mounting

디스크의 레이아웃은 운영체제와 볼륨 관리 소프트웨어에 따라서 다양한 변형을 가진다. 디스크는 다중 파티션으로 슬라이스되거나 볼륨은 다중 디스크의 다중 파티션을 가진다. 두번쨰 것은 RAID의 형태로 고려하면 된다.

각 파티션은 파일 시스템을 포함하지 않은 "raw"이거나, 파일 시스템을 포함한 "cooked"이다. **Raw disk**는 어떠한 파일 시스템도 적절하지 않은 곳에서 쓰인다. 유닉스 스왑 공간은, 예를 들어서, 그것이 디스크위의 전용 포맷을 사용하고 파일 시스템을 이용하지 않는다. Raw disk는 어떤 블럭이 미러링되었는지 어떤 것이 변하고 미러가 필요한지 알리는 비트맵같은 디스크 RAID 시스템에 의해서 필요한 정보를 가진다. 비슷하게, raw disk는 어떤 디스크가 각 RAID 집합의 멤버인지 알리는 RAID 설정 정보를 가진 미니어처 데이터베이스를 포함한다. Raw disk는 11.5.1에서도 언급되었다. 

만약 파티션이 적절하게 설치된 운영체제에게 부트가능한 파일 시스템을 포함하면, 파티션은 또한 부트 정보를 필요로한다. 이 정보는 그것의 전용 포맷을 가지고, 시스템이 로드된 파일 시스템 코드를 가지고 있지 않고 그러므로 파일 시스템 포맷으로 해석되지 않는다. 오히려, 부트 정보는 보통 메모리에 이미지로 블럭의 연속적인 시리즈이다. 이미지의 실행은 첫번째 바이트 같은 지정된 위치에서 시작된다. 이미지, **bootstrap loader**는 찾을 수 있는 파일 시스템 구조와 커널에 로드될 수 있고 실행을 시작할 정보에 대해 충분히 알고있다.

부트 로더는 특정 운영체제를 부팅할 명령어보다 더 많이 포함한다. 예를 들어서, 많은 시스템들은 **dual-booted**를 통해서 단일 시스템에 여러개의 운영체제를 설치한다. 어떻게 시스템이 무엇이 부트될지 알수 있을까? 다중 파일 시스템과 다중 운영체제를 가진 부트 로더는 부트 공간을 차지할 수 있다. 한번 로드되면, 그것은 드라이브에 존재하는 운영체제 중에서 하나를 부트할 수 있다. 드라이브는 다중 파티션을 가질 수 있고, 각각은 다른 타입의 파일 시스템과 운영체제를 가진다. 만약 부트 로더가 특정 파일 시스템 포맷을 이해하지 못하면, 파일 시스템에 저장된 운영체제는 부트가 불가능하다. 이 이유는 오직 몇가지 파일 시스템들이 주어진 운영체제에 루트 파일 시스템으로 지원한다.

**root partition**은 운영체제 커널와 다른 파일 시스템을을 포함한 부트로더에 의해서 선택되는데, 부트 시점에 마운트된다. 다른 볼륨들은 부트 시점에 자동으로 마운트되거나 후에 수동으로 마운트된다. 성공적인 마운트 명령어의 일부로서, 운영체제는 디바이스가 유효한 파일 시스템을 포함한다고 검증한다. 그것은 디바이스 드라이버가 디바이스 디렉토리를 읽게하고 디렉토리가 원하는 포맷을 가졌는지 검증한다. 만약 포맷이 유효하지 않으면, 파티션은 그것의 일관성을 체크하고 유저의 개입또는 없이 가능하면 고쳐져야한다. 마지막으로, 운영체제는 그것의 인 메모리 마운트 테이블에 파일 시스템이 마운트되었다고 하고, 파일 시스템의 타입을 따른다. 이 함수의 디테일은 운영체제에 달려있다.

마이크로소프트 윈도우 베이스 시스템은 각 볼륨을 분리된 이름 공간으로, 레터와 콜론으로 표시한다. 예를 들어서, 운영체제는 원하는 디바이스 구조의 필드의 파일 시스템에 포인터를 위치한다. 프로세스가 드라이브 레터를 특정하면, 운영체제는 적절한 파일 시스템 포인터를 찾고 특정 파일 또는 디렉토리를 찾기 위해서 디렉토리 구조를 순회한다. 최근의 윈도우즈는 파일 시스템을 존재하는 디렉토리 구조의 아무 위치에 마운트할 수 있다.

유닉스에서, 파일 시스템들은 어느 디렉토리에도 마운트 가능하다. 마운팅은 디렉토리의 아이노드의 인메모리의 카피안의 플래그를 세팅하는 것으로 구현된다. 플래그는 디렉토리가 마운트 포인트임을 알린다. 마운트 테이블안의 엔트리를 가르키는 필드는, 어떤 디바이스가 거기에 마운트되었는지 지목한다. 마운트 테이블 엔트리를 파일 시스템의 슈퍼블럭 포인터를 가진다. 이 구조는 운영체제가 그것의 디렉토리 구조를 순회하게하고, 다양한 타입의 파일 시스템사이에서 빈틈없이 스위칭한다.

## 15.4 File Sharing

파일을 공유하는 능력은 협업과 원하는 목표에 필요한 노력을 줄이는 것을 원하는 유저에게 바람직하다. 그러므로, 유저 기반 운영체제는 반드시 내재된 어려움때문에 파일을 공유하는 필요를 만족시켜주어야한다.

이 절에서는, 우리는 파일 공유의 측면을 살펴보겠다. 다중 유저가 파일을 공유할때 생기는 일반적인 문제를 논의하겠다. 한번 다중 유저가 파일을 공유하게 허용되면, 목표는 다중 파일 시스템을 공유하는 것으로 확장되고, 원거리 파일 시스템을 포함한다. 우리는 이 챌린지또한 알아보겠다. 마지막으로, 우리는 공유 파일에 생기는 충돌하는 문제가 무엇인지 알아보겠다. 예를 들어서, 다중유저가 파일을 쓰고있으면, 모든 쓰기가 일어나야할까? 또는 다른이로부터 유저의 행동을 보호해야할까?

### 15.4.1 Multiple Users

운영체제가 다중 유저를 허가하면서, 파일 공유, 파일 이름, 파일 보호에 대한 문제가 대두되었다. 파일이 유저에 의해서 공유되는 것을 허용하는 디렉토리 구조가 주어지면서, 시스템은 파일 공유를 중재하였다. 시스템은 유저가 다른 유저의 파일에 접근하게 허용하거나 파일에 접근 권한을 특별히 필요로하게 했다. 이것은 접근 컨트롤과 보호의 이슈이다. 

공유와 보호를 구현하기 위해서, 시스템은 반드시 싱글 유저 시스템보다 더 필요한 파일과 디렉토리 속성을 가지게 되었다. 비록 많은 접근이 이 필요를 만족하기 위해서 생겼고, 시스템들은 파일 유저와 그룹의 컨셉을 사용하기 위해서 진화했다. 소유자는 속성과 접근 권한을 바꾸고 파일의 대부분의 권한을 가진 유저이다. 그룹 속성은 파일의 접근을 공유할 수 있는 유저의 서브셋을 정의한다. 예를 들어서, 유닉스 시스템에서 파일의 소유주는 파일에 모든 명령어를 발행하는 동안, 파일의 그룹의 멤버는 이런 명령어의 일부만 실행가능하고, 모든 다른 유저들은 다른 명령어의 부분 집합을 실행할 수 있다. 정확히 그룹 멤버와 다른 유저에 의해서 어떤 명령어이 실행되는지는 파일의 소유주가 정의가능하다.

주어진 파일의 소유주와 그룹 아이디는 다른 파일 속성에 저장된다. 유저가 파일에 명령을 요청하면, 유저 ID는 만약 요청하는 유저가 파일의 소유주인지 결정하기 위해서 파일 성질과 비교된다. 이와 같이, 그룹 ID가 비교될 수 있다. 결과는 허용이 적용가능한지 알린다. 시스템은 요청한 명령에 허가를 적용하고 허하거나 불허한다.

많은 운영체제들은 싱글 디스크의 볼륨 또는 여러개가 부착된 디스크의 다중 볼륨을 포함한 다양한 로컬 파일 시스템을 가진다. 이런 경우에, 파일 시스템이 한번 마운트되면, ID 체킹과 허용 매칭은 직관적이다. 그러나 시스템 사이에서 이동할 수 있는 외부 디스크를 고려하겠다. 만약 시스템의 ID가 다르다면? 디바이스가 움직이거나 파일 소유가 이런 이동중에 리셋될때 ID가 시스템 사이에서 매치되는지 확인하는 주의가 필요하다.(예를 들어서, 우리는 새로운 유저 ID를 만들고 그 ID에 포터블 디스크의 모든 파일을 세팅한다. 아무런 파일이 실수로 존재하는 유저에의해서 접근되지 않게 확신한다.)

## 15.5 Virtual File Systems

우리가 보았듯이, 현대 운영체제는 반드시 파일 시스템의 다양한 타입을 지원해야한다. 그러나 어떻게 운영체제가 다양한 파일 시스템을 디렉토리 구조에 합치는 것을 허용할까? 그리고 어떻게 유저가 파일 시스템 공간을 다니면서 파일 시스템 타입 사이를 빈틈없이 움직이게 할까? 우리는 몇가지 구현 디테일을 살펴보겠다.

한가지 명료하지만 파일 시스템을 구현하는 메서드는디렉토리와 각 타입의 파일 루틴을 쓰는 것이다. 그러나, 유닉스를 포함한 대부분의 운영체제는 이 구현을 단순화하고, 구성하고, 모듈화하는 객체 지향 기술을 사용한다. 이 메서드의 사용은 비슷하지 않은 파일 시스템 타입을 네트워크 파일 시스템을 포함한 같은 구조안에 구현하게 허용한다. 유저들은 로컬 드라이브 위의 다중 파일 시스템을 포함한 파일 또는 네트워크를 건너서 가용한 파일 시스템에 접근한다.

데이터 구조와 단계는 구현 디테일로부터 기본 시스템콜 기능을 고립하는데 사용된다. 그러므로, 파일 시스템 구현은 3가지 계층으로 구현된다. 첫번째 계층은 파일 기술어와 open(), read(), write(), close()를 기반으로한 파일 시스템 인터페이스이다. 

두번째 계층은 **virtual file system** 계층이다. VFS 계층은 두가지 중요한 함수를 기능한다.
1. 그것은 VFS 인터페이스를 정의함으로서 그들의 구현을 파일 시스템 제너릭 명령어로부터 분리한다. VFS 인터페이스를 위한 몇가지 구현은 같은 기계에 공존하고, 지역적으로 마운트된 파일 시스템의 다른 타입에 접근을 투명하게 허용한다.
2. 그것은 네트워크를 통해서 파일을 유일하게 대표하는 메커니즘을 제공한다. VFS는 네트워크 와이드 유니크 파일인 수로 나타낸 유일한 지명인을 포함하는 **vnode**라는 파일 대표 구조에 기반한다. 이 네트워크 와이드 유일성은 네트워크 파일 시스템의 지원을 필요로한다. 커널은 모든 실행중인 노드에 대한 한개의 vnode 구조를 유지한다.

그러므로, VFS는 원거리의 것과 로컬 파일을 구별한다. 로컬 파일들은 그들의 파일 시스템 타입에 따라서 구별된다.

VFS는 파일 시스템 특정 명령어를 파일 시스템 타입에 따라서 로컬 요청을 핸들하기 위해서 활성화하고 NFS 프로토콜을 원거리 요청을 위해서 콜한다. 파일 핸들러는 관계있는 vnode를 건설하고 이런 프로시저에 어규먼트를 패스한다. 파일 시스템 타입 또는 원거리 파일 시스템 프로토콜을 구현하는 계층은 아키텍처의 3번째 계층이다.

리눅스에서의 VFS 아키텍처를 짧게 살펴보겠다. 4가지 메인 객체 타입이 리눅스 VFS에 있다.

- **inode object**는 개인 파일을 대표한다.
- **file object**는 열린 파일을 대표한다.
- **superblock object**는 전체 파일 시스템을 대표한다.
- **dentry object**는 개인 디렉토리 엔트리를 대표한다.

4가지 객체 타입에서, VFS는 구현될 명령어의 집합을 정의한다. 모든 타입의 오브젝트는 함수 테이블을 가르키는 포인터를 포함한다. 함수 테이블은 특정 객체에 대해서 정의된 명령어를 구현하는 실제 함수의 주로를 리스트한다. 예를 들어서, 파일 오브젝트를 위한 몇가지 명령어의 요약한 API는 다음을 포함한다.

- `int open(. . .)` - 파일을 연다
- `int close(. . .)` - 이미 열린파일을 닫는다.
- `ssize_t read(. . .)` - 파일로 부터 읽는다.
- `ssize_t write(. . .)` - 파일에 쓴다.
- `int mmap(. . .)` - 파일을 메모리 맵한다.

특정 파일 타입에 대한 파일 객체의 구현은 파일 객체의 정의에 특정화된 각 함수를 구현할 필요가 있다.(파일 객체의 완벽한 정의는 `/usr/include/linux/fs.h`에 위치한 파일 구조 `file_operations`에 되어있다.)

그러므로, VFS 소프트웨어 계층은 객체의 함수 테이블로부터 적절한 함수를 부르는 것으로서 이 오브젝트의 명령어를 수행하고, 정확히 어떤 종류의 객체가 아는지에 대해서 알고있다. VFS는 아이노드가 디스크 파일, 디렉토리 파일, 원격 파일을 가르키는지 모른다. 파일의 `read()` 명령어의 적절한 함수는 항상 그것의함수 테이블에 있고, VFS 소프트웨어 계층은 어떤 데이터가 실제로 불리는지 신경쓰지 않는다.

## 15.6 Remote File Systems

네트워크의 발전과 함께, 원격 컴퓨터간의 소통이 가능해졌다. 네트워킹은 캠퍼스에 퍼진 리소스의 공유를 허용했다. 한가지 공유해야할 리소스는 파일의 데이터이다.

네트워크와 파일 기술의 진화를 통해서, 원격 파일 공유 메서드는 바뀌었다. 첫번째 구현 메서드는 `ftp`같은 프로그램을 통한 머신 사이에서의 파일 전송을 포함했다. 두번째 주요 메서드는 리모트 디렉토리가 로컬 머신으로부터 보이는 **distributed file system(DFS)**를 사용했고, 3번쨰는 **Word Wide Web**이라는 ftp의 새버전이다. 브라우저는 리모트 파일에 접근을 얻을 필요가 있고, 분리된 명령어들이 파일을 전송하기 위해서 사용되었다. 증가하는, 클라우드 컴퓨팅은 파일 공유에 또한 사용된다.

`ftp`는 익명과 실명 접근을 둘다 사용했다. **Anonymous access**는 유저가 리모트 시스템의 허가 없이 파일을 전송하게 허용했다. WWW는 대부분의 파일공유를 익명으로 했다. DFS는 원격 파일에 접근하는 머신과 파일을 제공하는 머신의 통합을 더 단단히 했다. 이 통합은 이번 절에서 말하겠지만, 복잡성을 추가했다.

### 15.6.1 Client Server Model

리모트 파일 시스템은 하나 또는 이상의 리모트 시스템으로부터 하나 또는 이상의 파일 시스템을 컴퓨터가 마운트하게 허용한다. 이런 경우에, 파일을 포함하는 머신은 **server**이고, 파일에 접근을 탐색하는 머신은 **client**라고 한다. 클라이언트 서버 관계는 네트워크 머신에서 일반적이다. 일반적으로, 서버는 리소스가 클라이언트에 가용하다고 클라이언트에게 알리고 어떤 어떤 리소스이고 어떤 클라이언트인지 명시한다. 주어진 서버-클라이언트 시설의 상세 구현에 따라서 서버는 다중 클라이언트를 서브하고, 클라이언트는 다중 서버를 사용할 수 있다. 서버는 일반적으로 볼륨 또는 디렉토리 단계에서 가용한 파일을 명시한다. 클라이언트 확인은 어렵다. 클라이언트는 네트워크 이름 또는 다른 식별자로 구별되는데 IP 주소같은 것인데, 그러나 이런 것들은 **spoof**되거나 모방 당할 수 있다. 스푸핑의 결과로, 허가되지 않은 클라이언트가 서버에 접근할 수 있다. 더 안전한 솔루션은 암호화된 키를 통한 클라이언트의 보안 인증을 포함한다. 불행히도, 많은 도전과 함께하는 보안은, 클라이언트와 서버의 호환성(그 둘은 반드시 같은 암호화 알고리즘을 사용한다.)과 키 교환의 보안(낚아챈 키는 비인가 접근을 다시 허용한다.)을 포함한다. 이런 문제의 해결의 어려움 때문에, 안전하지 않은 인증 메서드는 대부분 여전히 사용된다.

유닉스의 경우와 그것의 NFS에서, 인증은 클라이언트 네트워킹 정보를 통해서 자리한다. 이런 구조에서, 클라이언트와 서버의 유저 ID는 반드시 일치해야한다. 만약 그들이 그렇게 하지 않으면, 서버는 파일에 접근 권한을 판단할 수 없게된다. 클라이언트에서 1000의 ID와 서버에서 2000을 가진 유저의 예시를 들겠다. 특정 파일에 대한 클라이언트에서 서버로의 요청은 적절히 다루어지지 않을 수 있고, 서버는 만약 실제 유저 ID 2000의 결정보다는 유저 1000이 파일에 접근을 가졌다고 결정한다. 접근은 그러므로 정확하지 않은 인증 정보에 기반해서 수락되거나 거절당한다. 서버는 반드시 클라이언트가 정확한 유저 ID를 가졌다고 믿어야한다. NFS 프로토콜은 many-to-many 관계를 허용한다. 즉, 많은 서버가 많은 클라이언트에게 파일을 제공할 수 있다. 실제로, 주어진 머신은 몇몇 NFS 클라이언트에게 서버가 되고 다른 NFS 서버의 클라이언트가 될 수 있다.

한번 리모트 파일 시스템이 마운트되면, 파일 명령어 요청은 네트워크를 통해서 DFS 프로토콜을 통해서 서버에 유저의 측면으로 보내진다. 일반적으로, 파일 오픈 요청은 요청하는 유저의 ID를 따라서 보내진다. 서버는 그러면 표준 접근을 만약 유저가 요청한 모드안에서 파일을 접근할 계정을 가졌다면 표준 접근 체크를 적용한다. 요청은 수락되거나 거절된다. 만약 수락되면, 파일 핸들은 클라이언트 앱에 리턴되고 앱은 파일에 읽기, 쓰기, 다른 명령어를 수행한다. 클라이언트는 접근이 완료되면 파일을 닫는다. 운영체제는 로컬 파일 시스템 마운트와 비슷한 시맨틱을 적용하거나 다른 시맨틱을 사용한다.

### 15.6.2 Distributed Information Systems

클라이언트-서버를 관리하기 쉽게 만들려면, **distributed naming services**라고도 알려진 **Distributed information systems**는 원격 컴퓨팅에 필요한 정보에 통합된 접근을 제공한다. **Domain name system**은 host-name-to-network-address 변역을 전체 인터넷을 대상으로 제공한다. DNS가 널리 알려지기 전에, 같은 정보를 포함한 파일들은 이메일 또는 ftp를 통해서 네트워크 호스트끼리 보내졌다. 명백히, 이 방법론은 그렇게 크지 않다.

다른 분산 정보 시스템은 *user name/password/user ID/group ID* 공간을 분산 환경에 제공한다. 유닉스 시스템들은 분산 정보 메서드의 넓은 다양성을 제공한다. Sun Microsystems는 **yellow pages**(since renamed **network information service(NIS)**)를 소개했고 대부분의 산업들이 그것의 사용을 채택했다. 그것은 유저 이름, 호스트 이름, 프린터 정보의 저장소를 중심화했다. 불행히도, 그것은 패스워드가 보안화되있지 않고 IP 주소를 통해서 호스트끼리 식별하는 안전하지 않은 메서드를 사용했다. Sun의 NIS+는 NIS에 대해서 보안 교체를 이루어냈고 더욱 복잡해지고 널리 채택되지는 않았다.

마이크로소프트의 **common Internet file System(CIFS)**의 경우에, 네트워크 정보는 서버가 요청된 파일 시스템에 접근을 허용하고 거절하는 네트워크 로그인을 생성하는 유저 인가와 함께 결합하기 위해서 사용되었다. 이 인가가 명백해지려면, 유저 이름은 반드시 기계에서 기계까지 매치되어야한다. 마이크로소프트는 유저를 위한 단일 이름 공간을 제공하기 위해서 분산 이름 구조로서 **active directory**를 사용했다. 한번 생성되면, 분산된 네이밍 시설은 MS의 **kerberos** 네트워크 인가 프로토콜을 통해서 유저를 인가하기 위해서 모든 클라이언트와 서버에 의해서 사용되었다.

산업은 안전한 분산 naming 메커니즘으로서 **lightweight directory access protocol**의 사용을 향해서 움직인다. 실제로, active directory는 LDAP에 기반했다. 오라클 솔라리스와 대부분의 주요한 운영체제들은 LDAP를 포함하고 그것이 유저 인가를 실행하게 허용한다. 상상컨데, 한가지 분산된 LDAP 디렉토리는 모든 유저와 리서스 정보를 저장하기 위해서 조직에 의해서 사용될 수 있다. 결과는 유저에게 안전한 단일 사인-on을 준다. 유저는 그들의 조직내의 모든 컴퓨터에서 그들의 인가 정보를 들어간다. 그것은 또한 각각의 시스템 또는 다른 분산 정보 서비스에서 흩날려있는 다양한 파일 정보를 합침으로서 시스템 관리자 노력을 경감시킨다.

### 15.6.3 Failure Modes

로컬 파일 시스템들은 파일 시스템을 포함한 드라이브의 실패, 디렉토리 또는 디스크 관리 정보(collectively called **metadata**)의 부패, 디스크 컨트롤러 실패, 케이블 실패, 호스트 어댑터 실패를 포함한 다양한 이유로 실패할 수 있다. 유저 또는 시스템 관리자 실패는 또한 파일을 잃게하거나 전체 디렉토리 또는 볼륨이 삭제되게 야기할 수도 있다. 많은 실패들이 호스트가 크래시하고 에러 상황이 보여지게 할 수 있고 사람 개입이 데미지를 수리하게 필요할 수 있다.

원격 파일 시스템들은 더 큰 실패 모드를 가진다. 네트워크 시스템의 복잡성과 원격 머신간의 상호작용때문에, 많은 문제들이 원격 파일 시스템의 적절한 작동과 간섭하면서 생긴다. 네트워크의 경우에는, 네트워크는 두 호스트 사이에서 방해받을 수 있다. 이런 방해는 하드웨어 실패, 낮은 하드웨어 설정, 네트워킹 구현 이슈로부터 생길 수 있다. 비록 몇몇 네트워크들이 빌트인 resiliency을 가진다. 어떠한 단일 실패는 DFS 명령어의 플로우에 따라서 방해되지 않는다.

원격 파일 시스템을 사용하는 중간을 고려하겠다. 그것은 원격 호스트로부터 오픈된 파일을 가진다. 그것은 파일을 열기위한 디렉토리 룩업, 데이터를 파일에 읽고 쓰기, 파일 닫기같은 행동을 포함한다. 이제 네트워크의 파티셔닝, 서버의 크래시, 또는 서버의 예정된 종료를 고려하겠다. 갑자기, 원격 파일 시스템은 닿을 수 없어진다. 이 시나리오는 잘 일어나지 않고, 그래서 클라이언트 시스ㅔㅁ이 만약 로컬 파일 시스템을 잃으면 적절하지 않다. 오히려, 시스템은 모든 명령어가 서버를 잃거나 명령어를 모두 종료시키거나 다시 서버가 접근가능해질떄까지 명령어를 딜레이할 수 있다. 이런 실패 시맨틱들은 원격 파일 시스템 프로토콜의 일부로서 구현되고 정의될 수 있다. 모든 명령어의 종료는 유저의 데이터 읽기를 야기한다. 그러므로, 대부분의 DFS는 파일 시스템 명령어의 지연을 강제하거나 허용하고, 원격 호스트가 다시 가용해지리라는 희망을 가진다.

이런 실패로부터의 회복을 구현하려면, 몇몇 **state information**의 종류는 클라이언트와 서버에서 유지될 수 있다. 만약 서버와 클라이언트가 그들의 현재 활동과 열린 파일의 지식을 유지하면, 그때 그들은 실패로부터 빈틈없이 회복가능하다. 서버가 충돌한 상황, 그러나 그것이 원격으로 마운트된 파일 시스템과 열린 파일을 인지하면, NFS 버전 3는 단순한 접근을 취하는데, **stateless DFS**를 구현한다. 그것은 비록 파일 시스템이 원격으로 마운트되고 파일이 이미열렸지 않으면 파일 읽기 또는 쓰기를 위한 클라이언트 요청이 일어나지 않았다고 가정한다. NFS 프로토콜은 적절한 파일을 위치할 정보를 가지고 요청한 명령을 실행한다. 비슷하게, 그것은 어떤 클라이언트가 마운트된 볼륨을 전했는지 추적하지 않는다. 이 stateless 접근이 NFS resilient하게 만들고 구현하기 쉽지만, 그것은 안전하지 않게 만든다. 예를 들어서, 단조한 읽기 또는 쓰기 요청은 NFS 서버에 의해서 허용될 수 있다. 이런 이슈들은 산업 표준 NFS 4에서 지시된다.

## 15.7 Consistency Semantics

Consistency semantics는 파일 공유를 지원하는 파일 시스템에서 중요한 기준이다. 이 시맨틱은 어떻게 시스템의 다중 유저들이 공유 파일에 동시에 접근하는지 명시한다. 특히, 그들은 한 유저에 의한 데이터의 수정이 있을때 다른 유저들에 의해 관측가능하게 명시한다. 이런 시맨틱들은 파일 시스템과 함께 코드로 구현된다.

일관성 시맨틱들은 6장의 프로세스 동기화 알고리즘과 직접 연결이 되어있다. 그러나, 그 장의 복잡한 알고리즘들은 파일 I/O에서는 구현되지 않는데 왜냐하면 디스크와 네트워크의 느린 전송속도와 큰 레이턴시 때문이다. 예를 들어서, 원거리 디스크에 아토믹 트랜잭션은 몇가지 네트워크 통신, 디스크 읽기/쓰기를 포함한다. 이런 기능의 집합에 시도는 형편없이 수행된다. 성공적인 복잡한 공유 시맨틱의 구현은 Andrew 파일 시스템에서 찾아볼 수 있다.

다음 절에서, 우리는 유저에 의한 같은 파일 접근의 시리즈가 항상 open()과 close() 명령어 사이에 쌓여있다고 가정하겠다. 접근의 시리즈는 **file session**을 만든다. 이 개념을 설명하기 위해서, 우리는 몇가지 유명한 consistency semantics를 살펴보겠다.

### 15.7.1 UNIX Semantics

유닉스 파일 시스템은 다음의 consistency semantics를 사용한다.

- 유저에 의한 열린 파일에 쓰기는 이 파일을 오픈한 다른 유저들에게 즉시 영향을 준다.
- 공유의 한가지 모드는 파일의 현재 위치의 포인터를 공유하게 허용한다. 그러므로, 한 유저에 의한 포인터의 전진은 모든 공유하는 유저에게 영향을 준다. 여기서, 파일은 모든 접근을 끼우는 단일 이미지를 그들의 원본에 상관없이 가진다.

유닉스 시맨틱에서, 파일은 독점적인 자원으로 접근되는 단일 물리 이미지와 연관된다. 단일 이미지에 대한 논쟁은 유저 프로세스에서 지연을 야기한다.

### 15.7.2 Session Semantics

Andrew file system(OpenAFS)는 다음의 consistency semantics를 사용한다.

- 유저에 의한 열린 파일에 쓰기는 같은 파일을 연 유저에게 즉시 영향을 주지 않는다.
- 한번 파일이 닫히면, 그것에 생긴 변화는 오직 후에 시작된 세션에만 영향을 준다. 이미 열린 파일의 인스턴스는 이런 변화들을 반영하지 않는다.

이러한 시맨틱에 따르면, 파일은 동시에 몇가지 이미지에 임시적으로 연관될 수 있다. 결과적으로, 다중 유저들은 그들의 파일의 이미지에 동시에 읽기와 쓰기를 수행하도록 허용된다. 접근을 스케쥴링하는 것에는 아무런 제약이 없다.

### 15.7.3 Immutable-Shared-Files Semantics

유일한 접근은 **immutable shared files**이다. 한번 파일이 그것의 생성자에 의해서 공유된다고 선언되면, 그것은 수정되지 않는다. immutable 파일은 2가지 주요한 성질을 가진다. 그것의 이름은 재사용되지 않고, 그것의 컨텐츠는 변하지 않는다. 그러므로, immutable file의 이름은 파일의 컨텐츠는 고정되었다고 의미한다. 분산 시스템에서 이런 시맨틱의 구현은 간단한데, 왜냐하면, 공유가 통제되기 때문이다.(read-only)

## 15.8 NFS

네트워크 파일 시스템은 일반적이다. 그들은 전체 디렉토리 구조와 클라이언트 시스템의 인터페이스와 통합되어있다. NFS는 널리 사용되고 잘 구현된 클라이언트서버 네트워크 파일 시스템의 좋은 예시이다. 여기서, 우리는 nfs의 구현 상세를 살펴보겠다.

NFS는 LANs(WANs)를 통해서 원격 파일에 접근하는 소프트웨어 시스템의 구현이고 상세이다. NFS는 대부분의 유닉스 행상인과 몇몇 PC 운영체제가 지원하는 ONC+의 일부이다. 여기서 묘사된 구현은 솔라리스 운영체제의 일부이고, UNIX SVR4의 변형된 버전이다. 그것은 TCP 또는 UDP/IP 프로토콜을 사용한다. 명세와 구현은 우리의 NFS의 설명에 얽혀있다. 디테일이 필요할떄마다, 우리는 솔라리스 구현을 언급하겠다. 설명이 일반적이면, 그것또한 명시로 적용할 것이다.

NFS는 여러가지 버전이 있고, 버전4가 최신이다. 여기서 우리는 버전3을 언급하고, 가장 일반적인 것이다.

### 15.8.1 Overview

NFS는 연결된 워크스테이션을 독립적인 파일 시스템을 가진 독립적인 머신의 집합이라고 생각한다. 목적은 투명한 상태에서 이런 파일 시스템사이에서 공유의 단계를 허용하는 것이다. 공유는 클라이언트-서버 관계에 기반한다. 기계는 아마, 자주, 클라이언트이고 서버이다. 공유는 어느 머신의 페어에서도 허용된다. 머신 독립성을 보장하기 위해서, 원격 파일 시스템의 공유는 오직 클라이언트 서버에만 영향을 미치고 다른 것에는 미치지 않는다.

그래서 원격 디렉토리는 다른 특정한 머신 M1으로부터 투명한 상태로 접근이 가능하다. 그 머신의 클라이언트는 반드시 첫번째로 마운트 명령어를 실행해야한다.  명령의 시맨틱은 로컬 파일 시스템의 디렉토리를 넘어서 원격 디렉토리를 마운팅하는 것을 포함한다. 한번 마운트 명령이 완료되면, 마운트된 디렉토리는 로컬 파일 시스템의 서브트리처럼보이고, 로컬 디렉토리로부터 내려오는 서브트리를 교체한다. 로컬 디렉토리는 새롭게 마운트된 디렉토리의 루트의 이름이 된다. 마운트 명령어를 위한 아규먼트는 원격 디렉토리의 명세는 투명하게 수앻되지 않는다. 원격 디렉토리의 위치는 반드시 제공되어야만한다. 그러나, 머신 M1에서의 유저는 원격 디렉토리에서 파일에 접근해야한다.

접근 권리 허용에 따라서, 파일 시스템 내의 어느 파일 시스템, 디렉토리는 로컬 티렉토리의 꼭대기에 원격으로 마운트될 수 있다. 디스크가 없는 워크스테이션도 그들의 루트로부터 서버를 마운트할 수 있다. 마운트를 연속하는 것은 몇몇 NFS 구현에서 허용된다. 즉, 파일 시스템은 다른 파일 시스템을 넘어서 마운트될 수 있는 것은 원격으로 마운트된다. 머신은 그자체로 실행된 오직 이런 마운트에만 영향을 받는다. 원격 파일 시스템을 마운팅 하는 것은 클라이언트에게 다른 파일 시스템의 접근을 주지 않고, 이전의 파일 시스템에 마운트된다. 그러므로, 마운트 메커니즘은 타동성 성질을 가지지 않는다.

NFS의 디자인 목표중에 하나는 다른 머신, 운영체제, 네트워크 아키텍처에서 여러종류의 환경에서 작동하는 것이다. NFS 명세는 이런 매체에 독립적이다. 이 독립성은 두가지 구현 독립적인 인터페이스 사이에서 external data representation(XDR) 프로토콜의 꼭대기에 지어진 RPC의 사용을 통해서 성취된다. 따라서, 만약 시스템의 다른 종류의 머신과 파일 시스템이 적절히 NFS에 의해서 연결되면, 다른 타입의 파일 시스템은 locally와 remotey하게 마운트될 수 있다.

NFS 명세는 마운트 메커니즘과 실제 원격 파일 접속 서비스에 의해서 제공되는 서비스 사이에서 구별한다. 따라서, 두가지 분리된 프로토콜들은 이런 서비스에 의해서 명시된다. 마운트 프로토콜과 원격 파일 접속을 위한 프로토콜이, **NFS protocol**이다. 프로토콜들은 RPCs의 집합으로 명시된다. 이 RPCs들은 투명한 원격 파일 접속을 구현하는데 사용되는 빌딩 블럭이다.

### 15.8.2 The Mount Protocol

**mount protocol**은 서버와 클라이언트 사이에 초기 연결을 설치한다. 솔라리스에서, 각 머신은 커널 밖에서 서버 프로세스를 가지고, 프로토콜 함수를 수행한다.

마운트 명령어는 마운트될 원격 디렉토리의 이름과 그것을 저장할 서버 머신의 이름을 포함한다. 마운트 요청은 적절한 RPC에 매핑되고 특정 서버 머신에서 작동하는 마운트 서버로 포워드된다. 서버는 그들이 마운트되도록 허용된 머신의 이름을 따라서 마운팅을 위해 배출하는 로컬 파일 시스템을 명시하는 **export list**를 유지한다.(솔라리스에서, 이 리스트는 `/etc/dfs/dfstab`, 이고 sudo로만 수정이 가능하다.) 명시는 또한 read-only와 같은 접근권한을 포함한다. export lists와 mount tables의 관리를 단순화하기 위해서, 분산 이름 구조는 이 정보를 유지하고 적절한 클라이언트에게 가용하게 사용된다.

배출된 파일 시스템의 어느 디렉토리는 허용된 머신에 의해서 원격으로 마운트될 수 있다. 구성요소 유닛은 디렉토리이다. 서버가 그것의 export list에 만족하는 마운트 요청을 받으면, 그것은 클라이언트에게 마운트된 파일 시스템 안의 파일에 접근을 위한 키처럼 서브하는 파일 핸들을 리턴한다. 파일 핸들은 서버가 그것이 저장한 개별 파일을 구별하는데 필요한 모든 정보를 포함한다. 유닉스에서, 파일 핸들은 파일 시스템 식별자와 배출된 파일 시스템 안의 정확한 마운트된 디렉토리를 식별하는 아이노드 번호를 포함한다.

서버는 또한 클라이언트 머신의 리스트와 현재 마운트된 디렉토리에 적합한 리스트를 유지한다. 이 리스트는 관리자 목적으로 주로 사용되는데, 예를 들어서, 모든 클라이언트에게 서버가 꺼지고 있다고 알린다. 이 리스트의 엔트리에 추가와 삭제를 통해서만 서버 스테이트가 마운트 프로토콜에 의해서 영향을 받을 수 있다.

보통, 시스템은 부트 시점에 설립된 정적 마운팅 preconfiguration을 가진다. 그러나, 이 레이아웃은 수정될 수 있다. 실제 마운트 프로시저에 의해서, 마운트 프로토콜은 몇가지 다른 프로시저를 포함한다.

### 15.8.3 NFS 프로토콜

NFS 프로토콜은 원격 파일 명령어를 위한 RPC의 집합을 제공한다. 절차는 다음 명령어를 지원한다.

- 디렉토리 내에서 파일을 검색한다.
- 디렉토리 엔트리의 집합을 읽는다.
- 링크와 디렉토리를 조작한다.
- 파일 설질에 접근한다.
- 파일을 읽고 쓴다.

이런 프로시저는 원격으로 마운트된 디렉토리를 위한 파일 핸들이 생긴 이후에 호출된다. 

open과 close 명령의 생략은 의도적인 것이다. NFS의 좋은 기능은 그들이 stateless라는 것이다. 서버는 그들의 클라이언트부터 다른 접근하는 것에 관한 정보를 유지하지 않는다. 유닉스의 오픈 파일 테이블 또는 파일 구조는 서버 단에 존재한다. 결과적으로, 각 요청은 유니크 파일 식별자와 파일 안의 절대적인 오프셋을 포함한 아규먼트의 전체 집합을 제공해야한다. 결과적인 디자인은 탄탄하다. 어떤 특별한 측정이 충돌 이후에 서버를 복구하기 위해서 필요하지 않다. 파일 명령어들은 멱등법칙을 지키고, 즉, 같은 명령어를 여러번 시도해도 반드시 같은 효과를 일으킨다. 멱등법칙을 달성하기 위해서, 모든 NFS 요청은 시퀀스 넘버를 가지고, 서버가 만약 요청이 중복되거나 잃은 것이 있는지 결정하게 허용한다.

우리가 언급한 클라이언트의 리스트를 유지하는 것은 서버의 statelessness를 위반하는 것처럼 보인다. 그러나, 이 리스트는 클라이언트 또는 서버의 정확한 명령어에는 중요하지 않고 따라서 그것은 서버 충돌 이후에 복구될 필요가 없다. 결과적으로, 그것은 비일관적인 데이터를 포함하고 힌트로만 취급된다.

stateless 서버 철학의 내포와 RPC의 공시성은 수정된 데이터가 결과가 클라이언트에게 리턴되기 전에 반드시 서버의 디스크에 커밋되어야한다는 것이다. 즉, 클라이언트는 쓰기 블럭을 캐시하지만, 그것이 서버로 플러시하면, 그것은 그들이 서버의 디스크에 닿았다고 가정한다. 서버는 반드시 모든 NFS 데이터를 동시에 써야한다. 그러므로, 서버 충돌과 회복은 클라이언트에게 보이지 않고, 서버가 클라이언트를 위해 관리하는 모든 블럭은 온전할 것이다. 결과적인 성능 패널티는 클 것인데, 왜냐하면 캐싱의 장점을 잃기 때문이다. 성능은 그것의 비휘발성 캐시에 저장소를 사용함으로서 증가시킬 수 있다. 디스크 컨트롤러는 디스크 쓰기를 쓰기가 비휘발성 캐시에 저장될때 인지한다. 본질적으로, 호스트는 빠른 동기적 쓰기를 본다. 이런 블럭은 온전함을 시스템 충돌 이후에도 온전함을 유지하고 디스크에 주기적으로 안정적인 저장소를 쓴다.

단일 NFS 쓰기 프로시저 콜은 아토믹하다고 보장되고 같은 파일에 쓰기 콜이 섞인다. NFS 프로토콜은, 동시성 컨트롤 메커니즘을 제공하지 않는다. `write()` 시스템 콜은 몇가지 RPC 쓰기로 쪼개지고, 왜냐하면 각 NFS 쓰기 또는 읽기 콜은 최대 8KB의 데이터와 1500바이트 한계의 UDP 패킷을 가진다. 결과적으로 같은 원격 파일에 쓰는 유저들은 그들의 데이터가 섞인다. 락 관리가 내포적으로 stateful함으로, NFS밖의 서비스는 반드시 락킹을 제공해야한다. 유저는 NFS의 스코프 밖에서 공유 파일 메커니즘을 이용해서 절차적인 접근을 하도록 권장한다.

NFS는 VFS를 통해서 운영체제에 통합된다. 아키텍처의 설명으로, 이미 열린 리모트 파일에서 명령어가 어떻게 조절되는지 확인해보겠다. 클라이언트가 일반적인 시스템콜으로 명령어를 시작한다. 운영체제 레이어는 VF 명령어에 적절한 vnode를 매핑한다. VFS 레이어는 파일을 원거리의 것으로 식별하고 적절한 NFS 프로시저를 실행한다. RPC 콜이 NFS 서비스 레이어에 만들어진다. 이 콜은 다시 원격 시스템의 VFS 레이어에 삽입된다. 이 콜은 이것이 로컬이란 것을 찾고 적절한 파일 시스템 명령어를 실행한다. 이 경로는 결과로 리턴된다. 이 아키텍처의 장점은 클라이언트와 서버가 같다는 것이다. 그러므로, 머신은 클라이언트이자 서버가 될 수 있다. 각 서버에서의 실제 서비스는 커널 스레드에 의해서 실행된다.

### 15.8.4 Path Name Translation

**Path Name Translation**은 `usr/local/dir1/file.txt`을 분리된 디렉토리 엔트리 또는 구성요소로 파싱하는 것이다 : (1) usr, (2) local, (3) dir1. 패스 이름 번역은 패스를 컴포넌트 이름으로 분리하고 분리된 NFS 룩업 콜을 모든 구성요소의 페어와 디렉토리 vnode에 실행하는 것이다. 한번 마운트 포인트가 크로스되면, 모든 컴포넌트 룩업은 서버에 분리된 RPC를 실행한다. 이 비싼 패스이름 순회 구조는 필요한데, 왜냐하면 각 클라이언트의 논리적 이름 공간은 유일하고 클라이언트가 실행한 마운트에 의해서 지시된다. 이것은 서버에게 패스이름을 주는것보다 훨씬 효율적이고 한번 마운트 포인트를 마주한 타겟 브이노드를 받는다. 어느 점에서든, 특정 클라이언트를 위한 마운트 포인트가 존재한다.

룩업이 빨라서, 디렉토리 이름 룩업 캐시는 원거리 디렉토리 이름을 위한 vnode를 캐시한다. 이 캐시는 같은 패스 이름에 대해서 빠른 레퍼런스를 제공한다. NFS의 구현이 원격 파일 시스템을 이미 존재하는 원격 파일 시스템의 탑에 마운트를 허용한다. 클라이언트가 연속적인 마운트를 가지면, 더 많은 서버가 패스 순회에 포함된다. 그러나, 클라이언트가 어떤 파일 시스템에서 서버가 마운트된지 알면, 클라이언트는 마운트된 디렉토리 대신 존재하는 디렉토리를 본다.

### 15.8.5 Remote Operations

파일을 열고 닫는 예외로, 일반적인 유닉스 시스템 콜을 위한 파일 명령어와 NFS 프로토콜 RPC 사이에 관계가 있다. 그러므로, 원격 파일 명령어는 상응하는 RPC로 즉시 번역된다. 개념적으로 NFS는 원격 서비스 패러다임을 향유한다. 그러나 실전에서는, 버퍼링과 캐싱 기술이 성능을 만족한다. 어떤 직접적인 연관이 리모트 명령어와 RPC 사이에는 없다. 대신에, 파일 블럭과 파일 성질들은 RPC에 의해서 실행되고 지역적으로 캐시된다. 미래의 원격 명령어는 캐시된 데이터를 사용한다.

두가지 캐시가 있다 : file attribute(inode information) cache와 file block cache. 파일이 열리면, 커널은 원격 서버로부터 캐시된 성질을 실행할지 다시 갱신할지 결정한다. 캐시된 파일 블럭은 오직 적절한 캐시된 성질이 있을때만 사요오딘다. 성질 캐시는 서버로부터 새로운 성질이 올때마다 업데이트된다. 캐시된 성질은 60초 이후에 자동으로 버려진다. read-ahead와 delayed write 기술은 서버와 클라이언트 사이에서 사용된다. 클라이언트는 딜레이된 라이트 블럭을 서버가 그 데이터가 디스크에 쓰였다고 하기전까지는 해제하지 않는다. 딜레이된 쓰기는 파일이 동시에 열려도 다시 얻는다.

