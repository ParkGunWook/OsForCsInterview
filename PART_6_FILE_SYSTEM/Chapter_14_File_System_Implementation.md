## What we gonna learn

13장에서 보았듯이, 파일 시스템은 온라인 저장소와 데이터와 프로그램을 포함한 파일컨텐츠에 접근을 위한 메커니즘을 제공한다. 파일 시스템들은 보통 2차 저장소에 영구적으로 저장되고, 큰 양의 데이터를 잡기위해서 디자인되었다. 이번 장은 파일 저장소와 2차저장소의 접근을 둘러싼 주요한 문제를 고려하겠다. 우리는 파일 사용을 구조화하고, 저장 공간을 할당하고, 풀린 공간을 회복하고, 데이터의 위치를 추적하고, 운영체제에서 2차저장소로의 인터페이스를 하는 몇가지 방법을 살펴보았다. 성능 문제는 전반적으로 살펴보겠다.

일반적인 운영체제는 다양한 파일시스템을 제공한다. 추가적으로, 많은 운영체제는 관리자 또는 유저가 파일 시스템을 추가하도록 허용한다. 파일 시스템은 기능, 성능, 신뢰성, 디자인 목적을 포함한 다양한 측면을 가진다. 예를 들어서, 임시 파일 시스템은 빠른 저장소와 비영구 파일의 회수로 사용되고, 일반적인 2차 저장소 파일 시스템은 신뢰성과 기능을 위해서 성능을 희생한다. 우리가 운영체제의 공부로 보았듯이, 다양한 선택과 변형이 있고 다양한 도전으로 채운다. 이 장에서, 우리는 일반적인 분모에 집중하겠다.

## Objectives

- 로컬 파일 시스템과 디렉토리 구조를 구현하는 상세한 방법을 설명하겠다.
- 블럭 할당과 자유 블럭 알고리즘과 트레이드오프를 살펴보겠다.
- 파일 시스템 효율성과 성능 이슈를 살펴보겠다.
- 파일 시스템 실패로부터의 회복을 보겠다. 
- WAFL 파일 시스템을 예시로 보겠다.

## 14.1 파일 시스템 구조

디스크들은 파일 시스템을 유지하는 2차 저장소의 대부분을 제공한다. 두가지 성질이 그들을 이 목적을 편하게 달성하기 위해서 있다.

1. 디스크는 한 장소에서 다시 쓰일 수 있다. 그것은 디스크로부터 블럭을 읽고, 블럭을 수정하고, 같은 블럭에 쓸 수 있다.
2. 디스크는 그것을 포함하는 어떠한 정보에도 직접 접근할 수 있다. 그러므로, 어떠한 파일에 연속적으로 또는 무작위로 접근하는 것은 단순하고, 한 파일에서 다른 파일로 변경하는 것은 읽기-쓰기 헤드를 움직이는 것과 미디어가 회전하기를 기다리는 것을 필요로한다.

NVM 디바이스들은 파일 저장소로 점점 상용화되어가고 있고 그러므로 파일 시스템의 위치이다. 그들은 그들이 제자리에 다시 쓰일 수 없고 다른 성능 성질을 가지고있다는 차이가 존재한다. 우리는 디스크와 NVM 구조는 11장에서 다루었다.

I/O 효율을 증가시키기 위해서, 메모리와 대량 저장소 사이의 I/O 전송은 **block**단위로 수행된다. 하드 디스크 드라이브의 각 블럭은 하나 또는 여러개의 섹터를 가진다. 디스크 드라이브에 따라서, 섹터 사이즈는 보통 512 바이트 또는 4096바이트이다. NVM 디바이스는 보통 4096 바이트의 블럭을 가지고, 전송 메서드들은 디스크 드라이브에 쓰이는 것과 비슷하다.

**File systems**는 저장되고, 위치되고, 복구될 데이터를 허용함으로서 저장 디바이스에 효율적이고 간편한 접근을 제공한다. 파일 시스템은 2가지 꽤 다른 디자인 문제를 취한다. 첫번째 문제는 파일 시스템이 유저에게 어떻게 보여주는 가이다. 이 태스크는 파일과 그것의 성질, 파일에 허용된 명령어와 파일을 구성하는 디렉토리 구조를 정의하는 것을 포함한다. 두번째 문제는 논리적 파일 시스템 위에 물리적 2차 저장소 디바이스를 매핑하는 데이터 구조와 알고리즘을 만드는 것이다.

파일 시스템은 다른 레벨로 구성되어있다. 각 레벨은 낮은 단계의 기능을 이용하고 높은 레벨에 의해서 새롭게 사용되는 기능을 생성한다.

**I/O control** 레벨은 디바이스 드라이브를 포함하고 메인 메모리와 디스크 시스템 사이의 정보를 전송한다. 디바이스 드라이버는 번역기로서 생각될 수 있다. 그것의 인풋은 `retrieve block 123`과 같이 높은 레벨 커맨드로 구성되어있다. 그것의 아웃풋은 로우 레벨이고, 하드웨어 컨트롤러에 의해서 사용되는 I/O 디바이스에서 시스템의 나머지 하드웨어에 인터페이스하는 특정 명령어를 포함한다. 디바이스 드라이버는 보통 특정 비트 패턴을 컨트롤러에게 어떤 디바이스 위치에서 행동하고 어떤 행동을 취할지 알려주는 I/O 컨트롤러의 메모리안의 특별한 위치에 쓴다. 디바이스 드라이버와 I/O 기반 구조는 12장에서 다루었다.

**basic file system**은 적절한 디바이스 드라이버가 저장 디바이스에 블럭을 읽고 쓰는 제너릭 커맨드를 발행한다. 논리 블럭 주소에 기반한 드라이버에 명령을 발행한다. 그것은 또한 I/O 요청 스케쥴링을 포함한다. 이 층은 다양한 파일시스템, 디렉토리, 데이터 블럭을 유지하는 메모리 버퍼와 캐시를 관리한다. 버퍼안의 블럭은 대규모 저장소 블럭의 전송이 일어나기전에 할당된다. 버퍼가 가득차면, 버퍼 매니저는 반드시 많은 버퍼 메모리를 찾거나 요청된 I/O가 완료되기 위해서 버퍼 공간을 해제해야한다. 캐시들은 자주 사용되는 파일 시스템 메타데이터를 저장해서 성능을 향상시키고, 그들의 컨텐츠를 관리하는 것은 최적의 시스템 성능에 중요하다.

**file organization module**은 파일과 그들의 논리 블럭을 알고 있다. 각 파일의 논리 블럭들은 0부터 N까지 번호를 가진다. 파일 구성 모듈은 할당되지 않은 블럭을 추적하고 요청이 있을때 이런 블럭을 제공하는 여유 공간 매니저를 포함한다.

마지막으로, **logical file system**은 메타 데이터 정보를 관리한다. 메타 데이터는 실제 데이터를 제외한 모든 파일 시스템의 구조를 포함한다. 논리 파일 시스템은 후에 필요한 심벌릭 파일 네임과 함께 파일 구성 모듈을 제공하기 위해서 디렉토리 구조를 관리한다. 그것은 파일 컨트롤 블럭을 통해서 파일 구조를 유지한다. **File control block(FCB)**는 파일에 관한 소유권, 허가, 파일 컨텐츠의 위치같은 정보를 포함한다. 논리 파일 시스템은 또한 13장과 17장에서 언급할 보호에도 책임이 있다.

계층 구조가 파일 시스템 구현을 위해서 사용되면, 코드의 복제는 최소화된다. I/O 컨트롤과 가끔 기본 파일 시스템 코드는 다양한 파일 시스템을 위해서 사용될 수 있다. 각 파일 시스템은 그것의 논리 파일 시스템과 파일 구성 모듈을 가진다. 불행히도, 레이어링은 더 많은 운영체제 오버헤드를 초래해서 성능을 낮출 수 있다. 레이어링의 사용, 얼마나 많은 레이어를 사용하고 각 레이어가 무엇을 할지를 포함하는 결정은 새로운 시스템을 디자인하는데에 큰 도전과제이다.

오늘날의 많은 파일 시스템과 운영체제는 하나이상을 지원한다. 예를 들어서, 대부분의 CD-ROMs들은 CD-ROM 생산자들이 쓰는 표준 ISO 9660 포맷으로 쓰였다. 삭제 가능한 미디어 파일 시스템에서는, 각 운영 체제는 하나 이상의 파일 시스템을 가진다. 유닉스는 버클리 Fast File System(FFS)를 기반으로한 **UNIX file system(UFS)**를 사용한다. 윈도우는 FAT, FAT32, NTFS의 디스크 파일 시스템 포맷을 지원한다. 비록 리눅스는 130개의 다른 파일 시스템을 지원하지만, 표준 리눅스 파일 시스템은 **extended file system**이다. 서버에서의 분산 파일 시스템은 네트워크를 통해서 마운트된 하나 이상의 클라이언트 컴퓨터이다.

파일 시스템 연구는 운영체제 디자인과 구현의 영역에서 지속된다. 구글은 큰 디스크를 걸친 많은 클라이언트로부터의 높은 성능의 접근을 포함한 회사의 특정 저장소와 복귀 능력을 충족하는 자신의 파일 시스템을 만들었다. 다른 흥미로운 프로젝트는 FUSE 파일 시스템인데, 커널 레벨 코드보다는 유저 레벨에서 파일 시스템을 구현하고 실행해서 파일 시스템 개발과 사용에 유연성을 제공한다. FUSE를 사용하면, 유저는 운영체제에 새로운 파일 시스템을 추가할 수 있고 그녀의 파일을 관리하기 위해서 파일 시스템을 사용할 수 있다.

## 14.2 File System Operations

13.1.2에서 보앗듯이, 운영체제는 프로세스를 위해 파일 컨텐츠로의 접근을 요청하는 `open()`과 `close()` 시스템 콜을 구현했다. 이 절에서, 우리는 파일 시스템 명령어를 구현하기 위한 구조와 명령어를 뒤지겠다.

### 14.2.1 Overview

몇가지 온 스토리지와 인 메모리 구조는 파일 시스템을 구현하기 위해서 사용된다. 이런 구조들은 운영체제와 파일 시스템에 따라서 다양하지만, 몇몇 일반적인 원리가 적용된다.

저장소에서, 파일 시스템은 저장된 운영체제를 어떻게 부트할지, 블럭의 전체 수, 여유 블럭의 위치와 수, 디렉토리 구조, 개인 파일에 대한 정보가 담겨 있다. 이런 구조들이 이 장의 전반에 담겨 있다. 여기서는 간단하게 그들을 설명하겠다.

- **Boot control Block**은 시스템이 운영체제를 부팅할 정보를 포함한다. 만약 디스크가 운영체제를 포함하지 않으면, 이 블럭은 비어진다. 그것은 보통 볼륨의 첫 블럭이다. UFS에서 그것은 **boot block**이라고 불린다. NTFS에서는 **partition boot sector**라고 불린다.
- **volume control block**은 볼륨의 블럭의 수, 블럭의 사이즈, 여유 블럭 수, 여유 블럭 포인터와 여유 FCB 카운트와 FCB 포인터 같은 볼륨 상세를 포함한다. UFS에서 **super block**이라하고 NTFS에서 **master file table**이라고 한다.
- 디렉 토리 구조는 파일을 구성하기 위해서 사용된다. UFS에서, 이것은 파일 이름과 연관된 inode 수를 포함한다. NTFS에서는, master file table에 저장한다.
- 파일당 FCB는 파일에 관한 많은 상세정보를 포함한다. 그것은 디렉토리 엔트리에 관련된 유일 식별자수를 가진다. NTFS에서, 이 정보는 마스터 파일 테이블에 사용되고, 관계형 데이터 구조를 파일당 열로 가진다.

인 메모리 정보는 파일 시스템 관리와 성능 향상을 캐싱을 통해서 사용한다. 데이터들은 마운트 시간에 로드되고, 파일 시스템 명령 중에 업데이트되고 디스마운트에 버려진다. 몇가지 타입의 구조가 포함된다.

- 인 메모리 **mount table**은 각 마운트된 볼륨에 관한 정보를 가진다.
- 인 메모리 디렉토리 구조 캐시는 최근에 접근한 디렉토리의 정보를 가진다.
- **system-wide open file table**은 각 오픈 파일의 FCB의 복사본을 포함한다.
- **per process open file table**은 시스템 와이드 오픈 파일 테이블의 적절한 엔트리 포인터를 가진다. 프로세스가 오픈한 모든 파일이다.
- 버퍼는 그들이 파일 시스템에서 읽고 쓸때 파일 시스템 블럭을 가진다.

새로운 파일을 생성하기 위해서, 프로세스는 논리 파일 시스템을 콜한다. 논리적 파일 시스템은 디렉토리 구조의 포맷을 안다. 새로운 파일을 만들기 위해서, 그것은 새로운 FCB를 할당한다.(대안으로, 만약 파일 시스템 구현이 모든 FCB를 파일 시스템 생성시간에 만들면, FCB는 여유 FCB의 집합에 할당된다.) 시스템은 그리고 적절한 디렉토리를 읽는다.

몇몇 운영체제는 디렉토리를 파일과 같이 취급하고, "type"을 디렉토리로 가르킨다. 다른 운영체제는 파일과 디렉토리에 분리된 시스템 콜을 구현하고 파일로부터 분리된 엔티티를 디렉토리로 취급한다. 구조적 이슈가 더 클수록, 논리 파일 시스템은 저장소 블록 위치로 디렉토리 I/O를 맵하기 위해서 파일 구성 모듈을 콜한다. 

### 14.2.2 Usage

이제 파일은 생성되었고, 그것은 I/O를 위해서 사용될 수 있다. 먼저, 그것은 열려야한다. `open()` 콜은 파일 네임을 논리 파일 시스템에 패스한다. `open()` 시스템 콜은 먼저 시스템 와이드 오픈 파일을 이용해서 다른 프로세스가 접근중인지 확인한다. 만약 그렇다면, per process open file table 엔트리는 존재하는 시스템 와이드 오픈 파일 테이블을 가르키고 있다. 이 알고리즘은 오버헤드를 아끼게한다. 만약 파일이 미리 열리지 않았으면, 디렉 토리 구조는 주어진 파일 이름을 위해서 검색된다. 디렉토리 구조의 일부는 디렉토리 명령어를 가속하기 위해서 메모리에 캐시된다. 파일을 찾으면, FCB는 메모리의 시스템 와이드 오픈 파일 테이블에 복사된다. 이 테이블은 FCB를 저장할 뿐만 아니라 파일을 연 프로세스의 수 또한 추적한다.

다음으로, 엔트리는 per process open file table에서 시스템 와이드 오픈 파일 테이블과 다른 영역의 엔트리를 가르키는 포인터로 만들어진다. 이런 다른 필드들은 파일의 현재위치를 알리는 포인터와 어떤 파일이 열릴떄의 엑세스 모드를 포함한다. `open()`콜은 적절한 엔트리의 포인터를 리턴한다. 모든 파일 명령어들은 포인터를 통해서 수행된다. 파일 이름은 오픈 파일 테이블의 일부가 아니고, 시스템은 한번 적절한 FCB가 디스크에 위치될때 필요가 없다. 엔트리에 의해서 주어진 이름은 다양한데, 유닉스는 **file descriptor** 윈도우는 **file handle**이라고 한다.

프로세스가 파일을 닫으면, per process table 엔트리는 사라지고, 시스템 와이드 엔트리의 오픈 카운트는 줄어든다. 파일을 열었던 모든 유저가 닫으면, 업데이트된 메타데이터들은 디스크 베이스의 디렉토리 구조에 복사되고 시스템 와이드 오픈 파일 테이블은 제거된다.

파일 시스템의 캐싱또한 간과하면 안된다. 대부분의 운영체제들은 오픈 파일에 대한 모든 정보를포함한다. BSD 유닉스 시스템은 디스크 I/O가저장가능할떄마다 캐시를 사용한다. 그것의 캐시 히트 레이트는 85퍼센트가량된다. BSD 유닉스는 Appendix C에서 더 다룬다.

## 14.3 디렉토리 구현

디렉토리 할당과 디렉토리 관리 알고리즘의 선택은 효율성, 성능, 파일 시스템의 신뢰성에 큰 영향을 준다. 이 절에서, 우리는 이 알고리즘을 선택하는 것에 대한 트레이드 오프를 보겠다.

### 14.3.1 리니어 리스트

디렉토리를 구현하는 가장 간단한 메서드는 데이터 블럭을 향한 포인터를 가진 파일이름의 리니어 리스트를 사용하는 것이다. 이 메서드는 구현은 간단하지만 실행에 시간을 소모한다. 새로운 파일을 생성하기 위해서, 우리는 같은 파일의 이름이 디렉토리에 없는 것을 먼저 찾아야한다. 그리고, 우리는 디렉토리의 끝에 새로운 파일을 추가한다. 파일을 삭제하기 위해서, 우리는 파일 이름을 디렉토리에서 찾고 할당된 공간을 해제한다. 디렉토리 엔트리를 재사용하기 위해서, 우리는 몇가지 일을 할 수 있다. 우리는 엔트리를 비사용이라고 마크하거나, 우리는 여유 디렉토리 엔트리의 리스트를 붙인다. 3번째 대안은 여유로워진 위치에 디렉토리의 마지막 엔트리를 복사하는 것이다. 그리고 디렉토리의 길이를 줄이는 것이다. 

리니어 리스트의 단점은 파일을 찾는 것이 선형 검색을 필요로하는 것이다. 디렉토리 정보는 자주 사용되고, 유저들은 만약 접근이 너무 느리면 통보한다. 실제로, 많은 운영체제가 가장 최근에 사용한 디렉토리 정보를 저장하는 소프트웨어 캐시를 구현한다. 캐시 히트는 2차 저장소로부터 다시 읽는 것을 피한다. 정렬된 리스트는 이진 검색을 허용하고 평균 검색 시간을 감소시킨다. 그러나, 리스트를 정렬된채로 두는 것은 파일의 생성과 삭제를 복잡하게 하고, 우리는 정렬된 디렉토리를 유지하기 위해서 디렉토리 정보를 더 써야한다. 정렬 트리의 장점은 정렬된 디렉토리 리스팅은 분리된 정렬 과정이 생략된다.

### 14.3.2 해시 테이블

파일 디렉토리에 사용되는 다른 데이터 구조는 해시 테이블이다. 여기서, 리니어 리스트는 디렉토리 엔트리를 저장하지만, 해시 테이블 구조도 사용한다. 해시 테이블은 파일이름으로부터 계산된 값을 가지고 파일 이름의 포인터를 리턴한다. 그러므로, 이것은 디렉토리 검색시간을 최대로 줄일 수 있다. 삽입, 삭제 또한 직관적이고, 몇몇 영역은 충돌을 일으킬수있긴하다.

해시테이블의 주요한 어려움은 그것의 일반적인 고정사이즈와 그사이즈에 따른 해시 함수의 의존성이다. 예를 들어서, 우리가 64개의 엔트리를 가진 해시 테이블을 만들었다. 해시 함수는 파일 이름은 0~63으로 정수화했다. 만약 우리가 65를 찾으면, 우리는 디렉토리 해시 테이블을 크게하고 128 엔트리가 된다. 결과적으로 0~127을 위한 새로운 해시 함수가 필요하고 존재하는 디렉토리 엔트리를 새로운 해시 함수 값을 반영하기 위해서 재구성해야한다.

대안으로, 우리는 체인드 오버 플로 해시테이블을 사용한다. 각 해시 엔트리는 개인 값대신 링크드 리스트를 가지고 링크드 리스트에 새로운 엔트리를 추가해서 충돌을 해결한다. 룩업은 조금 느려지겠지만, 왜냐하면 링크드 시스트의 충돌 테이블 엔트리를 선형으로 찾아야기 때문이다. 그러나, 이 메서드는 여전히 전체 디렉토리를 선형검색하는 것보다 빠르다.

## 14.4 Allocation Methods

2차 저장소의 직접 연결 환경은 우리에게 파일의 구현에 대한 유연성을 제공한다. 대부분의 모든 경우에, 많은 파일들은 같은 디바이스에 저장된다. 주요한 문제는 어떻게 이런 파일들을 공간에 할당해서 저장 공간이 효과적으로 활용되고 파일들이 빠르게 접근되느냐이다. 3가지 주요 메서드가 널리 쓰이고 있다. Contiguous, Linked, Indexed이다. 각 메서드는 장단이 있다. 비록 몇몇 운영체제는 3가지를 모두 지원하지만, 파일 시스템 타입안에 모든 파일이 하나의 메서드만 쓰는 것이 일반적이다.

### 14.4.1 Contiguous Allocation

**Contiguous Allocation**은 각 파일이 연속 블럭의 집합을 차지하는 것을 필요로한다. 디바이스 주소는 디바이스에 선형으로 정의된다. 이 순서로, 한가지 일이 디바이스에 접근한다고 가정하면, 블럭 b이후에 블럭 b+1에 접근하는 것은 헤드 움직임이 필요가 없다. 헤드가 움직일 필요가 생기면, 헤드는 한 트랙에서 다음으로 움직일 필요만 있다. 그러므로, HDD에서, 디스크 탐색의 수는 최소이고, 탐색 시간은 오직 탐색이 마지막에 필요할때뿐이다.

파일의 연속 할당은 첫 블럭의 주소와 파일 길이이다. 만약 파일이 n 블럭의 길이이고 b에서 시작하면 b~b+n-1의 블럭을 차지한다. 각 파일의 디렉토리 엔트리는 시작 블럭의 주소와 파일에 할당된 길이를 가르킨다. 연속 할당은 구현이 쉽지만 제한이 있고, 그러므로 현대 파일 시스템에서는 사용되지 않는다.

연속적으로 할당된 파일의 접근은 간단하다. 연속 접근에서, 파일 시스템은 마지막에 할당된 블럭의 주소를 기억하고, 필요하면 다음 블럭을 읽는다. b에서 시작하는 파일의 i번째 블럭의 직접 접근은 즉시 블럭 b+i에 하면된다. 그러므로, 연속적이고 직접 접근은 연속할당에 의해서 지원이 가능하다.

연속적인 할당은 몇가지 문제를 가진다. 한가지 어려움은 새로운 파일을 위한 공간을 찾는 것이다. 여유 공간을 관리하는 시스템은 어떻게 이 태스크가 완료되는지 결정한다. 이런 관리 시스템은 14.5절에서 언급하겠다. 어느 관리 시스템이 사용되어도, 다른 것들보다는 느리다.

연속 할당 문제는 9.2절에서 언급한 어떻게 요청한 n의 크기를 여유 홀의 리스트로부터 배정하는 **dynamic storage allocation**문제와도 관련이 있다. first fit과 best fit은 가용 홀의 집합으로부터 여유 홀을 선택하는 가장 일반적인 전략이다. 두가지 fit 전략이 worst fit보다 시간과 공간 활용도에서 효율적이다. 저장소 효율성은 둘이 비슷하지만, 시간은 first fit이 훨씬 빠르다.

모든 이런 알고리즘은 **external fragmentation**에 고통받는다. 파일들이 할당되고 삭제되면서, 여유 공간은 작은 조각으로 부서진다. 외부 단편화는 여유 공간이 청크로 부서지면서 존재한다. 이 문제는 커다란 연속 청크가 요청에 만족하지 못할때 생긴다. 저장소는 많은 홀로 단편화되고, 데이터를 저장하기에 충분하지 않게된다. 전체 디스크의 양과 평균 파일 사이즈에 따라서, 외부단편화는 주요하거나 주요하지 않은 문제가 된다.

외부 단편화에 대한 엄청난 양의 저장소의 로스를 방지하는 한가지 전략은 전체 파일 시스템을 다른 디바이스에 복사하는 것이다. 원본 디바이스는 완벽히 여유로워지고, 한가지 큰 연속 여유 공간을 만드는 것이다. 우리는 그러고 한개의 큰 홀로부터 여유 공간을 할당함으로서 파일을 다시 원본 디바이스에 백업한다. 이 구조는 한개의 연속 공간으로 모든 여유 공간을 압축한다. 이 압축의 코스트는 시간이고, 그러나, 코스트는 큰 저장 디바이스에 대해서는 특히 더 높다. 이런 디바이스들을 압축하는 것은 시간을 가져가고 매주 기초에 필요하다. 몇몇 시스템들은 이 함수가 **off-line**에서 일어나게 하고 파일 시스템은 언마운트 해둔다. **down time**동안에, 일반적인 시스템 명령어는 허용되지 않고, 그래서 압축은 생산 기계에서 모든 코스트를 피한다. 대부분의 시스템은 단편화 해제가 일반적인 시스템 명령어에서 **on-line**으로 수행되기에 성능 페널티는 상당하다.

다른 연속 할당의 문제는 파일에 공간이 얼마나 필요하느냐이다. 파일이 생성되면, 필요한 공간의 전체 양은 반드시 찾아지고 할당되어야한다. 어떻게 생성자(프로그램 또는 사람)이 생성될 파일의 사이즈를 알까? 몇몇 경우에, 이 결정은 꽤 간단하다. 일반적으로, 아웃풋 파일의 사이즈는 측정하기 어렵다.

만약 우리가 파일에 너무나도 작은 공간을 할당하면, 우리는 파일이 확장될 수 없는 것을 찾는다. 특히 best-fit 할당 전략에서, 파일의 양쪽은 사용중이다. 따라서, 우리는 공간보다 크게 할당할 수 없다. 두가지 가능성이 존재한다. 먼저, 유저 프로그램이 종료되고, 적절한 에러 메시지가 주어진다. 유저는 반드시 더 많은 공간을 할당하고 다시 프로그램을 실행한다. 이런 반복된 실행은 비싸다. 그들을 예방하기 위해서, 유저는 필요한 공간의 양을 과대측정하고, 엄청난 공간을 낭비한다. 다른 가능성은 큰 홀을 찾는 것인고, 새로운 공간에 파일을 복사하고, 이전 공간을 해제한다. 이 일련의 행동은 공간이 가용할떄까지 반복할 것이고, 시간 낭비이다. 유저가 필요한 것은 결코 무슨 일이 일어나는지 정확히 공지하지 않는다. 시스템은 더더욱 느려져도 문제 때문에 계속 진행한다.

파일에 필요한 전체 공간이 미리 알려져도, 예비 할당은 비효율적이다. 시간에 따라서 점점 자라는 파일은 반드시 그것의 최종 사이즈에 적절하게 할당되어야하고, 긴 시간동안 공간은 사용되지 않을 수 있다. 파일은 그러므로 큰 내부 단편화를 가지게 된다.

이런 장애를 최소화하기 위해서, 운영체제는 수정된 연속할당 구조를 쓴다. 여기서, 공간의 연속적인 청크는 초기에 할당된다. 그리고, 만약 양이 충분히 크지 않다고 드러나면, **extent**라고 알려진, 연속 공간의 다른 청크가 추가된다. 파일 블럭의 위치는 위치와 블럭 카운트에 더불어서 다음 extent의 첫번째 링크가 더해진다. 몇몇 시스템에서, 파일의 소유자는 extent 사이즈를 지정할 수 있지만, 만약 소유자가 틀렸다면 이 세팅은 비효율을 결과로 만든다. 내부 단편화는 여전히 extent가 크면 문제이고 외부 단편화는 할당되고 해제되는 다양한 크기의 extent때문에 생긴다. 상업적인 Symantic Veritas file system은 성능을 최적화하기 위해서 extent를 사용한다. Veritas는 UNIX UFS에 대한 고성능 대체이다.

### 14.4.2 Linked Allocation

**Linked Allocation**은 연속할당의 모든 문제를 해결한다. 링크드 할당, 각 파일은 저장소 블럭의 링크드 리스트이다. 디렉토리는 파일의 첫번쨰와 마지막 블럭을 가르키는 포인터를 가진다. 예를 들어서, 5개의 블럭의 파일이 블럭 9에서 시작하고 16, 1, 10, 25로 거친다. 각 블럭은 다음 블럭의 포인터를 포함한다. 이런 포인터들은 유저에게 가용하지 않다. 그러므로, 만약 각 블럭이 512 바이트이면, 블럭 주소는 4바이트를 필요하면, 유저는 508바이트의 블럭을 본다.

새로운 파일을 만들기 위해서, 우리는 단순히 디렉토리에 새로운 엔트리를 만든다. 링크드 할당에서, 각 디렉토리 엔트리는 파일의 첫번째 블럭을 향하는 포인터를 가진다. 이 포인터는 null로 초기화되고 빈 파일을 의미한다. 사이즈 필드도 0으로 설정된다. 파일로의 쓰기는 여유 공간 관리 시스템이 여유 블럭을 찾게 하고 이 새로운 블럭은 쓰이고 파일의 끝에 연결된다. 파일을 읽기 위해서, 우리는 단순히 블럭에서 블럭으로 따르는 포인터를 통해서 블럭을 읽는다. 링크드 할당에서는 외부단편화는 존재하지 않고, 여유 공간 리스트의 프리 블럭은 요청을 만족하는데에 사용된다. 파일의 사이즈는 파일이 만들어 졌을때 선언될 필요가 없다. 파일은 여유 블럭이 가용해지는만큼 계속 자란다. 결과적으로, 그것은 결코 디스크 공간을 압축할 필요가 없다.

링크드 할당은 단점이 존재한다. 주요한 문제는 그것은 오직 연속적 파일 엑세스 파일에 효과적으로 사용된다는 것이다. 파일의 i번째 블럭을 찾으려면, 우리는 파일의 시작부터 i번쨰 블럭에 도달할때까지 따라가야한다. 각 접근은 저장소 읽기를 필요하고, HDD 탐색을 필요로한다. 결과적으로, 링크드 리스트 할당 파일은 직접 접근을 지원하기에는 불충분하다.

다른 단점은 포인터에 필요한 공간이다. 만약 포인터가 512 바이트 블럭에서 4바이트를 요구하면, 0.78 퍼센트의 디스크가 포인터로 사용된다. 각 파일은 그것보다 공간이 더 필요해진다.

이 문제는 블럭을 **cluster**라 불리는 다중 블럭으로 수집하고, 블럭보다는 클러스터를 할당하는 것이다. 예를 들어서, 파일 시스템이 클러스터를 4개의 블럭으로 정의하고 오직 클러스터 유닛으로만 2차저장소를 작동한다. 포인터들은 파일 공간의 작은 퍼센트만 사용한다. 이 메서드는 논리에서 물리 블럭 매핑을 간단하게 하고 HDD 산출량을 늘리고(적은 디스크 해드 탐색이 필요하다.) 블럭 할당에 필요한 공간을 줄이고 여유 리스트 관리를 줄이게 허용한다. 이 접근의 코스트는 내부 단편화를 높이는데, 클러스터가 블럭이 부분적일떄보다 더 많이 남기 떄문에 공간이 더 낭비된다. 또한 랜덤 I/O 성능이 작은 데이터 요청에서 큰 데이터로 바뀌기 때문이다. 클러스터들은 많은 다른 알고리즘의 디스크 접근 시간을 향샹시키는데 필요로하고, 그래서 그들은 대부분의 파일 시스템에서 사용된다.

그러나 링크드 할당의 다른 문제는 신뢰성이다. 파일들이 모든 디바이스에 의해서 흩어진 포인터에 의해서 뭉쳐져 있고 만약 포인터가 손상되면 무슨 일이 일어날까? 운영체제 소프트웨어 또는 하드웨어 실패의 버그는 잘못된 포인터를 집는 것이다. 이 에러는 여유 공간 리스트에 링크되거나 다른 파일에 링크되게 할 수 있다. 한가지 부분적인 해결책은 더블리 링크드 리스트를 사용하거나, 다른 방법은 파일 이름을 저장하고 각 블럭의 상대적인 블럭 넘버를 저장하는 것이다. 그러나, 이런 구조들은 각 파일에 대해 더큰 오버헤드를 필요로한다.

링크드 할당의 중요한 변형은 **file allocation table(FAT)**를 사용하는 것이다. 이 간단하지만 효율적인 디스크 할당의 메서드는 MS-DOS 운영체제에 의해서 사용된다. 저장소의 섹션 시작은 테이블을 포함한다. 그 테이블은 각 블럭에 하나의 엔트리를 가지고 블럭 넘버로 인덱스 되어있다. 체인은 테이블 엔트리로서 eof 값을 가진 마지막 블럭에 도달할떄까지 지속된다. 사용되지 않은 블럭은 0을 가르킨다. 파일에 새로운 블럭을 할당하는 것은 간단하다. 값이 0인 테이블 엔트리를 찾고 이전 eof 값을 새로운 블럭의 주소로 교체하는 것이다. 0은 eof값으로 대체된다. 

FAT 할당 구조는 심지어 FAT가 캐시되는데도 심각한 디스크 탐색을 필요로한다. 디스크 헤드는 반드시 FAT를 읽기 위해서 볼륨의 시작으로 움직이고 블럭의 위치를 찾고, 블럭의 위치로 이동한다. 최악의 경우에, 두 이동은 각 블럭에서 일어난다. 이익은 랜덤 액세스 타임은 향상되는데, 왜냐하면 디스크 헤드는 FAT에서 정보를 읽음으로서 위치를 찾을 수 있기 떄문이다.

### 14.4.3 Indexed Allocation

링크드 할당은 외부 단편화와 사이즈 선언 문제를 해결했다. 그러나, FAT의 부재에서, 링크드 할당은 효율적인 direct access를 지원하지 못하는데, 블럭의 포인터가 디스크 전체에 널리 퍼져있고 반드시 순서대로 접근해야기 때문이다. **Indexed allocation**은 이 문제를 포인터를 하나의 **Index block**에 모으는 것으로 해결했다. 

각 파일은 인덱스 블럭을 가지고, 저장소 주소의 행렬이다. 인덱스 블럭의 i번째 엔트리는 파일의 i번째 블럭을 가르킨다. 디렉토리는 인덱스 블럭의 주소를 포함한다. i번째 블럭을 읽고 쓰려면, 우리는 i번쨰 블럭 엔트리의 포인터를 사용한다. 이 구조는 9.3에서의 페이징 전략과 유사하다. 

파일이 생기면, 인덱스 블럭의 모든 포인터는 null로 세팅된다. i번째 블럭이 처음 쓰이면, 여유 공간 관리자로부터 블럭을 획득하고, 그것의 주소는 i번째 인덱스 블럭 엔트리에 놓여진다.

인덱스 할당은 직접 접근을 지원하고, 외부단편화도 없는데, 왜냐하면 저장 디바이스의 여유 블럭은 더 많은 공간 요청을 만족한다. 인덱스드 할당은 낭비된 공간으로 부터 고통 받는다. 인덱스 블럭의 포인터 오버헤드는 링크드 리스트 오버헤드보다 크다. 우리가 오직 하나 또는 2개의 블럭의 파일을 가졌다고 가정하겠다. 링크드 할당에서, 우리는 블럭당 하나의 포인터를 잃지만, 인덱스드 할당에서, 전체 인덱스 블럭이 할당되어야하고, 오직 포인터는 한개나 두개인 것이다.

이 문제는 얼마나 인덱스 블럭이 커야하는지 생긴다. 모든 파일은 반드시 인덱스 블럭을 가지고, 그래서 우리는 인덱스 블럭을 최대한 적게 원한다. 만약 인덱스 블럭이 너무 작으면, 그것은 큰 파일에 대해서 충분한 포인터를 가지지 못하고 메커니즘은 이런 이슈를 다룰 수 있어야한다. 이 목적을 위한 메커니즘은 다음과 같다.

- Linked scheme : 인덱스 블럭은 보통 한개의 저장 블럭을 가진다. 그러므로, 그것은 자체적으로 읽고 쓰기가 가능하다. 큰 파일을 위해서, 우리는 여러개의 인덱스 블럭을 묶는다. 예를 들어서, 인덱스 블럭은 파일의 이름을 제공하는 작은 헤더를 포함하고 첫번째 100개의 디스크 블럭 주소를 가진다. 다음 주소는 null 또는 다른 인덱스 블럭의 포인터이다.
- Multilevel index : 링크드 대표의 변형은 1 레벨 인덱스 블럭을 2레벨 인덱스 블럭을 가르키기 위해서 사용하는 것이다. 그리고 파일 블럭을 가르킨다. 블럭에 접근하려면, 운영체제는 먼저 1레벨 인덱스를 2레벨 인덱스 블럭을 찾기 위해서 사용하고 원하는 데이터 블럭을 찾기위해서 블럭을 사용한다. 이 접근은 3번째 또는 4번쨰까지도 확장이 가능하다. 4096 바이트 블럭에서, 우리는 1024의 4바이트 포인터를 저장하고 2레벨 인덱스에서는 1048576 데이터 블럭과 4GB를 가진다.
- Combined scheme : 다른 대안은 유닉스 기반 파일 시스템인데, 파일의 아이노드의 인덱스 블럭을 15개를 가지는 것이다. 첫번째 12개의 포인터는 **direct blocks**를 가르킨다. 즉, 그들은 파일의 데이터를 포함하는 블럭의 주소를 포함한다. 그러므로, 작은 파일의 데이터는 분리된 인덱스 블럭을 필요로하지 않는다. 만약 블럭 사이즈가 4KB이면, 48KB의 데이터는 직접 접근된다. 나머지 3개의 블럭은 **indirect blocks**이다. 첫번째는 데이터가 아닌 데이터를 포함하는 블럭의 주소를 가지는 인덱스 블럭을 포함하는 **single indirect block**이다. 두번쨰는 실제 데이터 블럭으로의 포인터를 포함하는 블럭의 주소를 포함하는 블럭의 주소를 포함한 **double indirect block**이다. 다음은 **triple indirect block**이다. 
  
  이 메서드에서, 파일에 할당하는 블럭의 수는 운영체제가 다수 사용하는 4바이트 이상의 양을 초과해서 가르킬 수 있다. 32비트 파일 포인터는 2^32나 4GB에 달한다. 많은 유닉스와 리눅스 구현은 이제 64비트 파일을 지원하고 파일과 파일 시스템에 exbibytes(2^60)를 지원한다. ZFS 파일 시스템은 128비트 파일 포인터를 지원한다.

### 14.4.4 Performance

우리가 언급한 할당 메서드는 그들의 공간 효율성과 데이터 블럭 접근 시간에 따라서 다양하다. 두개 모두 적절한 메서드 또는 운영체제가 구현할 메서드를 선택하는 좋은 기준이다.

할당 메서드를 선택하기 전에, 우리는 어떻게 시스템이 사용될지를 결정해야한다. 연속 접근을 주로 사용하는 시스템은 랜덤 엑세스를 주로 사용하는 시스템과 다르게 메서드를 사용해야한다.

어느 엑세스의 타입이든, 연속 할당은 오직 한개의 엑세스로 블럭에 접근한다. 우리가 메모리에서 쉽게 파일의 첫 주소를 쉽게 보관하기에, 우리는 즉시 i번쨰 블럭의 주소를 계산하고 직접 읽을 수 있다.

링크드 할당에서, 우리는 메모리에 다음 블럭의 주소를 저장하고 그것을 바로 읽는다. 이 메서드는 연속 접근에는 좋지만, 직접 접근에는 i번쨰를 위해서 i번의 읽기가 필요하다. 이 문제는 왜 링크드 할당이 직접 엑세스를 필요로하는 앱에 사용되면 안되는 이유이다.

결과적으로 몇몇 시스템은 연속 할당으로 직접 파일 접근을 지원하고 연속 접근 파일은 링크드 할당을 이용한다. 이런 시스템에서, 접근의 종류는 파일이 생성되었을때 반드시 선언되어야한다. 연속 접근을 위해 생성된 파일은 링크드이고 직접 접근을 위해서 사용되면 안된다. 직접 접근을 위해 생성된 파일은 연속 할당이고 직접 접근과 연속 접근을 허용하지만, 그것의 최대길이가 생성되었을때 선언된다. 이런 경우에, 운영체제는 반드시 적절한 데이터 구조와 알고리즘으로 두 할당 메서드를 지원해야한다. 파일은 원하는 타입으로 원래 파일을 복사함으로서 바꿀수는 있다. 그리고 원본 파일은 지워지고 새로운 파일은 이름을 재설정한다.

인덱스드 할당은 더 복잡하다. 만약 인덱스 블럭이 이미 메모리에 있으면, 접근은 오직 직접적으로만 이루어진다. 그러나, 메모리에 인덱스 블럭을 유지하는 것은 상당한 공간을 필요로 한다. 만약 이 메모리 공간이 가용하지 않으면, 우리는 첫번째 인덱스 블럭을 읽고 원하는 데이터 블럭을 가진다. 2레벨 인덱스면, 2개의 인덱스 블럭을 읽어야한다. 정말로 큰 파일에서, 파일의 끝에서 블럭을 접근하는 것은 필요한 데이터 블럭을 읽기전에 필요로한다. 그러므로, 인덱스 할당의 성능은 인덱스 구조, 파일의 사이즈, 원하는 블럭의 위치에 달려있다. 

몇몇 운영체제는 인덱스드 할당을 연속할당과 합치는데, 작은 파일에는 연속할당을 쓰고 파일이 커지면 인덱스 할당으로 바꾸는 것이다. 대부분의 파일이 작기에, 연속할당이 효율적이고, 평균 성능은 꽤 좋다. 

많은 최적화가 사용된다. CPU 속도와 디스크 속도 사이의 차이때문에, 수천개의 추가 적인 명령어를 소수의 디스크 헤드 이동을 위해서 추가하는 것은 합리적이지 못하다. 더 나아가, 이 차이는 시간이 갈수록 커지고, 수천개의 명령어는 헤드 이동을 최적화하기 위해서 사용하기에 적절하다.

NVM 디바이스에서, 디스크 헤드 탐색은 없고, 그래서 다른 알고리즘과 최적화가 필요하다. 구식 알고리즘을 사용하는 것은 존재하지 않는 헤드 움직임을 피하는 것을 시도하는 것은 매우 비효율적이다. 존재하는 파일 시스템들은 수정되고 새로운 것이 NVM 저장 디바이스로부터 최대 효율을 얻기 위해서 생성된다. 이런 개발은 명령어 카운트를 줄이고 저장 디바이스에서 앱 엑세스의 전체 과정을 감소시켰다.

## 14.5 Free Space Management

스토리지 저장이 제한되면서, 가능하다면 우리는 삭제된 파일로부터 공간을 재활용할 필요가 있다. 여유 디스크 공간을 지키기 위해서, 시스템은 **free space list**를 유지한다. 여유 공간 리스트는 디렉토리 또는 파일이 할당되지 않은 모든 여유 디바이스 블럭을 저장한다. 파일을 생성하기 위해서, 우리는 여유 공간 리스트를 필요한 공간만큼 탐색하고 새로운 파일에 공간을 할당해야한다. 이 공간은 여유 공간 리스트로부터 제거된다. 파일이 삭제되면, 그것의 공간은 여유 공간 리스트에 추가된다. 여유 공간 리스트는 그것의 이름에도 불구하고, 리스트로 구현되지는 않는다.

### 14.5.1 Bit Vector

자주, 여유 공간 리스트는 **bitmap** 또는 **bit vector**로 구현된다. 각 블럭은 한 비트로 대표된다. 만약 블럭이 비었으면, 비트는 1이다. 만약 블럭이 할당되어 있으면, 비트는 0이다.

예를 들어서 블럭 2,3,4,5,8,9,10,11,12,13,17,18,25,26,27이 여유가 있고 나머지가 할당되었다. 그러면 여유 공간 비트맵은 다음과 같다.

`00111100111111000110000001110000....`

이 접근의 장점은 그것의 상대적 간편함과 첫번째 여유 블럭 또는 n개의 연속된 여유 블럭을 찾는 효율성이다. 공간을 할당할때 비트벡터를 사용하는 시스템에서 첫번째 여유블럭을 찾는 한가지 테크닉은 비트맵의 각 단어를 연속적으로 체크해서 0이 아닌 것을 확인한다. 첫번쨰 0이 아닌 워드는 첫번째 비트로 스캔되고, 첫번째 여유 블럭이다. 

블럭 넘버의 계산은 `(number of bits per word) * (number of 0-value words) + offset of first 1 bit`이다.

다시, 우리는 소프트웨어 기능을 사용하는 하드웨어 기능을 보겠다. 불행히도, 비트 벡터들은 전체 벡터가 메인 메모리에 보관됨에도 비효율적이다. 메인 메모리에 보관하는 것은 작은 디바이스에는 가능하지만 큰 것에는 불필요하다. 1.3 GB 디스크는 512바이트 블럭을 비트맵으로 332KB나 써야하는데, 비록 블럭을 클러스터링하면 디스크당 83KB을 가진다. 4KB 블럭의 1TB 디스크는 32MB(2^40/2^12=2^28bit = 2^25byte)를 필요로한다. 점점 커지는 디스크 사이즈는, 비트벡터를 증가시킨다.

### 14.5.2 링크드 리스트

다른 여유 공간 관리는 모든 여유 블럭을 링크하고, 첫번째 프리 블럭의 포인터를 파일 시스템의 특별한 공간에 저장하고 메모리에 캐싱하는 것이다. 이 첫번째 블럭은 다음 프리 블럭을 가르키는 포인터를 가진다. 이 구조는 효율적이지 않다. 리스트를 순회하기 위해서, 우리는 반드시 각 블럭을 읽어야하고, HDD의 I/O를 필요로한다. 다행히도, 여유 리스트를 순회하는 것은 자주 있는 일이 아니다. 대개, 운영체제는 단순히 하나의 여유 블럭을 필요로하고 그래서 여유 리스트의 첫번째 블럭만 사용한다. FAT 메서드는 여유 블럭을 할당 데이터 구조에 포함시킨다. 분리된 메서드는 필요하지 않다.

### 14.5.3 Grouping

여유 리스트 접근의 수정은 첫번쨰 블럭의 n개의 여유 블럭 주소를 저장하는 것이다. 첫번째 n-1의 블럭은 실제로 여유롭다. 마지막 블럭은 다른 n 여유 블럭의 주소를 포하만다. 여유 블럭의 여러개의 주소는 표준 링크드 리스트 접근에 비해서 빠르게 찾아진다.

### 14.5.4 Counting

다른 접근은 몇개의 연속된 블럭이 동시에 할당되거나 해제된다는 사실의 장점을 이용하는 것인데, 특히 공간이 연속 할당 알고리즘 또는 클러스터링을 통해서 할당될 때이다. 그러므로, n개의 여유 블럭 주소를 가지기 보다는, 우리는 첫번째 블럭의 주소와 n개의 여유 연속 블럭을 유지한다. 각 여유 공간 리스트의 엔트리는 디바이스 주소와 수를 포함한다. 비록 각 엔트리가 단순한 디스크 주소보다 더 많은 공간을 차지하지만, 전체 리스트는 수가 1보다 클수록 짧아진다. 이 여유 공간을 따라가는 메서드는 할당 블럭의 외부 메서드와 비슷하다. 이런 엔트리들은 룩업, 삽입, 삭제가 링크드 리스트보다 효율적인 밸런스드 트리에 저장된다. 

### 14.5.5 Space Maps

오라클의 ZFS 파일 시스템은 큰 수의 파일, 디렉토리, 파일 시스템을 포함하기 위해서 디자인되었다. 이런 규모에서, 메타데이터 I/O는 큰 성능 임팩트를 가진다. 예를 들어서, 만약 여유 공간 리스트가 비트맵으로 구현되면, 비트맵은 반드시 블럭이 할당되거나 해제될때 수정되어야한다. 1TB의 디스크에서 1GB의 데이터를 해제하는 것은 천번의 비트맵 업데이트를 발생시킨다. 확실히, 이런 시스템을 위한 데이터 구조는 크고 비효율적이다.

여유 공간의 관리에서, ZFS는 데이터 구조의 크기를 제어하고 구조를 관리하는데 필요한 I/O를 최소화하는 기술의 합성을 사용했다. 첫번쨰는, ZFS는 디바이스의 공간을 관리가능한 사이즈의 청크로 쪼개는 **metaslabs**를 생성한다. 주어진 볼륨은 수백개의 metaslabs를 가진다. 각 metaslab은 관련된 공간 맵을 가진다. ZFS는 여유 블럭에 대한 정보를 저장하는 카운팅 알고리즘을 사용한다. 디스크에 카운팅 알고리즘을 쓰기보다는, 그것은 로그 구조 파일 시스템 기술을 통해서 그들을 기록한다. Space map은 블럭 활동의 로그이고, 시간 순서, 카운팅 포맷이다. ZFS가 metaslab으로부터 여유 공간을 할당하기를 결정하면, 그것은 관련된 space map을 메모리의 밸런스 트리에 로드하고, 오프셋으로 인덱스하고, 구조에서 로그를 재시작한다. 메모리의 space map은 metaslab에서 할당되고 여유로운 공간의 정확한 대표가 된다. ZFS는 또한 한개의 엔트리에 연속된 여유 블럭을 합침으로서 최대한 맵을 압축한다. 마지막으로, 여유 공간 리스트는 디스크에 ZFS의 트랜잭션 기반의 명령어의 부분으로 업데이트된다. 수집과 소팅 페이지간에, 블럭 요청은 여전히 일어날 수 있고, ZFS는 로그로부터 이런 요청을 만족한다. 로그와 밸런스트리가 여유리스트이다.

### 14.5.6 TRIMing Unused Blocks

업데이트에 대해서 덮어쓰기를 허용하는 HDDs와 다른 미디어들은 여유 공간 리스트의 관리만이 필요하다. 블럭이 여유로워질때 특별한 관리는 필요없다. 여유로워진 블럭은 다음 블럭이 덮어쓰일떄까지 데이터를 유지한다.

덮어쓰기를 허용하지 않는 NVM 플래시 기반 저장 기기는 같은 알고리즘이 적용되면 심각하게 고통받는다. 11.1.2절을 회상하면, 이런 디바이스는 쓰이기전에 반드시 지워져야하고 이런 삭제는 큰 청크에서 일어나야지 긴 시간을 사용할 수 있다.

파일 시스템이 저장 디바이스에 페이지가 여유롭고 삭제가 되었다고 알리는 새로운 메커니즘이 필요하다. 그 메커니즘은 저장소 컨트롤러에 따라 다양하다. ATA-attached drives, Trim이다. 그것은 `unallocate` 커맨드가 있다. 어떤 특정 컨트롤러 커맨드던지, 이 메커니즘은 쓰기 가능한 저장 공간을 유지한다. 이런 능력 없이, 저장 디바이스는 가득차고 garbage collection과 block erasure가 필요하고, 느린 I/O write 성능을 초래한다.("a write cliff"라고 불린다.)

TRIM 메커니즘과 비슷한 능력이면, garbage collection과 삭제 단계는 디바이스가 가득차기전에 디바이스가 일관된 성능을 제공하게 해준다.

## 14.6 Efficiency and Performance

이제 우리는 다양한 블럭 할당법과 디렉토리 관리 옵션을 논의했고, 우리는 그들이 성능과 효율적인 저장소 사용에 끼치는 영향을 보겠다. 디스크들은 시스템 성능에서 보틀넥을 대표하는데, 그들이 메인 컴퓨터 부품중에서 제일 느리기 때문이다. 심지어 NVM 디바이스도 CPU와 메인메모리에 비하면 느리다. 이 절에서, 우리는 2차 저장소의 성능과 효율성을 증가시키는데 필요한 다양한 기술을 논의하겠다.

### 14.6.1 Efficency

저장 디바이스의 효율적인 사용은 사용중인 할당과 디렉토리 알고리즘에 크게 영향을 받는다. 예를 들어서, 유닉스 inodes들은 볼륨에 미리 할당된다. 심지어 빈 디스크도 inode에 의해서 잃은 공간을 가진다. 그러나, inode를 미리 할당하고 볼륨을 거쳐서 그들을 뿌림으로서 우리는 파일 시스템 성능을 향상한다. 이 향상된 성능은 UNIX 할당과 여유 공간 알고리즘으로부터 결과를 얻었고, 파일의 데이터 블럭을 파일의 inode 블럭 근처에 유지해서 탐색시간을 줄였다.

다른 예시는, 14.4절에서 언급한 내부 단편화를 대가로 파일탐색과 파일 전송 성능을 증가시킨 클러스터링 구조를 고려해보자. 이 단편화를 줄이기 위해서, BSD 유닉스는 파일이 클수록 클러스터의 크기를 변화시켰다. 큰 클러스터들은 그들이 찰 수 있는 공간에서 사용되었고 작은 클러스터들은 작은 파일에 사용되었다.

파일의 디렉토리 엔트리에 저장되는 데이터의 타입을 또한 고려가 필요하다. 일반적으로, "last write data"는 유저에게 정보를 공급하고 파일이 언제 백업이 필요한지 결정한다. 몇몇 시스템들은 또한 "last access date"를 유지하는데, 그래서 유저는 언제 파일이 마지막에 읽혔는지 결정할 수 있다. 이 정보를 유지하는 것의 결과는, 파일이 읽힐때마다, 디렉토리 구조의 필드는 반드시 쓰여야한다. 이 것은 블럭이 메모리로 읽혀야하고, 섹션이 바뀌고, 블럭이 다시 디바이스로 쓰이고, 왜냐하면 2차 저장소의 명령어는 오직 블럭 청크에서 일어나기 때문이다. 그래서 파일이 읽기를 위해서 열릴때 그것의 FCB는 반드시 읽히고 쓰여야한다. 이 필요사항은 자주 엑세스되는 파일에는 비효율적일 수도 있고, 그래서 우리는 파일 시스템을 디자인할때 그것의 성능 코스트에 반해서 그것의 이득도 값을 매겨야한다. 일반적으로, 파일에 연관된 모든 데이터 아이템은 그것이 성능과 효율성에 미칠 영향을 고려해야한다.

예를 들어서, 데이터에 엑세스하는데 사용되는 크기에 따라서 효율성이 얼마나 영향을 끼칠지를 고려해보자. 대부분의 시스템들은 32 bit 또는 64 bit 포인터를 운영체제를 통해서 사용한다. 32 비트 포인터는 파일의 크기를 2^32로 제한한다. 64비트 포인터는 매우 큰 파일 사이즈를 허용하지만, 64비트 포인터는 저장할 공간이 더 필요하다. 결과적으로, 할당과 여유공간 관리 메서드는 더 많은 저장 공간을 사용한다.

포인터 사이즈를 선택하는 한가지 어려운 점은, 운영체제 안의 고정된 할당 사이즈는 변화하는 기술의 영향에 대비중이다. IMB PC XT는 10MB 하드 드라이브를 가지고 MS DOS FAT 파일 시스템은 오직 32MB를 지언했다. 디스크 용량이 증가함에 따라서, 큰 디스크들은 32MB 파티션으로 나누어져야하기 때문에, 파일 시스템은 32MB를 넘는 블럭을 추적하지 못했다. 하드디스크가 100MB를 넘는 것이 일반적이게 되자, MS-DOS의 디스크 데이터 구조와 알고리즘은 큰 파일 시스템을 허용하기 위해서 수정되었다.(각 FAT 엔트리는 16비트로 확장되었고 32비트까지 확장되었다.) 첫번째 파일 시스템 결정은 효율성 문제로 만들어졌다. 그러나, MS-DOS 버전 4의 출시와 함께, 수백만의 컴퓨터 이용자는 새롭고 큰 파일 시스템으로 교환해야할때 불편을 겪었다. 솔라리스의 ZFS 파일 시스템은 128비트 포인터를 사용했고, 확장될 필요가 전혀없었다.

다른 예시로, 솔라리스 운영체제의 진화를 고려하겠다. 많은 데이터 구조들은 고정된 길이를 가지고, 시스템 시작에 할당된다. 이런 구조들은 프로세스 테이블과 오픈 파일 테이블을 포함한다. 프로세스 테이블이 가득차면, 어떠한 프로세스들은 생성되지 않는다. 파일 테이블이 가득차면, 어떠한 새로운 파일들이 열리지 않는다. 시스템은 유저에게 서비스를 제공하는데 실패할 수 있다. 테이블 사이즈들은 커널을 재 컴파일하거나 시스템을 리부트해야지만 커진다. 솔라리스의 최신 버전과 함께, 대부분의 모든 커널 구조들은 동적으로 할당되었고 시스템 성능에 인위적인 제한을 제거했다. 물론, 이런 테이블을 조정하는 알고리즘은 더 복잡해졌고, 운영체제는 그것이 테이블 엔트리를 동적으로 할당하고 해제해야기 떄문에 약간 느려졌다. 그러나 그 가격은 일반적인 함수와 비슷하다.

### 14.6.2 Performance

기본 파일 시스템 알고리즘이 선택된 후에도, 우리는 여전히 성능을 몇가지 방식으로 향상시킬 수 있다. 12장에서 언급했듯이, 저장소 디바이스 컨트롤러는 전체 트랙 또는 블럭을 충분히 저장할 수 있는 온보드 캐시를 구성할 로컬 메모리를 포함한다. HDD에서, 한번 탐색이 수행되면, 트랙은 디스크 헤드 아래의 섹터에서 시작하는 디스크 캐시를 읽는다. 디스크 컨트롤러는 어느 섹터 요청을 운영체제에 전달한다. 한번 블럭이 디스크 컨트롤러에 의해서 메모리로 가면, 운영체제는 그곳의 블럭을 캐싱한다.

몇몇 운영체제들은 블럭이 조만간에 쓰일 것이라는 가정아래 **buffer cache**를 위한 메인 메모리의 분리된 섹션을 유지한다. 다른 시스템 캐시 데이터는 **page cache**를 사용한다. 페이지 캐시는 파일 데이터를 파일 시스템 기반 블럭보다는 가상 메모리 기술을 이용한 페이지를 이용해서 캐시한다. 가상 주소를 이용해서 파일 데이터를 캐싱하는 것은 물리 디스크 블럭을 통해 캐싱하는 것보다 효율적이고, 파일 시스템 보다는 가상 메모리 접근 인터페이스를 이용한다. 솔라리스, 리눅스, 윈도우즈를 포함한 운영체제들은 프로세스 페이지와 파일데이터를 캐시하기 위해서 페이지 캐싱을 한다. 이것이 **unified virtual memory**라고 한다.

유닉스와 리눅스의 몇몇 버전은 **unified buffer cache**를 제공한다. 이 것의 장점을 설명하기 위해서, 파일을 열고 접근하는 두가지 대안을 고려하겠다. 한가지 접근은 메모리 매핑을 사용하는 것이다. 두번쨰는 시스템콜 `read()`와 `write()`를 사용하는 것이다. 여기서, `read()`와 `write()` 시스템콜은 버퍼 캐시를 통해서 일어난다. 메모리 매핑 콜은, 버퍼 캐시와 페이지 캐시 두개를 사용한다. 메모리 매핑은 파일 시스템으로부터 디스크 블록을 읽는 것과 그들을 버퍼 캐시에 저장하는 것으로 진행한다. 가상 메모리 시스템이 버퍼 캐시와 상호작용하지 않기 때문에, 버퍼 캐시의 파일의 컨텐츠는 페이지 캐시에 복사되어야한다. **double caching**이라고 알려진 이 상황은 캐싱 파일 시스템 데이터를 두번 필요로한다. 메모리를 낭비할 뿐만 아니라 엄청난 CPU와 I/O 사이클이 시스템 메모리의 추가 데이터 이동때문에 낭비된다. 추가적으로, 두 캐시 사이의 비일관성은 파일을 상하게한다. 반대로, 통합 버퍼 캐시가 제공되면, 두 메모리 매핑과 `read()`, `write()` 시스템 콜은 같은 페이지 캐시를 사용한다. 이것은 더블 캐싱을 피하는 장점을 자기고, 가상 메모리 시스템이 파일 시스템 데이터를 관리하는 것을 허용한다.

우리가 저장소 블럭 또는 페이지를 캐싱하는 것에 관계 없이, LRU는 블럭 또는 페이지 교체에 합리적인 알고리즘일 것이다. 그러나, 솔라리스 페이지 캐싱 알고리즘의 진화는 알고리즘에 선택이 어렵다는 것을 밝혀냈다. 솔라리스는 프로세스와 페이지 캐시가 사용하지 않는 메모리를 공유하게 허용했다. 2.5.1 이전의 버전은 프로세스에 페이지를 할당하는 것과 그들을 페이지 캐시에 할당하는 것에 큰 차이를 두지 않았다. 결과적으로, 많은 I/O 명령어를 수행하는 시스템은 그들의 가용메모리를 페이지 캐싱하는데 사용했다. I/O의 높은 비율 떄문에, 페이지 스캐너는 여유 메모리가 낮을 떄, 페이지 캐시보다는 프로세스로부터 페이지를 재 선언했다. 솔라리스 2.6과 솔라리스 7은 페이지 스캐너는 프로세스 페이지에 우선권을 주는 우선도 페이징을 구현했다. 솔라리스 8은 프로세스 페이지와 파일 시스템 페이지 캐시에 고정된 제한을 적용하였고, 둘을 메모리 밖으로 나가게하는 것을 방지했다. 솔라리스 9와 10은 메모리를 최대로 사용하고 스래싱을 최소화하는 알고리즘으로 바꾸었다.

I/O의 성능에 영향을 끼치는 다른 이슈는 파일시스템을 동기/비동기로 쓰는 것이다. **Synchronous writes**는 저장소 서브시스템이 그들을 받은 순서대로 일어나고, 쓰기는 버퍼되지 않는다. 그러므로 콜링 루틴은 반드시 그것이 진행하기 이전에 데이터를 리턴 받아야한다. **Asynchronous write**에서는, 데이터는 캐시에 저장되고, 콜러의 리턴을 제어한다. 대부분의 쓰기는 asynchronous이다. 그러나, 메타데이터 쓰기는 synchronous이다. 운영체제는 자주 오픈 시스템 콜에 writes가 동기적으로 수행되기 위한 프로세스가 리퀘스트하는 것을 허용하기 위해서 플래그를 포함시킨다. 예를 들어서, 데이터 베이서는 아토믹 트랜잭션의 기능을 사용하는데, 데이터가 안정된 저장소에 원하는 순서로 확신하기 위해서이다.

몇몇 시스템들은 파일의 엑세스 타입에 따라 다른 교체 알고리즘을 사용함으로서 페이지 캐시를 최적화한다. 연속적으로 읽거나 쓰이는 파일은 그것의 페이지가 LRU에 의해서 교체되면 안되는데, 왜냐하면 대부분의 최근에 사용된 페이지가 마지막에 사용될 것이거나, 두번다시 사용되지 않기 떄문이다. 대신에 순차 접근은 free-behind와 read-ahead로 알려진 테크닉으로 최적화가 가능하다. **Free-behind**는 다음 페이지가 요청되는대로 버퍼로부터 페이지를 제거한다. 이전 페이지는 다시 쓰이려고하지 않고 버퍼 공간을 낭비한다. **Read-ahead**는 요청된 페이지와 몇몇 다음 페이지들은 읽히고 캐시된다. 이런 페이지들은 현재 페이지가 진행된 후에 요청되는 경향이있다. 디스크로부터 한번의 전송으로 이런 데이터를 찿는 것과 그들을 캐싱하는 것은 엄청난 시간을 절약시킨다. 누군가는 컨트롤러 위에서 캐시를 추적하는 것이 멀티 프로그래밍 시스템에서 read-ahead를 위한 필요를 제거한다고 생각할 수 있다. 그러나, 캐시를 메인메모리에 조금 전송하는 것은 높은 지연율과 오버헤드를 포함하기에, read-ahead가 훨씬 효율적이다.

페이지 캐시, 파일시스템, 디바이스 드라이버는 흥미로운 접점을 가진다. 작은 양의 데이터가 파일에 쓰이면, 페이지들은 캐시에 버퍼링되고, 저장 디바이스 드라이버는 디바이스 주소에 따라서 그것의 아웃풋 큐를 정렬한다. 이런 두개의 활동은 디스크 드라이버가 디스크 헤드 탐색을 최소화한다. 비록 동기 쓰기가 필요하지만, 디스크에 쓰는 프로세스는 간단히 캐시에 쓰고, 시스템은 비동기적으로 데이터를 디스크에 쓴다. 유저 프로세스가 빠른 쓰기를 본다. 데이터가 디스크로부터 읽히면, 블럭 I/O 시스템은 몇가지 read-ahead를 실행한다. 그러나, 쓰기는 읽기보다 비동기적이다. 그러므로, 파일 시스템을 통한 디스크로의 아웃풋은 보통 작은 전송을 위한 인풋보다 빠르다. 얼마나 많은 버퍼링과 캐싱이 있던지 간에, 크고 연속적인 I/O는 용량을 초과하고 디바이스의 성능을 병목시킨다. 큰 영화 파일을 HDD에 쓰는 것을 고려하겠다. 만약 파일이 페이지 캐시보다 크면 페이지 캐시는 채워지고 I/O는 드라이브 속도로 생긴다. 현재의 HDDs는 쓰는 것보다 빨리 읽는다. 그래서 이 예시는 작은 I/O 성능과는 반대되는 성능이다.

## 14.7 회복

파일과 디렉토리들은 메인 메모리와 저장소 볼륨에 보관되고 시스템 실패가 데이터의 손실이나 비일관성을 일으키지 않도록 반드시 케어해주어야한다. 시스템 충돌은 디렉토리 구조, 프리-벌럭 포인터, 프리 FCB 포인터 같은 온-스토리지 파일 시스템 데이터 구조 사이에 비일관성을 야기한다. 많은 파일 시스템들은 제자리에 이런 구조에 변화를 적용시켰다. 파일 생성 같은 일반적인 명령어는 디스크의 파일 시스템안에많은 구조적 변화를 포함한다. 디렉토리 구조들이 수정되고, FCB가 할당되고, 데이터 블럭이 할당되고, 모든 블럭의 여유 카운트가 줄어든다. 이런 변화들은 충돌에 의해서 방해를 받고, 구조들 사이의 비일관성이 일어난다. 예를 들어서, 여유 FCB 카운트는 FCB가 할당될 것을 가르키지 않지만, 디렉토리 구조는 FCB를 가르키지 않을 수 있다. 이런 문제들을 합치는 것은 I/O 성능을 최적화하기 위해서 운영체제가 하는 캐싱이다. 몇몇 변화는 저장소에 직접 영향을 미칠수 있지만, 다른 것들은 캐시된다. 만약 캐시된 변화가 충돌이 일어나기전에 저장 디바이스에 도달하지 않으면, 더욱 큰 손상이 생길 수 있다.

파일 시스템 구현에서 버그, 크래시에 추가로, 디바이스 컨트롤러, 심지어는 유저 앱이 파일 시스템을 손상시킬 수 있다. 파일 시스템들은 파일 시스템 데이터 구조와 알고리즘에 따라서 손상에 맞설 다양한 메서드를 가진다. 

### 14.7.1 Consistency Checking

손상의 이유에 관계없이, 파일 시스템은 반드시 문제를 찾고 그들을 고쳐야한다. 탐색을 위해서, 각 파일 시스템의 모든 메타데이터의 스캔은 시스템의 일관성을 확신하거나 부정한다. 불행히도, 이 탐색은 몇 분이나 몇 시간이 걸리고 반드시 시스템 부팅시마다 일어나야한다. 대안으로, 파일 시스템은 파일 시스템 메타데이터안에 그것의 상태를 저장할 수 있다. 메타데이터 변화의 시작에, 상태 비트는 그 메타데이터가 변화중이라고 세팅된다. 만약 메타데이터로의 모든 업데이트가 성공적으로 완료되면, 파일 시스템은 비트를 비울 수 있다. 만약, 그러나, 상태비트가 세팅된 채로 남으면, 일관성 체커가 실행된다.

**Consistency checker**, 유닉스의 `fsck`같은 시스템 프로그램은 저장소 위의 디렉토리 구조와 다른 메타데이터 상태안의 데이터를 비교하고 발견하는 어떠한 비일관성도 수정한다. 할당과 여유 공간 관리 알고리즘은 체커가 찾을 수 있는 문제의 타입을 지시하고 어떻게 그들을 성공적으로 수정할지 찾아낸다. 예를 들어서, 만약 링크드 할당이 사용되고 그것의 다음 블럭을 가지는 링크를 가지면, 그때 전체 파일은 데이터 블럭으로 재건설되고 디렉토리 구조는 재생성된다. 반대로, 인덱스 할당 시스템에서의 디렉토리 엔트리의 손실은 재앙적인데, 왜냐하면 데이터 블럭은 서로간의 지식이 전혀없다. 이 이유로, 결과가 공간 할당, 메타데이터 변화를 만드는 쓰기가 없는 몇몇 유닉스 파일 시스템의 읽기를 위한 캐시 디렉토리 엔트리는, 해당하는 데이터 블럭이 쓰이기전에 동기적으로 일어난다. 물론, 문제는 여전히 만약 동기적 쓰기가 충돌에 의해서 일어나면 여전히 일어날 수 있다. 몇몇 NVM 저장소 디바이스는 디바이스 버퍼로부터 저장소 매체로 데이터를 쓰게하는 파워 로스에서도 충분한 파워를 제공하는 배터리 또는 슈퍼 커패시터를 포함한다. 그러나 이런 조심은 충돌에 의한 손상을 보호하지는 못한다.

### 14.7.2 Log Structured File Systems

컴퓨터 공학자들은 한 분야에서 사용하는 것을 다른 분랴에서도 유용하게 쓸수있다. 그런 경우는 데이터베이스 로그 베이스 회복 알고리즘이다. 이런 로깅 알고리즘들은 일관성 체킹의 문제로 성공적으로 적용되었다. 만들어진 구현은 **log-based transaction-oriented**(or **journaling**) 파일 시스템이다.

앞의 절에서 언급한 일관성 체크 접근을 다시 생각하면, 우리는 회복 과정에서 구조가 부서지고 그들을 수리하는 것을 허용했다. 그러나, 이 접근에는 심각한 문제가 있다. 한가지는 비일관성이 회복될 수 없다. 일관성 체크는 구조를 회복할 수 없고, 파일과 심지어는 디렉토리의 손상을 일으킨다. 일관성 체킹은 충돌을 해결하기 위해서 사람 개입을 필요로하고 만약 사람이 없으면 불편하다. 일관성 체킹은시스템과 클락 시간을 차지한다. 테라바이트의 데이터를 체크하려면, 클락타임의 시간이 꽤 필요하다.

이 문제의 해결책은 파일 시스템 메타데이터 업테이트를 위한 로그 베이스 회복 테크닉을 적용하는 것이다. NTFS와 Veritas file system은 이 메서드를 사용하고, 그것은 솔라리스의 UFS에 포함된다. 실제로, 이것은 ext3, ext4, ZFS를 포함한 많은 파일 시스템에서 일반적이다.

기본적으로, 모든 메타데이터 변화는 로그에 연속적으로 쓰인다. 특정 태스크를 실행하는 각 명령어의 집합은 **transaction**이다. 한번 변화가 로그에 쓰이면, 그들은 커밋되었다고 고려되고, 그 시스템 콜은 유저 프로세스를 리턴하고, 실행을 계속한다. 그동안에, 이런 로그 엔트리들은 실제 파일 시스템 구조를 건너서 리플레이 된다. 변화가 만들어지면, 포인터는 어떤 활동이 완료되고 어떤 것이 덜 완성되었는지 알리는 포인터를 업데이트한다. 전체 커밋 트랜잭션이 완료되면, 엔트리는 그것을 가르키는 로그안에 만들어진다. 로그 파일은 실제로 환형 버퍼이다. **Circular buffer**은 그것의 공간의 끝에 쓰고 시작점에서 다시 써서, 오래된 값을 덮어쓴다. 우리는 버퍼가 아직 저장되지 않은 데이터를 쓰기를 원하지 않고, 그래서 그 시나리오는 피해진다. 로그는 파일 시스템의 분리된 섹션에 존재할 수 있고 분리된 저장 디바이스에 있을 수 있다.

만약 시스템이 충돌을 일으키면, 로그 파일은 0을 포함하거나 많은 트랜잭션을 포함한다. 그것이 포함한 어느 트랜잭션들은 파일 시스템에 완성되지 않는데, 심지어 그들이 운영체제에 의해서 커밋되어도이다. 그래서 그들은 반드시 완료되어야한다. 트랜잭션은 그 일이 완료될때까지 포인터로부터 실행된다. 그래서 파일 시스템 구조는 일관성을 유지한다. 한가지 문제는 트랜잭션이 실패했을때인데, 즉, 시스템 크래시전에 커밋되지 못한 것이다. 이런 파일에 적용된 트랜잭션으로부터의 변화는 반드시 되돌려야하고, 파일 시스템의 일관성을 보존한다. 이 회복은 충돌 이후에 필요한데, 일관성 체킹을 가진 문제를 제거한다.

디스크 메타데이터 업데이트에 로깅을 사용하는 부가적인 장점은 그들이 디스크 데이터 구조에 직접 적용될 때보다 업데이트가 더 빨리 진행된다는 것이다. 이유는 랜덤 I/O보다는 연속 I/O의 성능 장점에서도 보인다. 값비싼 동기 랜덤 메타데이터 쓰기는 동기 연속 쓰기로 바뀌었다. 이런 변화는 랜덤 쓰기를 통해서 적절한 구조에 비동기로 리플레이된다. 전체적인 결과는 HDD의 파일 삭제와 생성같은 메타데이터 기반 명령어의 성능 향상이다.

### 14.7.3 Other Solution

다른 대안은 WAFL 파일 시스템과 솔라리스 ZFS 시스템이다. 이런 시스템들은 새로운 데이터로 블럭을 덮어쓰지 않는다. 트랜잭션이 완료되면, 오래된 버전의 블럭을 가르키는 메타데이터 구조는 새로운 블럭을 가르킨다. 파일 시스템은 이전 포인터와 이전 블럭을 제거하고 이전 블럭이 재사용가능하도록 만든다. 만약 이전 포인터와 블럭이 보관되면, **snapshot**이 생성된다. 스냅샷은 특정 시간 지점에서의 파일 시스템의 뷰이다. 이 해결책은 만약 포인터 업데이트가 아토믹하게 실행되면 일관성 체킹을 필요로하지 않는다. WAFL은 일관성 체커를 가지지만, 그래서 몇몇 실패 시나리오는 메타데이터 손상을 일으킨다.

ZFS는 디스크 일관성에 더욱 혁신적인 접근을 가진다. WAFL처럼, 그것은 블럭을 다시쓰지 않는다. 그러나, ZFS는 더 나아가고 모든 메타데이터와 데이터 블럭의 체크섬을 제공한다. 이 해결책은 데이터가 항상옳다고 보장한다. 그래서 ZFS는 일관성 체커에서 필요없다.(11.8.6에서 설명함)

### 14.7.4 Backup and Restore

저장소 디바이스는 가끔 실패하고 이런 실패로 데이터를 영영 잃지 않게 보장해야한다. 마지막으로, 시스템 프로그램은 한 저장 디바이스에서 다른 디바이스로 **back up** 데이터를 이용할 수 있다. 개인 파일, 전체 디바이스의 로스로부터의 회복은 백업으로부터 데이터를 **restoring**하는 문제인 것이다.

필요한 카피를 최소화하기 위해서, 우리는 각 파일의 디렉토리 엔트리로부터 정보를 사용할 수 있다. 예를 들어서, 만약 백업 프로그램이 파일의 마지막 백업 시점을 알면, 파일의 마지막 데이터 쓰기는 그날 이후로 변하지 않은 것을 알고 파일은 다시 카피되지 않는다. 일반적인 백업 스케쥴은 다음과 같다.
- Day 1 파일 시스템으로부터 모든 파일을 복사한다. **full backup**
- Day 2 1일 이후로 바뀐 것을 다른곳에 모두 복사한다. **incremental backup**
- Day N. N-1의 파일 변화로부터 복사하고 day1로 돌아간다.

새로운 사이클이 이전 집합으로 그것의 백업으로 다시 쓰이거나 새로운 백업 미디어의 집합으로 쓰일 수 있다.

이 메서드를 사용하면, 우리는 풀 백업과 각각의 incremental backup으로 복구로 전체 파일 시스템을 복구한다. 물론 N의 값이 클수록, 미디어의 수는 더 커지고 전체 복구를 위해서 더 많은 미디어를 읽어야한다. 이 백업 사이클의 추가된 장점은 사이클 동안 전날의 백업으로부터 삭제된 파일을 되찾아옴으로서 우리가 실수로 삭제한 파일을 언제든지 복구할 수 있다는 것이다.

사이클의 길이는 필요한 백업의 양과 복구될 날의 필요에서 타협한다. 복구를 위해서 읽어야할 테이프의 수를 줄이기 위해서, 풀 백업을 실행하고 각 날마다 풀 백업 이후에 바꾼다. 이 방식에서, 복구는 대부분의 최근 증가 백업을 통해서 일어나고 풀백업은 어떤 다른 증가 백업이 필요할때만 쓰인다. 트레이드 오프는 각 파일이 더 변할수록, 그래서 각 연속된 백업이 더 많은 파일과 백업 미디어를 필요로한다.

유저는 특정 파일이 사라지고 삭제된 것을 오랜 시간 후에 알게되었다. 이 이유로, 우리는 보통 평생 저장될 풀 백업을 가질 계획을 가진다. 이 아이디어는 일반적인 백업보다 좋은 아이디어다. 여러개의 백업을 가지는 것은 만약 너의 데이터가 중요하다면 좋은 아이디어이다.

## 14.8 WAFL File System

2차 저장소 I/O가 시스템 성능에 큰 영향을 미치기 때문에, 파일 시스템 디자인과 구현 커맨드는 시스템 디자이너에게 꽤 많은 집중을 요구한다.몇몇 파일 시스템은 일반 목적인데, 그들은 합리적인 성능과 기능을 파일 크기, 파일 타입, I/O부하 등의 넓은 분야에 제공한다. 다른 것들은 일반 목적 파일 시스템보다 특정한 태크스를 위해서 최적화되었고 좋은 성능을 제공한다. **Write anywhere file layout(WAFL)**은 이런 최적화의 예시이다. WAFL은 랜덤 쓰기에 최적화된 강력하고 우아한 파일 시스템이다.

WAFL은 넷앱에 의해서 만들어진 네트워크 파일 서버에 특화로 사용되고 분산 파일 시스템을 의미한다. 비록 그것은 NFS와 CIFS를 중점으로 디자인되지만, 그것은 NFS, CIFS, iSCSI, `ftp`, `http` 프로토콜을 통해서 클라이언트에게 파일을 제공한다. 많은 클라이언트가 파일 서버와 대화하기 위해서 이런 프로토콜을 사용하고, 서버는 랜덤 읽기를 위한 큰 수요를 보고 랜덤 라이트를 위한 큰 수요도 보았다. NFS와 CIFS 프로토콜은 읽기 명령어를 통해서 데이터를 캐시하고, 쓰기 또한 파일 서버 생성자에게 큰 고려를 준다.

쓰기를 위한 NVRAM 캐시를 포함하는 WAFL은 파일 서버에 의해서 사용된다. WAFL 디자이너는 안정된 저장소 캐시를 앞세워서 랜덤 I/O를 위해서 파일 시스템을 최적화하는 특정한 구조에서 실행하는 것의 장점을 얻는다. 간편한 사용은 WAFL의 원칙중하나이다. 그것의 제작자는 그것이 파일 시스템의 다중 읽기 전용 카피를 다른 포인트에 생성하는 새로운 스냅샷 기능을 포함하게 디자인했다.

파일 시스템은 버클리 FFS와 비슷하다. 그것은 블럭 베이스이고 파일을 설명하기 위해서 inodes를 사용한다. 각 inode는 블럭에 inode에 의해서 표현될 16개의포인터를 포함한다. 각 파일 시스템은 루트 inode를 가진다. 모든 메타 데이터가 파일 속에 있다. 모든 아이노드는 하나의 파일에 있고, 프리 블럭 맵은 다른 곳에 있고, 프리 아이노드는 3번쨰에 배치된다. 이것들이 표준 파일이기 때문에, 데이터 블럭은 장소에 한정되어있지 않고 어디로든 위치할 수 있다. 만약 파일 시스템이 추가 디스크로 확장되면, 메타데이터 파일의 길이는 파일 시스템에 의해서 자동으로 확장된다. 

그러므로, WAFL파일 시스템은 루트 inode를 베이스로하는 블럭의 트리이다. 스냅샷을 얻기위해서, WAFL은 루트 아이노드의 복사를 생성한다. 어떠한 파일 또는 메타데이터는 새로운 블럭으로 가서 업데이트하기 보다는 그들의 존재하는 블럭에 덮어쓴다. 새로운 루트 아이노드는 메타데이터와 쓰기의 결과를 변화시킨다. 그 동안에, 스냅샷은 여전히 업데이트되지 않은 오래된 블럭을 가르킨다. 그것은 그러므로 파일 시스템에 접근을 제공하고 그것이 마치 스냅샷이 만들어진 인스턴트이다. 그리고 적은 저장 공간을 차지한다. 실제로, 스냅샷에 의해서 점유된 추가 공간은 스냅샷 이후에 수정된 블럭을 포함한다.

표준 파일 시스템의 중요한 변화는 여유 블럭 맵이 블럭당 1bit이상을 포함한다. 그것은 각 스냅샷이 이용하는 블럭을 위한 비트 집합을 가진 비트맵이다. 블럭을 사용하는 모든 스냅샷이 삭제되면, 블럭의 비트맵은 모두 0이고 블럭은 재사용을 위해서 해제된다. 사용된 블럭은 결코 다시 쓰이지 않기에, 쓰기는 매우 빠르다. 쓰기가 현재 헤드 위치에 가까운 여유 블럭에서 일어난다. WAFL에는 여러가지 성능향상 최적화가 있다. 

많은 스냅샷이 동시에 존재해서, 한가지는 매시간, 매일 생길수 있다. 이런 스냅샷에 접근 가능한 유저는 그들이 스냅샷이 생기는 시간에 있기에 파일에 접근할 수 있다. 스냅샷 기능은 백업, 테스팅, 버저닝에 유용하다. WAFL의 스냅샷 기능은 그것이 각 데이터 블럭의 COW 카피도 필요하지 않다는 점에서 효율적이다. 다른 파일 시스템은 스냅샷을 제공하지만, 덜 효율적으로 자주이다. 

새로운 버전의 WAFL은 **clone**이라는 read-write 스냅샷을 제공한다. 클론들은 효율적인데, 스냅샷과 같은 기술을 이용한다. 이 경우에 리드 온니 스냅샷은 파일 시스템의 상태만을 캡처하고, 클론은 리드 온니 스냅샷을 언급한다. 클론으로의 쓰기는 새로운 블럭에 저장되고 클론의 포인터는 새로운 블럭을 언급하기 위해서 업데이트된다. 원본 스냅샷은 바뀌지 않고, 클론이 업데이트 되기전의 파일 시스템에 뷰를 제공한다. 클론들은 오리지널 파일 시스템을 대체할 수도 있다. 이것은 모든 오래된 포인터와 관련된 오래된 블럭을 버리는 것을 포함한다. 클론들은 테스팅과 업그레이드에 유용한데, 오리지널 버전이 만져지지 않았고 클론은 테스트가 끝나거나 업그레이드가 실패하면 버려지면된다. 

WAFL로부터 결과된 다른 기능은 **replication**인데, 복제와 데이터의 집합의 동기화가 네트워크를 통해 다른시스템에 적용된다. 먼저, WAFL의 스냅샷은 다른 시스템에 복제된다. 다른 스냅샷이 소스 시스템에서 생기면, 그것은 새로운 스냅샷에 포함된 모든 블럭을 보냄으로서 원격 시스템의 업데이트를 할 수 있다. 이런 블럭들은두개의 스냅샷이 찍힌 사이의 시간에서 생긴 변화이다. 원격 시스템은 이런 블럭을 더하고 그것의 포인터를 업데이트하고, 새로운 시스템은 두번쨰 스냅샷의 시간의 소스 시스템의 복제본이다. 이 과정을 반복하면 원격 시스템을 첫 시스템의 복사본이라고 볼 수 있다. 이런 복제는 회복을 위해서 사용된다. 첫 시스템이 파괴되면, 그들의 대부분의 데이터는 원격 시스템에 있다.