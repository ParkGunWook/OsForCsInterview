# what we gonna study

CPU 스케쥴링은 멀티프로그램 운영체제의 기초이다. 프로세스간에 CPU를 교체하는 것은, 운영체제가 컴퓨터를 더욱 생산적으로 만든다. 이 챕터에서는, 우리는 CPU 스케쥴링 개념과 리얼타임 시스템에서의 몇가지 CPU 스케쥴링 알고리즘을 보이겠다. 우리는 또한 특정 시스템에서 알고리즘을 고르는 문제를 고려하겠다. 

챕터 4에서, 우리는 프로세스-스레드 모델을 소개했다. 현재의 운영체제에서 프로세스가 아닌 커널 레벨 스레드이고 실제로 운영체제에 의해서 스케쥴된다. 그러나, "프로세스 스케쥴링"과 "스레드 스케쥴링"은 보통 혼용되어 쓰인다. 이 챕터에서, 우리는 프로세스는 일반적인 개념을 말할때 쓰고 스레드는 스레드 특정 지식을 의미할 때 사용하겠다.

비슷하게, 챕터 1에서 우리는 코어가 어떻게 CPU의 기초 단위인지 설명했고, 프로세스가 CPU의 코어에서 작동한다 했다. 그러나, 많은 곳에서, 우리는 프로세스 스케쥴링의 언어를 "run on a CPU"라고 표현할 것이고, 우리는 프로세스가 CPU의 코어에서 작동한다는 의미이다.

# Objectives

- 여러가지 CPU 스케쥴링 알고리즘을 접한다.
- 스케쥴링 기준에 따라서 스케쥴링 알고리즘을 평가한다.
- 멀티 프로세서와 멀티코어 스케쥴링에서의 이슈를 설명한다.
- 리얼-타임 스케쥴링 알고리즘을 설명한다.
- 윈도우, 리눅스, 솔라리스 운영체제에서의 스케쥴링 알고리즘을 본다.
- CPU 스케쥴링 알고리즘을 평가하기 위해서 모델링과 시뮬레이션을 적용한다.
- 몇가지 다른 CPU 스케쥴링 알고리즘을 디자인한다.

## 5.1. 기본 개념

싱글코어 시스템에서, 오직 하나의 프로세스가 한번에 작동한다. 다른 프로세스들은 반드시 CPU의 코어가 자유가 되고 다시 스케쥴되어야한다. 멀티프로그래밍의 목적은 프로세스가 모든 시간동안 동작하고, CPU의 효율을 극대화하는 것이다. 아이디어는 꽤나 단순하다. 프로세스는 실행되기전까지 대기해야한다.(보통은 I/O 요청시간이다.) 단순한 컴퓨터 시스템에서, CPU는 휴식을 취한다. 모든 대기시간은 낭비된다. 어떠한 유용한 일들도 성취되지 않는다. 멀티프로그래밍과 함께라면, 우리는 이 시간을 생산적으로 사용하려고한다. 몇몇 프로세스들은 메모리에 동시에 대기한다. 프로세스가 기다려야하면, 운영체제는 프로세스를 CPU 밖으로 내보내고 다른 프로세스를 가지고 온다. 이런 패턴이 지속된다. 프로세스가 기다려야 할때마다, 다른 프로세스는 CPU를 점유한다. 멀티코어 시스템에서, CPU를 바쁘게 유지하는 것은 모든 프로세싱 코어로 확장된다.

이런 종류의 스케쥴링은 운영체제 기능에서 기본적이다. 대부분의 컴퓨터 자원은 사용전에 스케쥴된다. CPU는 물론이고 대부분의 컴퓨터 리소스들이다. 그러므로, 그것들의 스케쥴링은 운영체제 디자인에서 주요하다.

### 5.1.1 CPU-I/O Burst Cycle

CPU 스케쥴링의 성공은 프로세스의 자원 관측에 달려있다. 프로세스 실행은 CPU 실행과 I/O 대기의 **사이클**로 이루어져있다. 프로세스들은 두가지 상태를 오간다. 프로세스 실행은 **CPU Burst**와 함께 시작한다. 그것은 **I/O Burst**를 뒤따른다. 그리고 다른 CPU burst를 따르고 또 I/O Burst를 따르고, 마침내, 마지막 CPU Burst은 시스템 리퀘스트로 실행을 종료한다. 

CPU Burst의 시간은 확장적으로 측정이 된다. 비록 그들이 프로세스-프로세스/컴퓨터-컴퓨터 간에 크게 다르지만, 그들은 주기에 따라 갈린다. 그들은 지수승으로 일반화되고, 많은 수의 short CPU burst와 적은 수의 long CPU burst로 나뉜다. I/O-bound 프로그램은 보통 short CPU-burst를 가진다. CPU-Bound 프로그램은 적은 long CPU-burst를 가진다. 이런 분포는 CPU 스케쥴링 알고리즘을 구현할 때 중요하다.

### 5.1.2 CPU 스케쥴러

CPU가 휴게를 가질때마다, 운영체제는 레디큐의 프로세스를 골라서 실행한다. 선택 프로세스는 **CPU 스케쥴러**가 가져오고, 메모리에서 실행할 준비가 된 프로세스 들에서 프로세스를 선택하고 CPU에 프로세스를 할당한다.

모든 레디큐는 FIFO일 필요가 없다는 것을 명심하자. 우리가 볼 스케쥴링 알고리즘에서 레디큐는 FIFO 큐, 우선순위 큐, 트리, 비정렬 링크드 리스트로 구현될 수 있다. 개념적으로, 그러나, 모든 레디큐의 프로세스들은 CPU에서 실행할 찬스를 기다리며 줄서있다. 큐에서의 레코드들은 보통 PCB이다. 

### 5.1.3 비선점과 선점 스케쥴링

CPU 스케쥴링 결정은 4가지 상황을 가정한다.

1. 프로세스가 작동 스테이트에서 대기 상태로 바뀔때이다.(예를 들어서, I/O 요청의 결과 또는 자식 프로세스의 종료를 위한 wait() 호출)
2. 프로세스가 러닝 상태에서 레디 상태(인터럽트 발생)
3. 프로세스가 대기 상태에서 레디 상태(I/O의 완료)
4. 프로세스가 종료할때.

1번과 4번 상황에서, 스케쥴링에게는 선택권이 없다. 새로운 프로세스(레디큐에 하나가 존재한다면)는 반드시 실행을 위해서 선택되어야한다. 선택권은 2번과 3번 상황에 있다. 

스케쥴링이 오직 1,4 상황에서 차지할때, 우리는 스케쥴링이 **비선점**이고 **Cooperative**라고 한다. 다른 경우에는, **선점**이라고 한다. 비선점 스케쥴링에서는, 한번 CPU가 프로세스에 할당되면 프로세스가 CPU를 놓을때까지 또는 종료/대기 상태가 될때 까지 지속된다. 가상적으로 모든 현대 운영체제는 선점을 사용한다.

불행하게도, 선점 스케쥴링은 데이터가 몇가지 프로세스사이에서 공유되면 레이스 컨디션에 빠지게 한다. 두개의 프로세스가 데이터를 공유한다고 고려하자. 프로세스하나가 데이터를 업데이트 할때, 두번째 프로세스가 자리를 뺏을 수도 있다. 두번째 프로세스는 데이터를 읽으려하지만, 그것은 내용이 다른 상태이다. 이러한 이슈는 6장에서 세세히 다루겠다.

선점은 또한 운영체제 커널의 디자인에 영향을 끼친다. 시스템콜 처리중에, 커널은 프로세스의 편에서 바쁠 수도 있다. 이런 활동들은 중요 커널 데이터 변경을 포함한다. 이런 변화의 중심에서 프로세스가 선점하고 커널이 같은 구조를 읽거나 바꾸어야하면 무슨 일이 일어나겠는가? 혼돈일 것이다. 6.2절에서 보게될, 운영체제 커널은 비선점과 선점을 둘다 사용한다. 비선점 커널들은 시스템콜이 완수되길 기다리거나 I/O의 완료되는 동안 기다릴 것이다. 이런 구조는 커널 구조가 간단하게 하고, 커널은 커널 데이터 구조가 inconsistent 상태일때에 선점하는 프로세스가 없게 할 것이다. 불행하게도, 커널 실행 모델은 리얼-타임 컴퓨팅을 지원하기에는 부족하고, 태스크들은 주어진 타임 프레임에서 실행을 완료해야한다. 5.6절에서, 우리는 리얼 타임 시스템의 스케쥴링 수요를 알아본다. 선점 커널은 뮤텍스락을 필요로한다. 대부분의 현대 운영체제는 커널 모드에서 완벽히 선점형이다.

인터럽트는 언제든 일어날 수 있기 때문에, 그들이 항상 커널에 의해서 무시될 수 없기때문에, 인터럽트에 영향을 받는 코드는 반드시 동시 사용에 의해서 보호되어야한다. 운영체제는 대부분의 시간에 인터럽트를 허용해야한다. 다른경우에는, 인풋은 잃어버려지거나 아웃풋이 겹쳐 쓰일수 있다. 그래서 이런 코드부분은 몇가지 프로세스에 의해서 동시에 접근이 불가능하고, 그들은 시작과 동시에 인터럽트를 비활성화하고 끝나면 인터럽트를 활성화한다. 이러한 부분은 자주 일어나지 않고 보통은 소수의 명령어만 포함한다.

### 5.1.4 Dispatcher

CPU 스케쥴링 기능에서 포함된 다른 구성요소는 **dispatcher**이다. 이 디스패처는 CPU 스케쥴러에 의해서 선택된 프로세스에게 CPU 코어의 제어를 주는 모듈이다. 기능은 다음과 같다.

- 한 프로세스에서 다른 프로세스로 컨텍스트 스위칭
- 유저모드로 스위칭
- 프로그램을 재시작하기위해서 유저프로그램의 적절한 위치로 점핑

디스패처는 최대한 빨라야하는데, 그것은 매 컨텍스트 스위치마다 일어난다. 디스패처에 의해서 프로세스가 멈추고 다른 것이실행되는 것은 **dispatch latency**라고 부른다.

흥미로운 질문은 어떻게 컨텍스트 스위치가 얼마나 일어나는 것이다. 시스템-와이드 레벨에서, 컨텍스트 스위치의 수는 vmstat 커맨드로 얻을 수 있다.
`vmstat 1 3`
이 명령어는 3줄의 결과를 1초간의 딜레이로 제공한다.
```c
---------cpu---
24
225
339
```

첫줄은 시스템 부팅이후 1초간의 평균 컨텍스트 스위치의 수이고, 다음은 두 1초 간격간에 일어난 수이다. 머신이 부팅된 이후로, 매초 24번의 컨텍스트 스위치가 일어났다. 그리고 이전 '초'에서, 225번의 컨텍스트 스위치가 만들어졌고, 339번이 그 다음 초에서 일어났다.

우리는 또한 /proc 파일 시스템을 사용해서 주어진 프로세스의 컨텍스트 스위치 수를 결정할수있다. 예를 들어서 /proc/2166/status를 cat하겠다.
```c
voluntary_ctxt_switches         150
nonvoluntary_ctxt_switches      8
```
아웃풋은 프로세스의 라이프 타임간에 일어난 컨텍스트 스위치의 수를 보여준다. 자발과 비자발 컨텍스트의 차이를 꼭 알아야한다. 자발적인 컨텍스트 스위치는 CPU의 컨트롤을 현재는 필요하지 않아서 포기한 것이다. 비자발 컨텍스트 스위치는 CPU를 다른프로세스에게 뺏긴 것인데, 타임 슬라이스가 만료되거나 높은 우선순위의 프로세스에게 선점당한것이다.

## 5.2 스케쥴링 표준

다른 CPU 스케쥴링 알고리즘들은 다른 기능을 가지고, 특정 알고리즘의 선택은 다른 프로세스에서는 선호될 수 있다. 어떤 알고리즘을 고를지 선택하는 것은, 우리는 반드시 다양한 알고리즘의 성능을 따져야한다.

많은 기준이 CPU-스케쥴링 알고리즘 비교를 위해서 제시되었다. 어떤 성질들이 비교를 위해서 사용되느냐는 어떤 알고리즘이 최고인가를 결정하게 한다. 기준은 다음과 같다.

- CPU 효율성 : 우리는 CPU가 항상 바쁘게 하고 싶다. 개념적으로, CPU 효율성은 0~100퍼센트이다. 실제 시스템에서 그것은 40퍼센트부터 시작되서 90퍼센트까지 되어야한다.
- Throughput : 만약 CPU가 프로세스 실행에 바쁘면, 일은 완료되어야한다. 일의 측정 방법은 타임 유닛에 마무리되는 수이다. 긴 프로세스들은, 이 수치는 한 프로세스/몇초가 될수있다. 보통은 초당 10개의 프로세스이다.
- Turnaround time : 특정 프로세스의 관점에서, 중요한 기준은 프로세스가 얼마나 실행되는 가이다. 프로세스의 제출부터 종료까지의 간격이 turnaround time이다. 이 시간은 레디큐에서의 대기시간과 CPU 실행, I/O를 합한다.
- Waiting time : CPU-스케쥴링 알고리즘은 어떤 프로세스가 실행되거나 I/O를 하는 시간은 영향을 끼치지않는다. 그것은 오직 프로세스가 레디큐에서 기다리는 시간만을 조정한다. 대기 시간은 레디큐에서의 대시 시간이다.
- Response time : 상호작용 시스템에서, turnaround time은 최고의 표준이 아니다. 보통, 프로세스는 꽤 빨리 생산되고 이전의 결과가 유저에게 보이는 동안 새로운 결과를 제공할 수 있다. 그러므로 다른 측정은 제출부터 첫번째 반응이 생성되기까지의 시간이다. 이 측정은, 반응 시간이라고 불리고, 반응에 결과를 내는 것이 아니라 반응을 시작하기까지의 시간이다.

CPU utilization과 thorughput을 최대화하고 turnaround time, waiting time, response time을 최소화하는 것은 이상적이다. 대부분의 경우에, 우리는 평균을 측정한다. 그러나 몇가지 상황에서, 우리는 평균보다는 미니멈과 맥시멈을 최적화하는 것을 선호한다. 예를 들어서, 모든 유저가 좋은 서비스를 보장받기 위해서, 우리는 최대 반응시간을 최소화 하기를 원한다.

연구자들은 제안했다. 상호작용 시스템(데탑, 랩탑)은 평균 반응시간을 줄이기보다는 반응시간 분포를 줄이는게 더 중요하다고 했다. 일리있고 예측가능한 반응시간을 가진 시스템은 평균은 매우 짧지만 넓은 분포를 가진 시스템이 더욱 이상적이다. 그러나 이런 분포를 줄이는 연구는 얼마 없다.

CPU 스케쥴링 알고리즘을 배울 것이다. 우리는 그들의 명령을 설명한다. 간편성을 위해서 우리는 하나의 CPU 버스트만 고려한다. 평균 대기 시간의 비교를 한다. 더욱 정교한 측정 메커니즘은 5.8에서 이루어진다.

## 5.3 스케쥴링 알고리즘

CPU 스케쥴링은 레디큐에 있는 어떤 프로세스를 CPU의 코어에 할당하는지를 결정하는 것이다. 많은 종류의 CPU-스케쥴링 알고리즘이 있다. 이 절에서는, 우리는 그들중 몇가지를 설명하겠다. 비록 현대의 CPU 구조가 멀티 프로세싱 코어를 가졌지만, 우리는 오직 한개의 프로세싱 코어가 있다고 가정하겠다. 즉, 단일 코어를 가진 단일 CPU이고, 그러므로 시스템은 한번에 하나의 프로세스만 실행한다. 5.5절에서, 우리는 CPU 스케쥴링을 멀티 프로세서 시스템에서 논의한다.

### 5.3.1 FCFS 스케쥴링

가장 단순한 스케쥴링 알고리즘은 **First-come, First-serve** 알고리즘이다. 이 구조에서는 요청순으로 CPU에 할당된다. FCFS의 구현은 FIFO 큐에 의해서 쉽게 관리된다. 프로세스가 레디큐에 들어오면, 그것의 PCB는 큐의 꼬리에 링크된다. CPU가 자유로워지면, 큐의 헤드 프로세스가 CPU에 할당된다. 실행 프로세스는 큐에서 제거된다. FCFS 스케쥴링 코드는 쓰기와 이해가 간단하다.

부정적인 면은 FCFS의 정책은 평균 대기시간이 쉽게 늘어나는 것이다. 동적인 상황에서의 FCFS 스케쥴링 성능을 고려하겠다. 우리가 한개의 CPU-bound 프로세스와 다수의 I/O bound 프로세스를 가졌다고 가정하겠다. 프로세스가 시스템에서 흐를때, 시나리오는 다음과 같다. CPU-바운드 프로세스는 CPU를 점유한다. 이 시간동안, 모든 다른 프로세스는 그들의 I/O를 종료하고 레디큐로 이동하고 CPU를 기다린다. 프로세스가 레디큐에서 기다리는 동안, I/O 디바이스 들은 유휴한다. 마침내, CPU-바운드 프로세스는 그것의 CPU 버스트를 종료하고 I/O 디바이스로 이동한다. 모든 I/O 바운드 프로세스는, 짧은 CPU 버스트를 가지고, 빠르게 실행되고 I/O 큐의 끝으로 간다. 이 시점에서, CPU는 idle해진다. CPU 바운드 프로세스는 레디큐로 이동하고 CPU에 할당된다. 다시, 모든 I/O 프로세스들은 CPU 바운드 프로세스가 종료되기까지 레디큐에서 기다린다. **conboy effect**는 CPU종료를 위해서 모든 프로세스가 하나의 큰 프로세스를 기다리는 것이다. 이 효과는 짧은 프로세스를 먼저하지 않아서 낮은 CPU/디바이스 효율성을 초래한다.

또한 FCFS는 비선점이다. CPU가 한번 할당되면, 그것은 CPU를 놓을때까지 지속된다. 따라서 FCFS는 상호작용 시스템에서 비효율적이다. 

### 5.3.2 SJF 스케쥴링

CPU 스케쥴링에 다른 접근은 SJF 스케쥴링 알고리즘이다. 이 알고리즘은 프로세스의 다음 CPU 버스트 길이에 관련되어있다. CPU가 가용해지면, 그것은 짧은 CPU 버스트의 프로세스를 할당한다. 만약 두개의 프로세스의 다음 CPU 버스트가 같으면 FCFS 스케쥴러가 해결한다. 더욱 정확한 이름은 *shortest-next-CPU-burst*라고 불린다. 왜냐하면 스케쥴링이 다음 프로세스의 CPU 버스트 길이에 달려있기 때문이다. 

SJF는 주어진 일련의 프로세스에 대해서 최소의 평균 대기시간을 준다는 것이 증명가능하다. 짧은 프로세스를 긴 것의 뒤로 넣는 것은 짧은 프로세스의 대기시간을 늘이는 것보다 많이 줄일수 있다. 결과적으로, 평균 대기시간이 줄어든다.

비록 SJF 알고리즘이 이상적이지만, 그것은 CPU 스케쥴링으로 구현이 불가한데, CPU 버스트의 길이를 알수있는 방법이 없기 때문이다. 한가지 문제를 해결하는 방법은 SJF 스케쥴링을 근사화하는 것이다. 우리는 다음 CPU 버스트의 길이는 모르지만, 예측하는 것은 가능하다. 우리는 CPU의 버스트를 이전의 것과 비교해서 예측한다. 다음 CPU 버스트의 길이 근사화 계산으로, 우리는 예측된 가장 짧은 CPU 버스트를 얻을 수 있다.

다음 CPu 버스트는 보통 **exponential average**로 예측된다.

![sjf_time_cal.png](./image/sjf_time_cal.png)

t_n의 값은 대부분의 최근 정보를 포함하고, 타우는 과거의 기록을 저장한다. 파라미터 알파는 최근과 과거의 정보를 어디에 비중을 싣느냐인데 0이라면 현재의 기록이 전혀 영향을 끼치지 않는 것이다. 만약 1이라면, 현재의 CPU 버스트만이 영향을 끼치는 것이다. 초기 타우_0는 상수로 지정되거나 시스템의 평균으로 지정된다. 

SJF 알고리즘은 선점과 비선점이 가능하다. 선택은 새로운 프로세스가 레디큐에 도달했을때 어떻게 하느냐에 따라 다르다. 다음 새로 도착한 프로세스의 CPU 버스트는 현재 실행중인 것의 남은 시간보다 짧을 수 있다. 선점형 SJF 알고리즘은 그러면 실행중인 프로세스를 뻇는다. 반면에 비선점형 SJF는 유지한다. 선점형 SJF 스케쥴링은 가끔 **Shortest-remaining-time-first** 스케쥴링이라고 부른다.

### 5.3.3 Round-Robin Scheduling

**Round-robin(RR)**은 FCFS와 비슷하지만, 선점이 가능해져서 프로세스간의 스위치가 가능해졌다. **Time quantum**이나 **Time slice**라고 불리는 작은 시간 단위가 정의된다. 그 길이는 10~100 미리 세컨드 가량 된다. 레디큐는 순환 큐 구조로 정의된다. CPU 스케쥴러는 레디큐를 돌게되고, 한개의 타임퀀텀만큼의 시간을 각 프로세스에게 할당한다. 

RR을 구현하기위해서, 우리는 다시 FIFO 구조의 큐를 준비한다. 새로운 프로세스가 레디큐의 테일에 들어갈 것이다. CPU 스케쥴러는 레디큐로부터 첫번째 프로세스를 가져오고, 한개의 타임퀀텀 이후에 인터럽트를 발생해서 프로세스를 배출한다.

두가지 일중에 하나는 반드시 생긴다. 프로세스는 한개의 타임퀀텀보다 적은 CPU 버스트를 가질수가 있다. 이런 경우에는, 프로세스는 그자체로 CPU를 해제한다. 스케쥴러는 그러면 레디큐의 다음 프로세스를 준비할 것이다. 만약 CPU 버스트가 한개의 타임퀀텀보다 크면, 타이머는 시작하고 운영체제에 인터럽트를 야기할 것이다. 컨텍스트 스위치는 실행될 것이고, 프로세스는 레디큐의 꼬리에 주어진다. CPU 스케쥴러는 레디큐의 다음 프로세스를 선택한다. 

RR의 정책은 평균 대기시간이 생각보다 길다. RR 알고리즘은 1 타임퀀텀보다 많은 시간을 CPU에 할당하지 않는다. 만약 프로세스의 시간이 초과하면, 그 프로세스는 선점당하고 레디큐의 뒤에 놓인다. RR 알고리즘은 따라서 선점형이다.

만약 n개의 프로세스가 레디큐에 들어가고 타임퀀텀이 q이면, 각 프로세스는 (n-1)*q의 대기시간을 가지게 될 것이다. 예를 들어서, 5개의 프로세스와 20미리초의 타임퀀텀이 있다면, 각 프로세스는 100초마다 20초만큼 실행된다.

RR 알고리즘의 성능은 타임퀀텀의 크기에 크게 의존한다. 한가지 극단으로는, 만약 타임퀀텀이 너무크면, RR 정책은 FCFS가 되버린다. 반면에 너무 작으면, RR은 수많은 컨텍스트 스위치를 야기한다. 예를들어서, 한개의 프로세스만이 10개의 단위 시간을 가진다. 만약 타임퀀텀이 12단위시간이라면, 프로세스는 1 타임퀀텀 보다 적게 종료되고, 추가비용은 더이상 들지 않는다. 그러나, 프로세스가 2개의 퀀텀을 요구하면, 컨텍스트 스위치가 생긴다. 만약 타임퀀텀이 1 단위시간이면 9번의 스위치가 생기고, 프로세스는 느리게 실행된다.

그러므로, 우리는 타임퀀텀을 컨텍스트 스위치 시간에 맞게 최대한 크기를 원한다. 만약 컨텍스트 스위치가 정확히 타임퀀텀의 10%라면, CPU 시간의 10%가 컨텍스트 스위치가 된다. 실제로, 대부분의 현대 운영체제는 10~100미리 세컨드를 가진다. 컨텍스트 스위치의 시간은 대개 10 마이크로 세컨드이하이다. 그러므로 컨텍스트 스위치는 타임퀀텀의 아주 작은 일부분이다.

턴어라운드 시간은 또한 타임퀀텀에 달려있다. 평균 턴어라운드 시간은 타임퀀텀의 증가에 따라 항상 향상되는 것은 아니다. 보통, 평균 턴어라운드 시간은 만약 대부분의 프로세스가 한번의 타임퀀텀에 종료될때 증가한다. 예를 들어서 10 단위시간의 3개의 프로세스가 주어지고 1단위시간의 퀀텀이라면, 평균 턴어라운드 시간은 29이다. 만약 타임퀀텀이 10이면, 그것은 20으로 떨어진다. 만약 컨텍스트 스위치 시간이 추가되면, 평균 턴어라운드 시간은 작은 타임퀀텀에 따라서 더욱 증가한다.

비록 타임퀀텀은 컨텍스트 스위치 시간과 크게 비교되어야지만, 그렇게 클 필요가 없다. 말했듯이 만약 타임퀀텀이 너무크면, FCFS로 퇴보된다. 보통 80퍼센트의 CPU 버스트는 타임퀀텀보다 적어야한다.

### 5.3.4 우선순위 스케쥴링

SJF는 **priority-scheduling**알고리즘의 특별한 케이스이다. 우선순위는 각 프로세스와 관련이 있으며, CPU는 가장 높은 우선순위의 프로세스가 할당된다. SJF 알고리즘은 CPU 버스트의 역이 우선순위인 간단한 알고리즘이다. CPU 버스트가 커질수록, 우선순위는 떨어진다.

우리가 높은 우선순위와 낮은 우선순위에 대해서 이야기한 것을 기억하자. 우선순위들은 보통 고정된 범위의 수를 의미한다. 예를 들어 0~7이나 0~4095이다. 그러나, 0이 최고나 최악이라는 공통의 규약은 없다. 이 차이는 혼란을 부르므로, 여기서는 낮은 숫자가 높은 우선권이다.

우선도는 내부 또는 외부적으로 정의된다. 내부로 정의된 우선도는 프로세스의 우선도를 측정하기 위해서 측정가능한 품질이나 양을 사용한다. 예를 들어서, 시간제한, 메모리 필요, 열려있는 파일의 수, I/O 버스트와 CPU 버스트의 비율이 있다. 외부 우선순위는 운영체제 밖의 요소들로 측정된다. 프로세스의 중요도인데, 컴퓨터 사용을 위해서 지불된 자본의 양과 종류, 일은 지원하는 부서 등등이 있다.

우선도 스케쥴링은 선점과 비선점이 가능하다. 프로세스가 레디큐에 도달하면, 그것의 우선도는 현재 우선도와 비교를 한다. 선점 스케쥴링 알고리즘은 만약 새로이 도착한 프로세스가 현재 것보다 높은 우선도를 가지면 선점한다. 비선점은 그저 레디큐의 헤드에 둔다.

우선순위 알고리즘의 가장 큰 문제는 **indefinite blocking**또는 **starvation**이다. 레디큐에서 CPU를 기다리는 프로세스는 봉쇄되었다고 한다. 우선순위 알고리즘은 낮은 우선순위의 프로세스를 무기한으로 기다리게 할 수 있다. 무겁게 로드된 컴퓨터 시스템에서, 높은 우선순위의 프로세스는 낮은 순위의 프로세스를 CPU에 영영 닿지 않게 한다. 보통, 두가지중 하나는 일어난다. 프로세스가 마침내 실행되거나, 컴퓨터 시스템이 마침내 충돌을 일으키고 모든 낮은 우선순위의 일을 잃어버린다.

이 문제의 해결책은 낮은 우선순위의 프로세스를 **aging**하는 것이다. 에이징은 시스템에서 오래 기다린 프로세스를 나이먹게 하는 것이다. 예를 들어서 0~127의 우선도가 있다면, 우리는 주기적으로 대기 프로세스의 우선도를 1씩 높이는 것이다. 마침내, 127의 우선도를 가진 것도 실행되게 된다. 

다른 방법은 RR과 우선순위를 합치는 것이다. 이 방법은 같은 우선순위의 프로세스들을 RR시키는 것이다.

### 5.3.5 멀티레벨 큐 스케쥴링

우선순위와 RR 스케쥴링에서는, 모든 프로세스는 한개의 큐에만 배치되었고, 스케쥴러는 높은 우선순위의 프로세스를 선택했다. 어떻게 큐가 관리되느냐에 따라서 O(n) 탐색이 우선순위 프로세스들에서 필요할 것이다. 실제로는, 우선도에 따라서 큐를 분리해둔다. 이 방법이 **멀티레벨 큐** 방식이라고 불린다. 그리고 RR이 섞인다. 만약 가장 높은 우선도가 여러개 있으면, 그들은 RR로 작동한다. 이 것이 가장 보편화된 방법이고, 우선도는 각 프로세스에 통계적으로 할당되고, 런타임간에 같은 큐에 머물게 된다.

멀티레벨 큐 알고리즘은 프로세스 타입에 따라서 여러가지 분리된 큐를 사용할 수도있다. 예를 들어서 **foreground(interactive)**와 **background(batch)**의 구분이다. 이 두 타입의 프로세스들은 다른 반응-시간 필요를 가지고 그래서 다른 스케쥴링이 필요하다. 추가로, 포어그라운드 프로세스는 백 그라운드 프로세스보다 우선도가 높다. 분리된 큐는 foregroound와 background 프로세스들이 있고 그들의 고유한 알고리즘이 있을 수 있다. foreground는 RR을 쓰고 background는 FCFS를 쓰는 것이다. 

추가로 큐 사이에 스케쥴링이 있고, 보통은 선점형 고정 우선순위로 구현한다. 예를 들어서, real-time 큐는 interactive 큐보다 절대적인 우선도를 가진다. 멀티레벨 큐 알고리즘은 4개의 큐를 가진다 가정하자.
1. Real-time processes
2. System processes
3. Interactive processes
4. Batch processes

각 큐는 낮은 우선도의 큐보다 절대적인 우선도를 가진다. 배치 큐의 어떠한 프로세스도 real-time 프로세스가 비기전까지 실행되지 않는다. 만약 배치프로세스가 실행중에 interactive 프로세스가 레디큐에 들어가면 배치프로세스는 선점당한다.

다른 가능성은 큐사이에 time-slice이다. 각 큐는 CPU 시간의 일부분을 가지고, 그것의 다양한 프로세스 사이에서 스케쥴된다. 예를 들어가서 fore-back 큐 예제에서, 80%의 fore 지분이 RR방식으로 돌아가고 20%의 back 지분이 FCFS로 돌아간다.

### 5.3.6 멀티레벨 피드백 큐 스케쥴링

일반적으로, 멀티레벨 큐 스케쥴링 알고리즘이 사용되면, 프로세스들은 시스템에 입장하면 영구적으로 큐에 할당되었다. 만약 분리된 큐가 있다면, 한번 배정받은 것은 바뀌지 않는다. 이 방식은 낮은 스케쥴링 오버헤드가 발생하지만, 유연하지 못하다. 

**Multilevel Feedback queue** 알고리즘은 큐사이의 이동을 허용한다. 이 아이디어는 CPU 버스트의 특성에 따라서 프로세스를 분리하기 위해서이다. 만약 프로세스가 많은 CPU 시간을 사용하면, 그것은 낮은 우선도큐로 이동된다. 이  구조는 I/O 바운드와 interactive 프로세스(짧은 CPU 버스트를 가졌다.)가 높은 우선도 큐에 들어가게 한다. 추가로, 낮은 큐에서 오래 기다린 프로세스는 높은 순위로 이동된다. 이 에이징이 기아를 방지한다.

예를 들어서, 멀티레벨 큐가 3가지 큐로 나누어져있다고 생각하자. 큐 0의 모든 프로세스가 제일 먼저 실행될 것이다. 오직 큐 0가 비고 큐 1이 실행된다. 비슷하게, 큐2의 프로세스들은 오직 큐0과 1이 비면 실행된다. 그리고 새로운 큐가 들어오면 선점당할 것이다.

들어오는 프로세스는 큐 0에 놓인다. 큐 0의 프로세스는 8 미리초의 타임퀀텀을 가진다. 만약에 제시간에 끝나지 않으면, 큐 1의 꼬리로 이동한다. 만약 큐 0이 비게되면, 큐1의 프로세스가 16 미리초의 타임퀀텀으로 실행된다. 만약에 종료되지않으면 선점당하고 큐2에 배치된다. 그리고 큐2는 FCFS를 기반으로 작동하는데 오직 큐 0과 1이 비었을 때이다. 기아를 방지하기 위해서 오래기다린 것들은 높은 순위의 큐로 이동시켜준다.

이 스케쥴링 알고리즘은 8 미리초의 어떤 프로세스든 높은 우선도를 준다. 이런 프로세스는 CPU를 쉽게 가지고, CPU 버스트를 종료하고, I/O 버스트를 바로 실행한다. 8~24 미리초의 프로세스들은 빠르게 운영된다. 긴 프로세스들은 자동적으로 큐 2에 배치되고 늦게 실행된다.

보통 멀티레벨 피드백 큐는 다음과 같은 파라미터를 가진다.

- 큐의 숫자.
- 각 큐의 스케쥴링 알고리즘
- 높은 우선순위 큐로 이동을 결정하는 메소드
- 낮은 우선순위 큐로 이동을 결정하는 메소드
- 프로세스가 서비스를 필요로 할 때 들어가게 될 큐를 결정하는 메소드

멀티레벨 피드백 큐 스케쥴러는 가장 일반적인 CPU 스케쥴링 알고리즘이다. 이것은 특정 시스템에 맞게 설정이 가능하다. 불행하게도, 이것은 가장 복잡한 알고리즘이고, 모든 파라미터를 가장 최적으로 선정할 필요성을 가진다.

## 5.4 스레드 스케쥴링

4장에서, 우리는 스레드-프로세스 모델을 소개했고, 유저-커널 레벨 스레드를 구분했다. 대부분의 현대 운영체제는 커널레벨 스레드이고 운영체제에 의해서 관리된다. 유저 레벨 스레드는 스레드 라이브러리에 의해서 관리되고, 커널은 그들을 알지 못한다. CPU를 실행하기 위해서, 유저레벨 스레드는 반드시 커널 레벨 스레드에 매핑되어야하고, 매핑이 간접적이고 lightweight process여도 있어야만 한다. 이 절에서는, 우리는 유저-커널 레벨 스레드를 포함한 스케쥴링 이슈와 Pthreads를 통한 예제를 보겠다.

### 5.4.1 contention scope

유저-커널 스레드사이의 구별점은 어떻게 그들이 스케쥴되는가이다. many-to-one과 many-to-many 방식이 구현된 시스템에서, 스레드 라이브러리는 유저레벨 스레드가 LWP에서 작동하게한다. 이 구조를 **process-contention scope**라고 한다. 같은 프로세스를 가진 스레드간의 CPU 경쟁이기 때문이다.(우리는 스레드 라이브러리가 가용한 LWP에서 유저레벨 스레드를 스케쥴하는 것을 말할때, 우리는 스레드가 실제 CPU에서 작동한다고 의미하지 않는다.) 어떤 커널 레벨 스레드가 CPU에서 스케쥴될지 결정하는 것은, 커널은 **system-contention-scope**를 사용한다. SCS 스케쥴링을 가진 CPU 경쟁은 시스템안의 모든 스레드가 포함된다. one-to-one 모델은 오직 SCS를 사용한다.

일반적으로, PCS는 우선도로 실행되고, 스케쥴러는 높은 우선도의 실행가능한 스레드를 실행한다. 유저레벨 스레드 우선도들은 프로그래머에 의해서 결정되고 스레드 라이브러리로는 조정되지 않는다. PCS는 또한 선점형이다. 그러나 같은 우선도의 스레드 간에는 RR을 지원한다는 보장이 없다.

### 5.4.2 Pthread Scheduling

우리는 4.4.1절에서 포식스 피스레드를 구현했다. 이제, 우리는 포식스 Pthread API의 PCS / SCS 를 명세해보겠다. Pthreads들은 다음과 같은 scope values를 구별한다.
- PTHREAD_SCOPE_PROCESS - PCS
- PTHREAD_SCOPE_SYSTEM - SCS

many-to-many 모델 구현 시스템에서, PSP 정책은 가용한 LWPs에서 유저레벨 스레드를 스케쥴한다. LWP의 수는 스레드 라이브러리에 의해서 유지되고, 아마도 scheduler activation을 이용한다. PSS는 LWP를 생성하고 각 유저 레벨 스레드를 many-to-many 모델에 묶고, one-to-one 정책으로 효과적으로 매핑한다.

Pthread IPC는 contention scope 정책을 setting and getting하는 주요한 기능을 제공한다.

- pthread_attr_setscope(pthread_attr_t *attr, int scope)
- pthread_attr_getscope(pthread_attr_t *attr, int *scope)

첫번째 파라미터는 스레드의 성질 셋을 가르키는 포인터이다. setscope의 2번째 파라미터는 PSS 또는 PSP 값을 전달하고, 어떻게 contention scope를 할지 가르킨다. getscope에서는 contention scope의 현재 값을 가르키는 인트 포인터를 포함한다. 만약, 에러가 발생하면, 각각의 함수는 0이 아닌 값을 반환한다.

코드는 우선 존재하는 contention scope를 결정하고 그것은 PSS로 설정한다. 그리고 5개의 분리된 스레드는 SCS 정책으로 작동한다. 몇몇 시스템에서는 한가지 종류의 정책만 선택이 가능할수도 있다. 예를 들어서, 리눅스와 macOS는 PSS만 지원을 한다.

## 5.5 멀티프로세서 스케쥴링

우리는 단일 프로세싱 코어에서의 스케쥴링 문제만을 집중했다. 만약 다중 CPU가 있으면, 다중 스레드를 병렬로 실행하는 **load sharing**이 가능해지고, 스케쥴링 이슈는 더욱 복잡해진다. 많은 가능성이 시도되었다. 우리가 싱글코어에서 봤듯이 최고의 답은 없다.

전통적으로, 멀티프로세서는 다중 물리 프로세서를 제공하는 시스템을 의미하는데, 각 프로세서는 하나의 CPU를 가진다. 그러나, 멀티프로세서의 정의는 바뀌었고, 현대에서는 다음과 같은 것을 의미한다.
- Multicore CPUs
- Multithreaded cores
- NUMA systems
- Heterogeneous multiprocessing

여기서, 우리는 다른 구조에 대해서 멀티프로세서 스케쥴링을 고려하는 것을 논의한다. 첫번째 3가지 예시에서, 우리는 같은 프로세서 시스템(homogeneous) 우리는 그리고 큐에있는 프로세스를 가용한 모든 CPU에서 사용하는 케이스를 본다. 마지막에서는, 우리는 프로세서의 능력이 각각 다른 경우를 보겠다.

### 5.5.1 멀티 프로세서 스케쥴링 접근

멀티프로세서 시스템에서의 CPU 스케쥴링 방식은 모든 스케쥴링 결정을 가지고, 다른 시스템 행동은 단일 프로세서(마스터 서버)에 의해서 관리된다. 다른 프로세서는 오직 유저코드에서 작동한다. 이 **assymmetric multiprocessing**은 간단한데 왜냐하면 오직 한개의 코어가 시스템 데이터 구조를 접근하고, 데이터 공유의 필요를 줄인다. 이 방법의 몰락은 마스터 서버가 점점 병목현상을 일으켜서 전체 성능이 떨어졌다.

멀티프로세서 지원하는 접근은 **symmetric multiprocessing(SMP)**인데, 각 프로세서가 개별로 스케쥴된다. 각 프로세서를 위한 스케쥴러를 가지는 것은 레디 큐를 평가하고 스레드가 작동하게 한다. 이 방법은 스케쥴 기능을 가진 스레드를 구성하는 두가지 전략을 제공한다.

1. 모든 스레드가 레디큐에 있다.
2. 각 프로세서가 그것의 개별 스레드 큐를 가진다.

만약 우리가 첫번째 옵션을 선택하면, 우리는 공유 레디큐에 의해서 레이스 컨디션을 가지고 두개의 프로세서가 같은 스레드를 실행하지 않도록 스케쥴해야한다. 6장에서, 우리는 레이스 컨디션 방지를 막기위해서 레디큐를 자물쇠형식으로 보호할 수 있다. 자물쇠는 다투지만, 모든 큐에 대한 접근은 자물쇠 소유권을 가져야하고, 공유 큐를 접근하는 것은 난처한 상황에 처하지 않도록 도와준다. 그러므로, SMP를 지원하는 시스템에게는 단순한 접근이다. 5.5.4절에서는, 각 프로세서가 개인 큐를 가짐으로서 캐시 메모리의 효율성을 보이겠다. 이 경우의 가장 큰 문제는 쉽게 변하는 사이즈에 대한 부담이다. 그러나, 우리는 프로세서간의 부담을 동등하게하는 밸런스 알고리즘을 사용할 것이다.

가상적으로 모든 운영체제는 SMP를 지원한다.(윈도우, 리눅스, 맥, 안드로이드, 아이오에스) 이 절의 남은 부분에서는, 우리는 CPU 스케쥴링 알고리즘 측면에서의 SMP 시스템을 논의하겠다.

### 5.5.2 Multicore Processors

전통적으로, SMP 시스템들은 다중 물리 프로세서를 제공하면서 여러가지 프로세스를 병렬적으로 동작시켰다. 그러나, 대부분의 현대 컴퓨터 하드웨어는 이제 같은 물리 칩에 여러개의 컴퓨팅 코어를 둔다. 각 코어는 그것의 구조적인 상태를 유지하고 분리된 논리 CPU를 보인다. 멀티코어 프로세서를 둔 SMP 시스템은 빠르고 여러개의 CPU 칩을 두는 것보다 파워 소모가 적다. 

멀티코어 프로세서들은 스케쥴링 이슈가 복잡해진다. 어떤 일이 일어나는지 보자. 연구자들은 프로세서가 메모리에 접근할 때, 데이터가 존재하기위해서 엄청난 시간을 기다리는 것을 확인했다. 이런 상황은 **Memory Stall**이라고 불리며, 현대 프로세서가 메모리보다 빠르게 작동하기 때문에 일어난다. 그러나, 이 현상은 캐시 미스라는 것 때문에 일어나기도 한다. 이런 상황을 완화하기 위해서, 많은 현대 하드웨어 디자인들은 각 코어마다 두개 또는 이상의 **하드웨어 스레드**를 두었다. 그 방식에서, 만약 메모리를 기다리면서 하나의 스레드 스톨이 일어나면, 코어는 다른 스레드로 스위칭한다. 운영체제의 입장에서는, 각 하드웨어 스레드는 그것의 구조적 상태(레지스터 셋, 명령어 포인터)를 유지하므로 논리 CPU가 소프트웨어 스레드를 작동시킨다. 

인텔은 단일 코어에 여러개의 하드웨어 스레드를 할당하는 것을 **hyper-threading**이라고 한다. 현대 인텔 프로세서는 한 코어당 2개를 지원하고, M7은 한코어당 8개의 스레드를 지원하면서 한 프로세서당 8개의 프로세서를 지원해서 64개의 논리 CPU를 제공한다.

일반적으로는, 프로세싱 코어를 멀티스레드하는 두가지 방식이 있다. **coarse-grained**와 **fine-grained**이다. 전자에서, 스레드는 메모리 스톨 같은 긴 레이턴시 이벤트가 일어나기 전까지 실행한다. 긴 레이턴시 이벤트 때문에 생긴 딜레이로 인해서, 코어는 반드시 다른 스레드를 실행하기 위해서 스위치한다. 그러나, 스레드 사이의 스위치 비용은 명령어 파이프라인은 반드시 다른 스레드가 실행되기전까지 플러시되야해서 크다. 한번 새로운 스레드가 실행을 시작하면, 그것은 파이프라인을 그것의 명령어로 채운다. Fine-grained 멀티스레딩은 더욱 질좋은 낱알의 레벨로 스레드를 스위치한다. 보통 명령어 사이클의 바운더리에서 일어난다. 그러나, fine-grained 시스템의 구조적 디자인은 스레드 스위칭을 위한 논리를 포함한다. 결과적으로, 스레드 사이의 교체 비용은 적다. 

물리적 코어(캐시와 파이프라인)의 자원은 하드웨어 스레드를 통해서 공유되는데, 그러므로 프로세싱 코어는 한번에 하나의 하드웨어 스레드만 작동할 수 있다. 결과적으로, 멀티스레드,멀티코어 프로세서는 두가지 레벨의 스케쥴링이 필요하다. 이런 상황에서는 여러가지 전략을 채택한다. 한가지 방법은 RR 알고리즘을 하드웨어 스레드에서 프로세싱 코어로 스케쥴하는 것이다. 이 방법은 UltraSPARC T3에서 사용된다. 다른 접근은 인텔 이타늄인데, 두개의 코어프로세서가 코어마다 두개의 하드웨어 관리 스레드를 가진다. 각 하드웨어 스레드는 0~7까지의 동적인 urgency 값이 있고, 0이 가장 덜급하고 7이 제일 급하다. 이타늄은 스레드 스위치를 야기하는 5가지 다른 이벤트를 구별한다. 이벤트가 일어나면, 스레드 스위칭 논리가 두 스레드 간의 urgency를 비교하고 높은 urgency 값을 실행한다. 

이중 레벨의 스레쥴링은 반드시 상호 배제일 필요는 없다. 사실은 운영체제 스케쥴러는 프로세서 리소스의 공유를 신경쓰기 위해서 만들어졌고, 그것은 효과적인 스케쥴링 결정을 내린다. 예를 들어서, CPU가 두개의 코어를 가지고, 각각의 코어는 두개의 하드웨어 스레드를 가진다. 만약 두개의 소프트웨어 스레드가 시스템에서 작동하면, 그들은 같은 코어 또는 분리된 코어에서 작동이 가능하다. 만약 그들이 같은 코어에서 스케쥴되면, 그들은 프로세서 리소스를 공유해야하고 분리된 코어에서 스케쥴될 때보다 느리게 진행된다. 만약 운영체제가 프로세서 리소스 공유에 의삭하면, 그것은 소프트웨어 스레드가 리소스를 공유하지 않도록 하면 된다.

### 5.5.3 부하 밸런싱

SMP 시스템에서, 프로세서 간에 일의 밸런스를 유지하는 것은 여러개의 프로세서를 두는 장점의 효율을 좌지우지한다. 다른경우에는, 하나 또는 많은 프로세서들이 휴식을 취하는 동안 다른 프로세서가 많은 일을 기다리면서 CPU를 기다리는 스레드의 레디큐 상태를 볼 수 있다. **Load balancing**은 SMP 시스템에서 모든 프로세서가 평등하게 일을 유지하는 것이다. 이 것은 오직 각 프로세서마다 각각의 스레드 레디큐를 가진 경우에만 필요하다. 일반 적인 런 큐를 가진 시스템에서, 로드 밸런싱은 불필요한데, 한번 프로세서가 대기하면, 그것은 레디큐에서 실행가능한 스레드를 바로 추출한다.

부하 밸런싱에는 두가지 일반적인 접근이 있다. 푸시 마이그레이션과 풀 마이그레이션이다. **Push migration**은 각각의 프로세서의 부하를 확인하고 만약 불균형을 찾으면 스레드를 덜바쁜 프로세서에 할당한다. **Pull migration**은 휴식 프로세서가 바쁜 프로세서로부터 대기 태스크를 당겨오는 것이다. push와 pull은 상호배제적일 필요는 없고, 보통 로드 밸런싱 시스템에서 둘다 구현된다. 예를 들어서, 리눅스 CFS 스케쥴러와 ULE 스케쥴러는 두가지 기법을 모두 사용한다.

"balanced load"는 다른 의미를 가질 수도 있다. 한가지 관점은 모든 큐가 동일한 수의 스레드를 가지는 것이다. 다른 경우에는 모든 큐사이에 동일한 스레드 우선도의 분포를 가지는 것이다. 추가적으로, 몇몇 상황에서는 두 전략이 모두 불충분하다. 실제로, 그들은 스케쥴링 알고리즘의 반대로 일을 한다.(뒤에서 더 살펴보겠다.)

### 5.5.4 프로세서 관련성

스레드가 특정한 프로세서에서 실행될 때 캐시메모리에서는 무슨일이 일어나는가? 스레드에 의해서 최근 접근된 데이터는 프로세서를 위해 캐시에 배치된다. 결과적으로, 스레드에 의해서 접근되는 연이은 메모리는 보통 캐리 메모리에서 만족된다. 만약 스레드가 로드 밸런싱 때문에 다른 프로세서로 이전하면 어떤 일이 일어날까? 캐시 메모리의 내용물은 첫번째 프로세서에게 부적합할 것이고, 두번째 프로세서의 캐시에 다시 배치되어야한다. 높은 비용의 부정확/재배치 캐시 비용때문에, 대부분의 SMP 시스템은 프로세서 간의 스레드 이동을 지양하고 같은 프로세서에서 일해서 warm cache의 장점을 얻으려한다. 이것이 **processor affinity**이다. 즉 프로세서는 일하던 곳에서 일할려는 친근성(affinity)가 있다.

스레드의 큐를 구성하는 방법은 두가지가 소개되었는데 프로세스 관련성을 위한 내포가 있다. 만약 우리가 일반 레디큐를 채택하면, 스레드는 어떤 프로세서에 의해서든 선택이된다. 그러므로, 만약 스레드가 새로운 프로세서에 스케쥴되면, 프로세서의 캐시는 재배치되어야한다. 개인 프로세서 레디큐에서는, 스레드는 항상 같은 프로세서에 스케쥴되고 warm cache의 장점을 이용한다. 그러므로, 프로세서 당 레디큐는 더욱 자유로운 processor affinity를 제공한다.

프로세서 관련성은 여러가지 형태를 취한다. 운영체제가 같은 프로세서에서 프로세스를 작동하는 것을 시도하는 정책을 가지고 항상 그런 것만은 아니면 **soft affinity**라고 한다. 여기서, 운영체제는 로드 밸런싱 과정에서 프로세스를 다른 프로세서에 이전할 수 있다. 반대로, 몇몇 시스템은 **hard affinity**를 제공하는데, 프로세스가 실행가능한 프로세서의 부분집합을 명시하는 것이다. 많은 시스템들은 두가지 정책을 모두 제공한다. 예를 들어서, 리눅스는 soft affinity를 구현했고, 그러나 sched_setaffinity() 시스템 콜을 통해서 스레드가 특정 CPU에서만 작동하게 돕는다.

시스템의 메인 메모리 구조는 프로세서 관련성 이슈에 영향을 미친다. 두개의 물리적인 프로세서 칩이 각각의 CPU와 로컬 메모리를 가지는데 CPU가 다른 CPU 메모리에 접근하는 Non-uniform memory access가 있다. 비록 시스템 내부 연결이 모든 CPU가 하나의 물리적 주소공간을 공유하게 하지만, 자신의 로컬 메모리에 접근이 훨씬 빠르다. 만약 운영체제의 CPU 스케쥴러와 메모리 배치 알고리즘이 NUMA를 의식하고 함꼐 일한다면, 특정 CPU에서 스케쥴된 스레드는 CPU가 있는 가장 가까운 메모리에 배치되고, 빠른 메모리 접근이 가능해진다.

흥미롭게도, 부하 밸런싱은 보통 프로세서 관련성의 이득과 상반된다. 즉, 스레드가 같은 프로세서에서 작동하는 장점은 프로세서의 캐시 메모리에서 데이터를 찾는 장점을 얻는다. 스레드를 옮기는 로드 밸런싱은 이 이점을 없앤다. 비슷하게 스레드 이전은 NUMA 시스템에서도 패널티를 주고, 스레드는 더욱 긴 시간을 메모리 접근에 사용한다. 다른 말로는, 로드 밸런싱과 메모리 접근시간 최소화는 상극이다. 그러므로, 현대의 멀티코어 NUMA시스템의 스케쥴링 알고리즘은 더욱 복잡해졌다. 5.7.1 절에서는 리눅스 CFS가 어떻게 이 상반되는 목적을 해결하는지 보겠다.

### 5.5.5 Heterogeneous Multiprocessing

예제에서 우리는, 모든 프로세서가 능력이 같으므로, 어떤 쓰레드든 어떤 프로세싱 코어에서 작동이 가능하다. 메모리 엑세스 시간의 차이는 로드 밸런싱과 프로세서 관련성에 기반한다.

비록 모바일 시스템은 멀티코어 구조를 포함하지만, 몇몇 시스템은 같은 명령어 셋을 가진 코어로 디자인 되었지만, 그들의 클락 스피드나 파워 관리는 다양하다. 이런 시스템이 바로 **heterogeneous multiprocessing(HMP)**이다. 이것은 5.5.1절에서 말한 assymetric multiprocessing이고 시스템과 유저 태스크가 어떤 코어에서도 작동가능하다. 오히려, HMP의 목적은 태스크의 특정 목표를 위해서 나은 파워 소모를 만들어낸다.

ARM 프로세서가 이를 지원하는데, **big.LITTLE**이라는 구조로 불린다. 높은 성능의 big core가 에너지 효율적인 LITTLE core와 합쳐져 있다. Big 코어는 큰 에너지를 소모하므로 단기간 사용된다. 비슷하게, little코어는 낮은 에너지로 긴시간 작동한다. 

이 방법에는 여러가지 장점이 있다. 느린 여러개를 하나의 빠른 코어와 합치므로, CPU 스케쥴러는 높은 성능이 필요하지 않은 태스크를 할당하고, 긴 시간동안 작동한다.(백그라운드 태스크) 그러므로서 배터리 소모를 보존한다. 비슷하게, 많은 프로세싱 파워를 필요로하지만 짧게 실행되는 interactive 앱은 big core에 할당된다. 추가적으로, 만약 모바일 디바이스가 파워-절약 모드이면, big 코어는 비활성화되고 오직 little 코어만 작동한다. 윈도우 10은 HMP 스케쥴링을 허용해서 그것의 파워 관리 수요를 최고로 지원한다.

## 5.6 Real-Time CPU 스케쥴링

리얼 타임 운영체제를 위한 CPU 스케쥴링은 특별한 이슈가 있다. 보통, 우리는 소프트 리얼타임과 하드 리얼타임 시스템으로 구별한다. **Sofr real-time systems**는 critical real-time process가 스케쥴될때 보장해주지 않는다. 그들은 오직 프로세스가 noncritical processes의 성향을 가질 것만 보장해준다. **Hard real-time system**은 더욱 엄격하다. 태스크는 반드시 데드라인까지 서비스되어야한다. 데드라인 이후의 서비스는 바로 만료된다. 이 절에서, 우리는 소프트와 하드 리얼타임 운영체제에서의 프로세스 스케쥴링 이슈를 살펴보겠다.

### 5.6.1 Minimizeing Latency

이벤트 위주의 리얼타임 시스템을 고려해보자. 시스템은 일반적으로 리얼타임에서 이벤트가 생기기를 기다린다. 이벤트들은 소프트웨어에서 생길수도있고(타이머가 만료됨) 하드웨어에서 생길수도 있다.(달리는 자동차 앞에 방해물이 나타났다.) 이벤트가 생기면, 시스템은 반응을 해야하고 최대한 빨리 서비스해야한다. 우리는 이벤트가 생긴 시점부터 서비스된 시점까지의 시간을 **event latency**라고 부른다. 

대개, 다른 이벤트들은 다른 지연도를 가진다. 예를 들어서, 안티락 브레이크 시스템의 지연 필요는 3~5미리초이다. 즉, 타이어가 움직일 때 감지하고 시스템은 3~5미리초 안에 반응하고 상황을 제어하는 것이다. 그것보다 오래 걸리게 되면 자동차는 제어를 잃을 것이다. 반대로, 레이더를 조종하는 임베디드 시스템은 몇초를 기다려도 된다.

리얼 타임 시스템에서 두가지 타입의 레이턴시가 성능에 영향을 미친다.

1. Interrupt latency
2. Dispatch latency

**Interrupt latency**는 인터럽트가 도착한 시간부터 인터럽트를 서비스하는 루틴의 시작을 의미한다. 인터럽트가 생기면, 운영체제는 반드시 그것이 실행하는 명령어를 완료하고 어떤 인터럽트인지 확인한다. 현재 프로세스의 상태를 저장하고 ISR를 실행한다. 이 전체 시간이 interrupt latency라고 한다.

확실히 리얼 타임 운영체제에서는 리얼 타임 태스크가 즉각적으로 반응하게 인터럽트 레이턴시를 줄이는 것이 중요하다. 실제로, 하드 리얼 타임 시스템에서는, 인터럽트 레이턴시는 반드시 간단히 최소화되면 안된다. 그것은 반드시 시스템의 엄격한 규정을 만족시켜야한다.

인터럽트 레이턴시에 영향을 주는 중요한 요소는 커널 데이터구조가 업데이트되는 동안 인터럽트가 비활성화되는 시간의 양이다. 리얼-타임 운영체제들은 인터럽트들이 오직 짧은 시간동안 비활성화되기를 필요로 한다. 스케쥴링 디스패처가 하나의 프로세스를 멈추고 다른 것을 시작하는 것이 **Dispatch latency**이다. CPU에 즉시 접근하는 리얼-타임 태스크 제공은 리얼 타임 운영체제가 이 레이턴시를 줄이도록 명령한다. 가장 효과적인 기술은 선점 커널을 제공하는 것이다. 하드 리얼 타임 시스템에서, 디스패치 레이턴시는 몇 마이크로 세컨드가 측정된다.

디스패치 레이턴시에서 **conflict phase**는 두가지 구성요소가 있다.

1. Preemption of any process running in the kernel
2. 높은 우선순위 프로세스의 필요에 의한 낮은 우선순위 프로세스의 리소스 해제

conflict 페이즈에 따르면, 디스패치 페이즈는 높은 우선도 프로세스를 스케쥴한다.


### 5.6.2 우선도 기반 스케쥴링

리얼타임 운영체제의 가장 중요한 기능은 프로세스가 CPU를 필요로하면 즉시 반응하는 리얼타임 프로세스이다. 결과적으로, 리얼 타임 운영체제의 스케쥴러는 반드시 선점을 가진 우선도 알고리즘을 가진다. 만약 스케쥴러가 선점을 지원하면, CPU에서 작동중인 프로세스는 높은 우선도 프로세스가 생기면 뺏긴다.

선점, 우선도 기반 스케쥴링 알고리즘은 5.3.4에서 다루었고 5.7은 소프트 리얼 타임 스케쥴링 기반의 예제이다. 각 시스템들은 리얼타임 프로세스를 가장 높은 우선도를 준다. 예를 들어 윈도우는 32개의 다른 우선도 레벨이 있다. 가장 높은 레벨은 16~31의 값을 가지고 리얼타임 프로세스를 위해 보존된다. 솔라리스와 리눅스도 비슷하다.

선점형, 우선도 기반 스케쥴러는 소프트 리얼타임 기능만 제공하는 것을 기억해라. 하드 리얼 타임 시스템은 리얼타임 태스크가 그들의 데드라인 필요에 맞게 서비스되고 이런 것은 추가적인 스케쥴링 기능이 필요하다. 남은 절에서는, 우리는 하드 리얼 타임 시스템에 적합한 알고리즘에 대해서 보겠다.

우리가 개별적인 스케쥴러의 상세를 보기전에, 우리는 반드시 스케쥴될 일부 프로세스의 성격을 정의해야한다.  먼저, 프로세스들은 **perodic**하다. 즉, 그들은 고정된 간격에서 CPU가 필요하다. 한번 주기적 프로세스가 CPU를 획득하면, 그것은 t의 시간을 가지고 데드라인 d까지 서비스될수있고, 주기는 p이다. 즉 이 시간들은 0 < t < d < p가 된다. 주기적 태스크의 비율은 1/p이다. 스큐젤러는 이러한 특성으로 이점을 가지고 프로세스의 데드라인또는 등수로 우선도를 할당한다.

이런 형태의 스케쥴링에 일반적이지 않은 것은 프로세스가 그것의 데드라인을 스케쥴러에 공지해야하는지이다. 그때, **admission-control**알고리즘을 사용한다. 스케쥴러는 두가지 일을 한다. 그것은 프로세스를 승인하는데, 프로세스가 제시간에 끝날 것이라고 보증하고, 또는 거절하는데 만약 그게 데드라인 이전에 끝내지 못할 경우이다.

### 5.6.3 Rate-Monotonic Scheduling

**Rate-monotonic** 스케쥴링 알고리즘은 주기적인 태스크를 선점과 함꼐 정적인 우선도 정책을 이용해서 스케쥴한다. 만약 낮은 순위의 프로세스가 작동중이고 높은 순위의 프로세스가 순서가 되면, 그것은 선점한다. 시스템에 진입한 후에, 각 주기성 태스크는 그것의 주기의 역으로 우선도를 할당한다. 주기가 짧을수록 우선도는 높아진다. 이 정책의 이유는 우선도가 높은 태스크를 CPU에 더 많이 할당하기 위해서이다. 더욱이, rate-montonic 스케쥴링은 주기성 프로세스의 프로세싱 시간이 CPU 시간이 항상 같다고 가정한다. 즉, 프로세스가 CPU를 필요할때마다, 그것의 CPU 버스트는 같다. 

주기가 50과 100인 P1, P2가 있다. 연산 시간은 20과 35이다. 각 프로세스의 데드라인은 그것의 CPU 버스트를 다음 주기의 시작전에 완료해야한다. 

우리는 반드시 이런 태스크를 확인해야는데, 각각이 그것의 데드라인을 맞출수 있는지이다. P1은 20/50을 가지고 P2는 35/100의 CPU utilization을 가지므로 총 합은 75/100이다. 그러므로 두개다 그들의 데드라인을 충족하고 CPU가 가능한 주기를 갖는다.

P2가 P1보다 높은 우선도를 가졌다고 가정하겠다. P2는 0초에 시작해서 35초에 종료한다. 이때, P1이 시작되고 55에 끝난다. P1의 첫번째 종료 데드라인은 50이므로 P1은 데드라인을 놓치게된다.

이제 rate-monotonic을 이용하면, P1이 P2보다 높은 우선도를 갖는다. P1은 0에 시작해서 20에 끝나고, 첫번째 데드라인을 만족한다. P2는 시작되서 50까지 진행된다. 그리고 주기가 돌아온 P1에 의해서 자리를 선점당하게 된다. 그리고 70에 P1이 끝나고 P2는 남은 5의 일을 처리한다.

Rate-monotonic 스케쥴링은 이 알고리즘으로 스케쥴될수 없어도 최적이라고 고려되는데, 정적인 순위 할당으로 어떤 다른 알고리즘도 스케쥴 할 수 없다. 이제 이것으로 스케쥴할 수 없는 예시를 보겠다.

P1이 50의 주기를 갖고 25의 CPU 버스트를 가지며, P2는 80의 주기와 35의 CPU 버스트를 가진다. 그러면 알고리즘은 P1을 우선에 넣는다. 전체 utilization은 (25/50)+(35/80)= 0.94를 가진다. 그것은 6%의 남는 시간을 가질수 있다. P1이 25에 처음 끝이나고 P2는 50까지 작동하다가 P1에 선점당한다. 그리고 76에 끝난 P1 이후에 P2는 85에서 끝나서 데드라인을 어기게 된다.

최적화 되있음에도 불구하고, 이런 한계는 존재한다. CPU utilization은 한정되어 있고, 그것은 CPU 리소스를 항상 전부 사용할 수 없다. 이 스케쥴링의 최악의 utilization은 N(2^(1/N)-1)을 가진다.
N이 1일때는 100%ㅇ이지만, 프로세스가 무한으로 되면 69%까지 떨어진다. 그리고 2개일때는 83%d이다. 첫번째 예제에서는 이 알고리즘은 프로세스가 확실히 스케쥴을 맞출수 있게 보장한다. 2번째 예제에서 보앗듯이, 이 알고리즘은 그들이 데드라인에 맞게 스케쥴되는 것은 보장하지 않는다.

### 5.6.4 Earliest-Deadline-First Scheduling

**Earliest-Deadline_first(EDF)**는 데드라인에 맞게 동적으로 우선도를 준다. 데드라인이 급할수록, 우선도가 높다. EDF 정책에서는, 프로세스가 가동가능해지면, 그것은 시스템에 그것의 데드라인을 알린다. 우선도들은 새로운 데드라인의 프로세스를 즉각적으로 반영한다. rate-monotonic과의 차이는 우선도가 고정되어있다는 점이다.

EDF를 설명하기 위해서, 우리는 앞의 2번쨰 예제를 이용하겠다. P1은 50의 데드라인 때문에 80인 P2보다 우선도가 높다. 그러나 다시 시작한 50의 시점에서 이전과 다르게 P2는 30의 잔여시간을 가지고 P1은 50의 잔여시간을 가지므로, P2는 P1에 의해서 선점당하지 않는다.

rate-monotonic과는 다르게, EDF는 프로세스가 주기적인 것을 필요로 하지 않고, CPU의 정적인 버스트 타임을 필요로 하지 않는다. 필요한 것은 오직 프로세스가 그것의 데드라인을 스케쥴러에 알리는 것이다. EDF의 매력은 이론상 최적이라는 것이다. 이론적으로는, 각 프로세스가 그것의 데드라일을 맞추고 CPU 활용도를 100%로 사용하게끔 스케쥴한다. 그러나, 실전에선, 그것은 프로세스와 인터럽트 핸들링 간의 컨텍스트 스위칭 비용때문에 좋은 성능을 내기는 힘들다.

### 5.6.5 Proportional share Scheduling

**Proportional share** 스케쥴링은 T의 지분을 모든 앱에 할당함으로 작동한다. 앱은 N의 지분을 가질수 있고, N/T의 프로세서 시간을 가진다. 예시로는, T=100의 지분이 3개의 프로세스로 나눠진다. A는 50, B는 15, C는 20을 받았다고 가정하자. 

스케쥴러는 허용-제어 정책으로 결합되어서 앱들이 할당된 시간을 받도록 보장한다. 허용-제어 정책은 클라이언트가 특정 수의 지분을 요청하는 것을 오직 충분할때만 허용한다. 현재의 예제에서는 충분하지만 만약에, D라는 30의 지분을 원하는 프로세스가 요청하면 허용 제어기가 D의 입장을 거절한다.

### 5.6.6 POSIX Real-Time Scheduling

POSIX 표준은 리얼타임 컴퓨팅을 제공한다. 여기서, 우리는 리얼타임 스레드 스케쥴링과 관련된 POSIX API를 다루겠다. POSIX는 두가지 스케쥴링 클래스를 리얼타임 스레드를 위해 정의한다.
- SCHED_FIFO
- SCHED_RR

FIFO는 스레드를 FCFS 정책으로 스케쥴한다. 그러나 같은 우선순위 간의 타임 슬라이싱은 없다. 그러므로, 높은 우선순위의 리얼타임 스레드는 그것의 종료이전까지 CPU를 허용받는다. RR은 RR 정책을 사용한다. 그것은 FIFO와 비슷한데, 같은 우선순위에 한해서 타임 슬라이싱을 한다. POSIX는 추가적인 스케쥴링 클래스인 SCHED.OTHER을 지원하지만, 그것의 구현은 정의가 없고 시스템 특화 형이므로 각 시스템마다 다르게 작동한다.

POSIX API는 정책을 설정하는 두가지 함수가 있다.

- pthread_attr_getschedpolicy(pthread_attr_t * attr, int *policy)
- pthread_attr_setschedpolicy(pthread_attr_t * attr, int policy)

위 함수는 에러 발생시 0이 아닌 값을 리턴한다.

## 5.7 운영체제 예제

우리는 리눅스, 윈도우, 솔라리스 운영체제에서의 스케쥴링 정책을 설명하겠다. *process scheduling*이라는 표현을 일반적으로 사용하겠다. 실제로, 우리는 솔라리스와 윈도우 시스템에서는 커널 스레드, 리눅스에서는 태스크를 다루게 될 것이다.

### 5.7.1 리눅스 스케쥴링

리눅스에서의 프로세스 스케쥴링은 흥미로운 역사가 있다. 2.5버전 이전에는 리눅스 커널은 유닉스 스케쥴링 알고리즘에서 작동했다. 그러나, 알고리즘은 SMP로 디자인되지 않았었고, 그것은 멀티 프로세서에 적합하지 않았다. 추가적으로, 그것은 많은 수의 프로세스에서는 빈약한 성능을 보였다. 버전 2.5의 커널에서, 스케쥴러는 O(1)의 시스템의 태스크 수와는 관련 없는 스케쥴링 알고리즘을 포함해서 성능이 발전했다. O(1) 스케쥴러는 또한 SMP 시스템을 지원하도록 했고, 프로세서 관련성과 로드밸런싱을 지원했다. 그러나, 실제론 비록 O(1)이 SMP에서는 고성능을 보였지만, 그것은 현대의 데스크탑 시스템이 갖는 상호작용의 리스폰스 타임에서는 낮은 성능을 보였다. 2.6 커널이 개발되고, 스케쥴러는 다시 한번 개정되었다. 그리고 2.6.23에서 *Completely Fair Scheduler*(CFS)가 리눅스의 고정 스케쥴링 알고리즘이 되었다.

리눅스 시스템에서의 스케쥴링은 **scheduling class**에 기반한다. 각 클래스는 특정한 우선도를 할당한다. 다른 스케쥴링 클래스를 이용해서, 커널은 시스템과 그것의 프로세스의 필요에 기반한 다른 스케쥴링 알고리즘을 제공한다. 리눅스 서버의 스케쥴링 기준은, 예를 들어서, 리눅스를 작동시키는 모바일 기기에 따라 달라질 수 있다. 어떤 태스크가 다음에 실행될지 결정하는 것은, 스케쥴러가 높은 우선 순위 스케쥴링 클래스 속의 높은 우선순위 태스크를 선택하는 것이다. 표준 리눅스 커널은 2가지 스케쥴링 클래스를 지원한다. 첫번째는 CFS 알고리즘을 사용하는 기본 스케쥴링 알고리즘과 두번쨰는 리얼 타임 스케쥴링 클래스이다. 각 클래스에 대해서 설명하고, 또한 다른 클래스도 추가될 것이다.

타음 퀀텀의 길이에 따라서 상대적인 우선도를 엄격하게 사용하는 것보다는 CFS는 각 태스크에 CPU 프로세싱 시간의 일부분을 나누어 준다. 이 부분은 각 프로세스에 할당된 **nice value**에 따라 계산한다. 이 나이스 밸류는 -20~19까지 분포되있고 낮은 나이스 밸류가 우선도가 높다. 낮은 나이스 밸류 태스크는 높은 CPU 할당시간을 받게된다. 기본 값은 0이다.(*nice*라는 것은 만약 태스크가 그것의 나이스 밸류가 증가하면, 그것은 그것의 상대적 우선도를 낮춤으로서 다른 다른 태스크에게 나이스하게 대한다고 말한다. 즉, 나이스 프로세스가 마지막에 끝낸다.) CFS는 타임슬라이스의 값을 이산화하지 않고 대신에 **targetes latency**(모든 실행가능한 태스크는 한번은 실행되게하는 시간 간격이다.)를 이용한다. CPU 시간의 부분은 targeted latency에 의해서 할당된다. 추가적인 디폴트와 미니멈 값을 가짐으로서, targeted latency는 만약 실행중인 태스크의 숫자가 스레시 홀드를 넘어서면 증가한다.

CFS 스케쥴러는 직접 우선도를 할당하지 않는다. 대신에, 그것은 각 태스크가 실행중에 얼마나 **virtual run time**을 유지했는지 태스크 별 변수인 vruntime에 기록한다. 가상 런타임은 태스크의 우선도에 기본해서 줄어드는 요소이다. 낮은 우선도 태스크는 높은 비율의 감소를 가진다. 보통 우선도를 가진 것은, 가상 런타임은 실제의 물리적 런타임과 같다. 그러므로, 만약, 200미리초의 태스크는 200미리초의 가상 런타임을 가진다. 비슷하게, 만약 높은 우선도가 200미리초 작동하면, 그것은 vruntime이 200미리초보다 적을 것이다. 어떤 태스크를 실행할지는, 스케쥴러는 단순하게 가장 작은 vruntime을 실행한다. 추가로, 높은 우선도의 태스크는 낮은 우선도의 태스크를 선점해서 일 할 수가 있다.

CFS 스케쥴러를 평가하겠다. 같은 나이스 밸류의 두 태스크가 있다. 한가지 태스크는 I/O 바운드이고 하나는 CPU 바운드이다. 일반적으로, I/O 바운드는 짧게 일하고 CPU는 길게 일한다. 그러므로 vruntime은 I/O바운드가 CPU보다 적을것이고 우선권은 I/O 바운드에게 간다. 이런 관점에서, 만약 CPU 바운드가 실행중이면, I/O가 가용해질때 선점당할 것이다.

리눅스는 또한 POSIX를 이용한 리얼 타임 스케쥴링도 구현해두었다. SCHED_FIFO나 SHCED_RR 리얼 타임 정책을 사용하는 태스크는 일반적인 태스크보다 높은 우선도를 가진다. 리눅스는 두가지 구별된 우선도 범위를 갖는데, 하나는 리얼타임, 하나는 노말용이다. 리얼 타임 태스크는 0~99의 정적인 값을 가지고, 일반적인 태스크는 100~39를 갖는다. 이런 두가지 범위는 글로벌 우선도 구조에 들어간다. 노말 태스크는 나이스 밸류에 기반해서 값을 찾고, 그 값은 -20은 100에 매핑하고 19는 139에 매핑된다.

CFS 스케쥴러는 또한 로드밸런싱을 사용하는데, NUMA를 의식하고 스레드의 이동을 최소화하는 섬세한 기술이다. CFS는 각 스레드의 부하를 스레드의 우선도와 CPU utilization의 비율의 합으로 정의한다. 그러므로, 높은 우선 순위를 가진 I/O같은 스레드는, 적은 CPU를 사용한다. 이런 미터법을 사용하면, 큐의 부하는 큐안의 모든 스레드의 부하의 합이고, 밸런싱은 모든 큐가 같은 부하를 가지도록 한다.

5.5.4절에서 강조했듯이, 스레드의 이동은 메모리 엑세스 단점을 준다. 이 문제를 해결하기 위해서 리눅스는 스케쥴링 도메인의 계층적인 시스템을 식별한다. **Scheduling domain**은 밸런스를 맞출 CPU 코어의 집합이다. 각 스케쥴링 도메인 안의 코어는 시스템의 리소스를 얼마나 공유하느냐에 따라서 그룹지어져있다. 4개의 코어에서 각각의 코어는 level1의 공유 캐시를 가지고 2개의 코어끼리 Level2의 공유 캐시를 가지고 4개의 코어가 Level3의 공유캐시를 갖도록 하는 것이다. 그리고 domain0와 domain1같이 2개로 나누는 것이다. NUMA 시스템에서 한단계 더나아가면, 큰 시스템 레벨 도메인은 프로세서 레벨 NUMA 노드를 합친 것이다. 

CFS의 일반적인 전략은 도메인 사이에서 부하를 밸런스 맞추고, 계층의 가장 낮은 레벨에서 시작하는 것이다. 예를 들어서, 첫번째 로드 밸런싱은 스레드는 오직 같은 도메인에서만 이동을 한다. 다음 로드 밸런싱은 도메인 0과 도메인 1을 밸런싱한다. CFS는 만약 스레드가 그것의 로컬 메모리로부터 멀리 이동하는 NUMA 노드간의 스레드 이동과 이런 이동은 로드 임밸런스를 초래할 수 있기에 꺼린다. 일반적인 방법으로, 만약 전체 시스템이 바쁘다면, CFS는 NUMA 시스템의 메모리 레이턴시 패널티를 피하기 위해서 로컬에서 다른 코어로 이동하는 로드 밸런스를 실행하지 않는다.

### 5.7.2 윈도우즈 스케쥴링

윈도우는 우선순위 기반, 선점 알고리즘으로 스레드를 스케쥴한다. 윈도우 스케쥴러는 높은 우선도의 스레드가 무조건 작동하게 보장한다. 윈도우즈 커널의 부분에서 스케쥴링을 담당하는 것을 **dispatcher**라고 한다. 디스패처에게 선택받은 스레드는 그것이 높은 우선도의 스레드에게 선점당하기전, 종료전, 타임퀀텀의 종료, I/O 시스템콜 블럭전까지 작동한다. 만약 높은 우선순위의 리얼타임 스레드가 준비되면, 낮은 우선순위의 스레드는 선점당할 것이다. 선점은 스레드가 이런 접근을 원할때 리얼 타임 스레드가 CPU에 접근하기 쉽게 만들어준다.

디스패처는 32단계의 우선도 구조를 가지고 스레드의 실행순서를 결정한다. 우선도는 두가지 클래스로 분할된다. **Variable class**는 1~15까지의 우선도를 가진다.그리고 **real-time-class**는 16~31까지의 우선도를 가진다.(메모리 관리로 사용될때 우선도 0으로 작동하는 스레드가 있다.) 디스패처는 각 스케쥴링 우선도를 위해서 큐를 사용하고 높은 것부터 낮은것 까지의 큐를 원하는 것을 찾을때까지 탐색한다. 만약 레디큐가 발견되지 않는다면, 디스패처는 **Idle thread**라는 특별한 스레드를 작동한다.

윈도우 커널과 윈도우 API의 많은 우선도에는 관계가 있다. 윈도우 API는 프로세스가 어떤 클래스가 6가지 클래스를 가질수 있는지를 구분한다.
- IDLE_PRiORITY_CLASS
- BELOW_NORMAL_PRIORITY_CLASS
- NORMAL_PRIORITY_CLASS
- ABOVE_NORMAL_PRIORITY_CLASS
- HIGH_PRIORITY_CLASS
- REALTIME_PRIORITY_CLASS

프로세스들은 보통 NORMAL_PRIORITY_CLASS이다. 이 클래스에 속한 프로세스는 부모가 IDLE_PRIORITY_CLASS나 프로세스가 다른 클래스로 판명되었을 떄를 제외한다. 추가적으로, 프로세스의 우선 클래스는 윈도우 API의 SetPriorityClass() 함수로 변경될 수 있다. 리얼타임 클래스를 제외한 모든 클래스의 우선도는 변할수 있고, 스레드의 우선도는 변할수 있다는 것이다.

우선도 클래스가 주어진 스레드는 상대적인 우선도를 가진다. 상대적인 우선도를 위한 값은 다음을 포함한다.
- IDLE
- LOWEST
- BELOW_NORMAL
- NORMAL
- ABOVE_NORMAL
- HIGHEST
- TIME_CRITICAL

각 스레드의 우선도는 그것의 우선도 클래스와 그것의 상대적인 우선도에 기반한다. 우선도 클래스의 값은 가장 위의 행에 나타난다. 좌측 열은 상대적인 우선도 값을 포함한다. 예를 들어, 만약 ABOVE_NORMAL 우선도 클래스의 스레드의 상대적인 우선도가 NORMAL이면 그것의 우선도는 10이다.

더나아가서, 각 스레드는 우선도의 범위의 값을 기반으로 기본 우선도를 가진다. 기본적으로, 기본 우선도는 NORMAL을 가지다. 각 클래스의 기본 우선도는 다음과 같다.
- REALTIME - 24
- HIGH - 13
- ABOVE_NORMAL - 10
- NORMAL - 8
- BELOW_NORMAL - 6
- IDLE - 4
스레드의 우선도는 스레드가 포함된 프로세스의 기본 우선도이다. 비록 윈도우 API의 SetThreadPriority() 함수가 스레드의 기본 우선도를 변경할 수 있다.

스레드의 타임 퀀텀이 끝났을때, 스레드는 인터럽트될 수 있다. 만약 스레드가 vairable-priority 클래스에 속하면, 그것의 우선도는 낮아진다. 우선도는 기본 우선도보다는 낮아질 수 없다. 우선도를 낮추는 것은 계산-바운드 스레드의 CPU 소모를 제한하려고 하는 경향이 있다. variable-priority 스레드가 wait 명령어를 해제하면, 디스패처는 우선도를 생성한다. 생성의 양은 스레드가 무엇을 기다렸는가에 달려있다. 예를 들어서 키보드 I/O를 기다렸다면 그것은 높은 증가를 얻는 반면에, 스레드가 디스크 명령어를 기다리면 평균적인 값을 받을 것이다. 이런 전략은 마우스와 윈도우를 사용하는 상호작용 스레드에 높은 반응 시간을 줄 수 있다. 또한 I/O 바운드 스레드가 계산-바운드 스레드가 백그라운드에서 CPU 사이클을 가지는 동안 I/O 디바이스를 바쁘게 할 수 있다. 이 전략은 유닉스에서도 사용되는 전략이다. 추가적으로, 유저가 현재 상호작용하는 윈도우에 대한 우선도 부스트 또한 반응시간을 향상시킨다.

유저가 상호작용 프로그램을 실행할때, 시스템은 특별히 좋은 성능을 보여야한다. 이런 이유로, 윈도우즈는 NORMAL 클래스의 프로세스에 대해서 특별한 스케쥴링 법칙을 가진다. 윈도우는 현재 선택된 스크린의 **foreground process**와 선택되지 않은 **background process**를 구별한다. 프로세스가 foreground로 이동하면, 윈도우는 3정도의 스케쥴링 퀀텀 값을 올려준다. 이 증가는 타임-쉐어링 선점이 일어나기 전까지 3배의 시간을 가지도록 해준다.

윈도우 7은 **user-mode-scheduling(UMS)**라는 앱들이 커널과는 독립적으로 스레드를 관리하고 생성하는 것을 제공한다. 그러므로, 앱은 윈도우 커널 스케쥴러 없이 스레드를 스케쥴하고 생성한다. 많은 수의 스레드를 생성하는 앱들은, 유저 모드에서의 커널은 커널 모드 스케쥴링보다 더욱 효과적이고, 어떠한 커널 중재가 필요하지 않다.

윈도우의 초기버전은 이와 비슷한 기능은 **Fibers**로 제공했는데, 유저 모드 스레드(fiber)를 단일 커널 스레드에 매핑해주었다. 그러나 이것은 실전에서는 제약이 있었다. fiber는 윈도우즈 API로 만들수 없었는데 왜냐하면, 모든 파이버는 현재 작동하는 스레드의 thread environment block(TEB)를 공유해야만했다. 이것은 만약 윈도우즈 API 함수가 하나의 파이버를 위한 TEB에 상태 정보를 넣어야할 때 문제가 생겼는데, 다른 파이버의 정보가 덮어져있을 수도 있었다. UMS는 이 제약을 그것의 스레드 컨텍스트를 가진 유저모드 스레드를 제공함으로서 극복했다.

추가적으로, fibers와 다르게, UMS는 프로그래머에게 직접적으로 사용되지 않는다. 유저 모드 스케쥴러의 상세를 적는 것은 너무나도 힘들고, UMS는 특정 스케쥴러를 포함하지 않는다. 오히려, 프로그래밍 언어의 라이브러리에서 온 스케쥴러가 UMS의 상단에 빌드되어있다. 예를 들어, 마이크로소프트는 **Concurrency Runtime(ConcRT)**라는 C++로 디자인된 병렬처리를 위해 디자인된 프레임워크를 제공한다.  ConcRT는 프로그램을 태스크로 분해하는 유저모드 스케쥴러를 제공하고, 가능한 프로세싱 코어에 스케쥴한다.

윈도우는 또한 멀티프로세서 시스템을 지원하는데 스레드에게 가장 최적의 프로세싱 코어를 제공한다.(스레드가 가장 최근에 사용한 프로세서를 유지한다.) 윈도우가 사용하는 한가지 기술은 논리 프로세서의 집합을 만드는 것이다.(**SMT** set이라고 알려짐) 하이퍼 스레드 SMT 시스템에서, 같은 CPU 코어에 속한 하드웨어 스레드는 같은 SMT 집합을 가진다. 논리 프로세서는 번호가 붙혀지고 0부터 시작할 것이다. 예를 들어서, 듀얼-스레드/쿼드 코어 시스템은 8개의 논리 프로세서를 가지고 SMT 셋은 다음과 같다. : {0, 1},{2,3}, {4, 5}, {6, 7}이다. 캐시 메모리 손해를 피하기 위해서 스케쥴러는 같은 SMT 셋에서 스레드를 계속 실행하게 한다. 

다른 논리 프로세서에서 부하를 나누기 위해서, 각 스레드는 스레드의 선호 프로세서 숫자가 가르키는 **ideal processor**를 할당받는다. 각 프로세스는 초기 시드 값을 가진다. 이 시드는 각 새로운 스레드가 프로세스에게서 생성될때 마다 증가하고, 다른 논리 프로세서에 분할된다. SMT 셋에서 증가는 다음 SMT 셋의 숫자를 가진다. 예를 들어서 듀얼 스레드/쿼드 코어에서 0 , 2, 4, 6, 0, 2와 같을 것이다. 이런 상황을 피하기 위해서, 각 프로세스의 처음은 프로세서가 0에만 배정되는 상황을 피하기 위해서, 프로세스는 다른 시드 값을 가지고, 시스템의 물리 칩에 스레드의 부하가 분산된다. 

### 5.7.3 솔라리스 스케쥴링

솔라리스는 우선도 기반의 스레드 스케쥴링이다. 각 스레드는 6개중 하나의 클래스를 가진다.
1. Time sharing(TS)
2. Interactive(IA)
3. Real time(RT)
4. System(SYS)
5. Fair share(FSS)
6. Fixed priority(FP)
각 클래스 안에서, 다른 우선도와 다른 스케쥴링 알고리즘이 존재한다.

프로세스를 위한 기본 스케쥴링 클래스는 ts이다. ts 클래스를 위한 스케쥴링 정책은 동적으로 우선도가 변하고 멀티레벨 피드백 큐를 이용해서 다른 길이의 타임 슬라이스를 할당한다. 기본적으로, 우선도와 타임 슬라이스에는 역의 관계가 있다. 우선도가 높을수록, 적은 타임 슬라이스를 가진다. 상호작용 프로세스는 높은 우선도를 가지고 CPu 바운드는 낮은 우선도를 가진다. 이 스케쥴링 정책은 좋은 반응 시간을 주고 CPU 바운드에는 좋은 throughput울 준다. 상호작용 클래스는 ts와 같은 스케쥴링 정책을 가지지만, 윈도잉 앱을 제공한다.(KDE나 GNOME같은 윈도우 매니저이다.(나은 성능을 위해서 높은 우선도 가짐))

솔라리스 시스템의 ts와 ia는 다음과 같은 우선도 값들을 가진다.

- Priority - ts와 ia 클래스에 의존하는 우선도 클래스. 높은 숫자는 높은 우선도를 말한다.
- Time quantum - 관련된 우선도를 위한 타임퀀텀. 이것은 우선도와 타임 퀀타에 역이다. 낮을 수록 높은 값을 가진다.
- Time quantum expired - 블럭킹 없이 이미 그것의 타임퀀텀을 쓴 스레드의 새로운 우선도이다. 이런 스레드는 CPU 인텐시브로 분류된다.
- Return from sleep - 슬리핑(I/O 대기)이후에 스레드의 우선도. I/O가 대기중으로 되면 스레드는 50~59사이의 우선도로 증가된다.

rt 클래스의 스레드는 높은 우선도를 가진다. rt 프로세스는 다른 클래스보다 먼저 실행된다. rt 프로세스는 시스템으로부터 지정한 시간이내에 반응한다는 보장이 있다. 그러나 소수의 프로세스만이 포함되어있다.

솔라리스는 시스템 클래스를 위해서 커널 스레드를 사용한다. 예를 들면 스케쥴러와 페이징 다이몬이다. 한번 시스템 스레드의 우선도가 정해지면, 그것은 변하지 않는다. 시스템 클래스는 커널 사용을 위해서 보존된다.(커널 모드에서 유저 프로세스는 시스템 클래스가 아니다.)

FSS와 FP 클래스는 솔라리스 9에서 소개되었다. FP 클래스의 스레드는 ts와 같은 범위의 우선도를 가진다. 그러나, 그들의 우선도는 동적으로 조정되지 않는다. FSS는 CPU **share**를 스케쥴링 결정대신에 사용한다. CPU share는 가용한 CPU 리소스 자격을 의미하고 프로세스의 집합을 할당된다.

각 스케쥴링 클래스는 우선도의 집합을 가진다. 그러나, 스케쥴러는 클래스 특화 우선도를 글로벌 우선도로 변환하고 글로벌 우선도가 가장 높은 스레드를 고른다. 선택된 스레드는 CPU에서 block, 타임슬라이스 사용, 선점될때까지 실행한다. 만약 같은 우선도의 다중 스레드가 있으면, 스케쥴러는 RR을 사용한다. 커널은 서비스 인터럽트를 위해서 10개의 스레드를 유지하는데, 이 스레드는 스케쥴링 클래스에 속하지 않고 최고로 높은 우선도를 가진다. 솔라리스는 m-m모델을 사용하지만, 솔라리스 9와 함꼐 1-1 모델로 변경되었다.

## 5.8 Algorithm Evaluation

특정 시스템에서 어떤 CPU 스케쥴링 알고리즘을 쓸지 어떻게 고를까? 우리가 5.3절에서 보앗듯이, 스케쥴링 알고리즘은 다양하고, 그것의 파라미터가 있다. 알고리즘을 선택하는 첫번째 문제는 기준을 정의하는 것이다. 5.2절에서 보았듯이, CPU 유틸성, 반응시간, Thruoughput과 같은 다양한 기준이 있다. 알고리즘을 고르려면, 우리는 반드시 이런 요소의 상대적 중요도를 정의해야한다. 우리의 기준은 몇가지 측정을 포함한다.

- 300미리 세컨드의 최대 반응시간을 가진채로 CPU utilization을 최대화하는 것
- turnaround time이 전체 실행시간에 비례하게 throughput을 최대화하는 것

선택 기준이 정의되면, 우리는 고려사항에서 알고리즘을 측정한다. 우리는 이제 다양한 측정법을 알아보겠다.

### 5.8.1 결정 모델링

측정 방법의 한가지 주요 클래스는 **analytic evaluation**이다. 분석 측정은 주어진 알고리즘을 사용하고 시스템 부하가 특정 부하에 맞는 알고리즘의 성능을 측정하는 공식이나 숫자를 생산한다. 

**Deterministic modeling**은 분석 측정의 한가지 방법이다. 이 메서드는 특정한 이미 정해진 부하를 가지고 그 부하에 맞는 알고리즘의 성능을 정의한다. 예를 들어서, 우리가 부하를 가졌다고 가정하겠다. 예를 들어서, 5가지 프로세스가 0초에 도착했다고 가정하겠다. FCFS는 28미리초의 평균 대기시간을 가지고, 비선점 SJF는 13미리초의 대기시간을 가지고 RR은 23미리초의 평균대기시간을 가진다. 이경우에서 우리는 SJF가 가장 대기시간이 짧다고 볼수 있다. 

결정형 모델링은 단순하고 빠르다. 그것은 정확한 숫자를 주고 우리가 알고리즘을 비교하게 해준다. 그러나, 그것은 인풋을 위한 정확한 숫자를 요구하고 그것의 정답은 이런 경우에만 준다. 결정형 모델링의 주요한 사용처는 스케쥴링 알고리즘 설명과 예시를 줄떄이다. 같은 프로그램을 계속 돌리고 또 돌리면 프로그램의 프로세싱 requirement를 측정한다. 우리는 아마도 결정형 모델링을 스케쥴링 알고리즘에 쓸수는 있을 것이다. 더욱이, 예시의 집합은, 결정형 모델링은 분석과 증명을 따로 실행할수 있다. 예를 들어서, 설명된 환경(모든 프로세스가 0초에 도착한다.)은 SJF가 가장 빠른 것을 알수있다.

## 5.8.2 Queueing Models

많은 시스템에서, 프로세스는 날마다 다르고, 결정형 모델링을 쓸수있는 프로세스의 집합은 존재하지않는다. 결정될 수 있는 것은, CPU와 I/O 버스트의 분포뿐이다. 결론은 특정 CPU 버스트의 가능성을 설명하는 수학적 공식이다 일반적으로, 이 분포는 지수적이고 그것의 평균으로 표현한다. 비슷하게, 우리는 프로세스가 시스템에 도착했을 때의 시간 분포를 알 수 있다. 이런 두가지 분포로, 우리는 평균 throughput, utilization, waiting time을 구할 수 있다. 

컴퓨터 시스템은 서버의 네트워크라고 표현된다.  각 서버는 대기 프로세스의 큐를 가진다. CPU는 레디큐를 가진 서버이고, I/O시스템을 가진다. 도착 비율과 서비스 비율을 알면, 우리는 utilization, 평균 큐 길이, 평균 대기시간을 계산할 수 있다. 이런 영역의 연구는 **queueing-network analysis**라고 한다. 

예를 들어서, n이 평균 큐 길이라고 생각하겠다.(서비스되는 프로세스를 제외) W를 큐의 평균 대기시간이라고 하겠다. 그리고 λ는 큐에 도착하는 프로세스의 평균 비율이라고 하겠다.(3개의 프로세스/초) 우리는 W시간동안 λ*W의 새로운 프로세스가 큐에 도착함을 알고있다. 
`n = λ * W`
이 식은 **Little's formula**라고 하고, 스케쥴링 알고리즘과 도착 분포로 특히 유용해진다. 예를 들어서, n은 가게안의 손님수가 될수도있다. 우리는 리틀의 공식을 만약 2가지를 알고있으면 3가지 변수를 알아낼수있다.

큐잉 분석은 스케쥴링 알고리즘을 비교할 때 유용하지만, 한계가 있다. 알고리즘의 클래스와 분포를 조절하는 것이 제한적일 때다. 복잡한 알고리즘과 분포의 수학또한 적용이 힘들다. 그러므로, 도착과 서비스 분포는 보통 수학적으로 다루기 쉽게 주어진다. 그러나 비현실적이다. 또한 독립적인 가정을 만드는 것은 정확하지 않더라도 필수적이다. 이런 어려움 때문에, 큐잉 모델은 실제 시스템의 근사화로 보통 나타내고, 계산된 결과는 불확실하다.

### 5.8.3 시뮬레이션

더욱 정확한 스케쥴링 알고리즘의 측정을 위해서, 우리는 시뮬레이션을 사용할 수 있다. 시뮬레이션 동착은 컴퓨터 시스템의 모델 프로그래밍을 포함한다. 소프트웨어 데이터 구조는 시스템의 주요한 구성요소이다. 시뮬레이터는 클락을 대신하는 변수가 있다. 이 변수의 값이 증가함에 따라서, 시뮬레이터는 장치, 프로세스, 스케쥴러의 활동을 반영하기 위해서 시스템 상태를 변경한다. 시뮬레이션이 진행됨에 따라서, 알고리즘 성능을 나타내는 수치들이 모이고 출력된다.

시뮬레이션을 향하는 데이터는 몇가지 방식으로 생성된다. 가장 일반적인 방법은 프로세스, CPU 버스트 시간, 도착, 마감을 확률 분포에 맞게 생성하는 랜덤 넘버 생성기를 사용하는 것이다. 분포는 수학적으로 정의될 수 있다. 만약에, 문포가 경험적으로 정의 되었다면, 실제 시스템의 측정이 가져와진다. 그 결과는 실제 시스템의 이벤트의 분포로 정의된다. 이 분포는 시뮬레이션을 이끌때 사용된다.

분포-기반 시뮬레이션은 실제 시스템의 연속적인 이벤트의 관계 때문에 정확하지 않을 수 있다. 빈도 분포는 각 이벤트의 인스턴스가 얼마나 일어났는지 가르친다. 그것은 그들의 발생의 순서를 따르지 않는다. 이 문제를 해결하려고, 우리는 **Trace Files**를 사용한다. 우리는 실제 시스템을 모니터링한 것으로 흔적을 만들고 실제 이벤트의 순서를 기록한다. 우리는 이 순서를 시뮬레이션에 사용한다. 트레이스 파일은 두가지 알고리즘이 정확하게 같은 인풋으로 다른 결과를 도출하게 한다. 이 메서드는 그 인풋의 정확한 결과를 생성한다.

시뮬레이션은 비싸고, 컴퓨터 시간을 많이 잡아먹는다. 시뮬레이션이 꼼꼼하고 더 정교할 수록 정확한 결과를 주지만, 컴퓨팅 시간은 증가한다. 추가적으로, 트레이스 파일들은 저장 공간의 큰 양을 필요로한다. 마지막으로, 디자인, 코딩, 시뮬레이터의 디버깅은 주요한 태스크이다.

### 5.8.4 구현

시뮬레이션이 제한된 정확도여도, 스케쥴링 알고리즘을 정확하게 측정할 수 있는 유일한 방법이다. 이 방법은 실제 운영체제 아래서 실제 알고리즘을 넣는다.

이 메서드는 비용과 무관하지 않다. 비용은 알고리즘 코딩과 운영체제가 그것을 지원하게 할때 발생한다. 그리고 변화를 테스팅할때 비용이든다.(보통 가상머신보다는 실제 하드웨어) **Regression testing**은 변화가 더이상 나빠지지 않음을 말하고, 새로운 버그나 옛날버그의 재림이 없다는 말이다.(예를 들어서, 알고리즘이 몇몇 버그를 잡으니 다른 버그가 나온것이다.)

다른 어려움은 어떤 알고리즘의 적용 환경이 변하는 것이다. 환경은 평범한 방식으로 변하지 않을 뿐더러, 새로운 프로그램이 쓰이고 문제의 타입의 변화뿐이 아니라 스케쥴러의 성능의 결과의 변화도 포함한다. 만약 짧은 프로세스의 우선도를 아는데, 유저가 큰 프로세스를 작은 프로세스 조각으로 나누었고, 유저가 상호작용 목적으로 바꾸었다. 이 문제는 보통 사용 툴 또는 스크립트를 가르킨다.

물론 사람 또는 프로그램 행동은 스케쥴링 알고리즘을 피할려고 노력한다. 예를 들어서, 연구자들은 상호작용-비상호작용 프로세스가 termianl I/O를 확인해서 분류하는 한가지 시스템을 디자인한다. 만약 프로세스가 터미널에서 1초동안 input or output하지 않으면, 프로세스는 비상호작용으로 분류되고 낮은 우선순위 큐에 들어간다. 이 정책에 따라서, 한 프로그래머는 1초이하의 터미널 랜덤 캐릭터를 넣도록 수정했다. 시스템은 그의 프로그램에 높은 순위를 주었다. 하지만, 이 행동은 터미널 아웃풋에 아무런 영향도 주지 않느다.

보통, 대부분의 유연한 스케쥴링 알고리즘은 시스템 매니저나 유저에 의해서 변하거나 특정한 앱또는 앱의 집합에 의해서 조정이 가능하다. 하이엔드 그래픽 장치를 수행하는 워크스테이션은 웹서버나 파일 서버에 다른 스케쥴링이 필요할 것이다. 몇몇 운영체제(특히 유닉스)는 시스템 매니저가 특정한 시스템 설정을 미세조정하는 것을 허용한다. 예를 들어서 솔라리스는 dispadimin 커맨드로 시스템 운영자가 스케쥴링 클래스의 파라미터를 변경하게 한다.

다른 접근은 API 사용으로 API로 프로세스 또는 스레드의 우선도를 변경하는 것이다. 자바, 포식스, 윈도우 API는 이런 함수를 제공한다. 이런 접근의 몰락은 시스템 또는 앱 성능 튜닝이고 일반적인 상황에서 성능을 향상시키지는 않는다.